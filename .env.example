# Azure Subscription Variables
TENANT_ID = ""
LOCATION = ""
BASE_NAME = ""
SP_APP_ID = ""
SP_APP_SECRET = ""

# Custom Vision Variables
CUSTOM_VISION_ENDPOINT = ""
CUSTOM_VISION_TRAINING_KEY = ""
CUSTOM_VISION_PREDICTION_KEY = ""
CUSTOM_VISION_PREDICTION_RESOURCE_ID = ""
CUSTOM_VISION_PROJECT_NAME = ""
CUSTOM_VISION_PUBLISH_ITERATION_NAME = ""

# Mock build/release ID for local testing
BUILD_ID = ""
BUILD_URI = ""

# Azure ML Workspace Variables
WORKSPACE_NAME = ""
SUBSCRIPTION_ID = ""
RESOURCE_GROUP = ""
EXPERIMENT_NAME = ""

# AML Compute Cluster Config
AML_TRAIN_ENV_NAME="dogs_clf_train_env"
AML_TRAIN_ENV_CONDA_FILE_PATH="environments/conda/training_environment.yml"
AML_COMPUTE_CLUSTER_NAME = "gpu-cluster"
AML_COMPUTE_CLUSTER_GPU_SKU = "STANDARD_NC6"
AML_CLUSTER_MAX_NODES = "4"
AML_CLUSTER_MIN_NODES = "0"
AML_CLUSTER_PRIORITY = "lowpriority"

# Training Config
MODEL_NAME = "dog_clf_model"
MODEL_VERSION = "1"

# AML Pipeline Config
TRAINING_PIPELINE_NAME = "dog-classification-training-pipeline"
MODEL_PATH = ""
PIPELINE_TRAIN_SCRIPT_PATH = "pipeline/train_model_step.py"
PIPELINE_EVALUATE_SCRIPT_PATH = "pipeline/evaluate_model_step.py"
PIPELINE_REGISTER_SCRIPT_PATH = "pipeline/register_model_step.py"
SOURCES_DIR_TRAIN = "src/training"
DATASET_NAME = "stanford_dogs_dataset"
DATASET_VERSION = "1"
# Optional. Set it if you have configured non default datastore to point to your data
DATASTORE_NAME = ""
SCORE_SCRIPT = "scoring/score.py"
CONDA_ENV_DIR= "environments/conda"

# Optional. Container Image name for image creation
IMAGE_NAME = ""

# Run Evaluation Step in AML pipeline
PIPELINE_RUN_EVALUATION = "true"

# Set to true cancels the Azure ML pipeline run when evaluation criteria are not met.
PIPELINE_ALLOW_RUN_CANCEL = "true"

# Flag to allow rebuilding the AML Environment after it was built for the first time. This enables dependency updates from conda_dependencies.yaml.
AML_TRAIN_ENV_REBUILD = "false"

# WIP
USE_GPU_FOR_SCORING = "false"
AML_ENV_SCORE_CONDA_DEP_FILE=""
AML_ENV_SCORECOPY_CONDA_DEP_FILE=""
# AML Compute Cluster Config for parallel batch scoring
AML_ENV_NAME_SCORING=""
AML_ENV_NAME_SCORE_COPY=""
AML_COMPUTE_CLUSTER_NAME_SCORING = ""
AML_COMPUTE_CLUSTER_CPU_SKU_SCORING = "STANDARD_DS2_V2"
AML_CLUSTER_MAX_NODES_SCORING = "4"
AML_CLUSTER_MIN_NODES_SCORING = "0"
AML_CLUSTER_PRIORITY_SCORING = "lowpriority"
AML_REBUILD_ENVIRONMENT_SCORING = "true"
BATCHSCORE_SCRIPT_PATH = "scoring/parallel_batchscore.py"
BATCHSCORE_COPY_SCRIPT_PATH = "scoring/parallel_batchscore_copyoutput.py"
SCORING_DATASTORE_INPUT_CONTAINER = "input"
SCORING_DATASTORE_INPUT_FILENAME = "diabetes_scoring_input.csv"
SCORING_DATASTORE_OUTPUT_CONTAINER = "output"
SCORING_DATASTORE_OUTPUT_FILENAME = "diabetes_scoring_output.csv"
SCORING_DATASET_NAME = "diabetes_scoring_ds"
SCORING_PIPELINE_NAME = "diabetes-scoring-pipeline"
SRC_DIR = "src"
DATA_DIR = "data"
DATASTORE_TARGET_DIR = "data/stanford_dogs"
