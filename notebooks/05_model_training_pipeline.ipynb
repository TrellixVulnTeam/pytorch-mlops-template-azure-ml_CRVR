{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Notebook Summary](#Notebook-Summary)\n",
    "* [Setup](#Setup)\n",
    "    * [Notebook Parameters](#Notebook-Parameters)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Pipeline Run Configuration & Environment](#Pipeline-Run-Configuration-&-Environment)\n",
    "* [Pipeline Inputs](#Pipeline-Inputs)\n",
    "* [Create Pipeline](#Create-Pipeline)\n",
    "    * [Training Step](#Training-Step)\n",
    "    * [Evaluate Step](#Evaluate-Step)\n",
    "    * [Register Step](#Register-Step)\n",
    "* [Publish Pipeline](#Publish-Pipeline)\n",
    "* [Run Pipeline](#Run-Pipeline)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, an Azure Machine Learning (AML) training / retraining pipeline will be built and published. After building and publishing the pipeline, a REST endpoint can be used to trigger the pipeline from any HTTP library on any platform. This pipeline will be used in the MlOps process for continuous model retraining, e.g. when data or model drift is detected or in general when the model should be retrained. A pipeline gives a more operationalizable way of training than a script run (which was used for original model training in the `02_model_training` notebook) as it can be easily automated and run based on triggers. It also allows for chaining of different steps that can then be executed sequentially. In general, machine learning pipelines help to optimize the workflow in terms of speed, portability and reuse.\n",
    "\n",
    "The training / retraining pipeline built in this notebook will consist of three different steps that are executed sequentially:\n",
    "- Model training using the same code for training as in the `02_model_training` notebook\n",
    "- Model evaluation (comparing the newly trained model with the model currently in production or with a manual threshold)\n",
    "- Model registration (registering the newly trained model to the AML workspace based on the outcomes of the model evaluation)\n",
    "\n",
    "Check out the [AML Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines) for more info on how to build pipelines in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import azureml\n",
    "from azureml.core import Dataset, Datastore, Environment, Experiment, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the remote compute target cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "# Define the name of the training environment created in the 00_environment_setup notebook\n",
    "env_name = \"stanford-dogs-train-env\"\n",
    "\n",
    "# Determine whether the pipeline training run should be evaluated before model registration\n",
    "run_evaluation = True\n",
    "\n",
    "# Define the pipeline endpoint name\n",
    "pipeline_name = \"dog_clf_model_training_pipeline\"\n",
    "\n",
    "# Define the pipeline endpoint version\n",
    "pipeline_version = \"1.ß\"\n",
    "\n",
    "# Define the model_name\n",
    "model_name = \"dog_clf_model\"\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"stanford_dogs_classifier_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the AML workspace, a workspace object needs to be instantiated using the AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a remote compute target to run the pipeline experiments on. The below code will first check whether a compute target with name **cluster_name** (defined in the [Notebook Parameters](#Notebook-Parameters) section) already exists and if it does, will retrieve it. Otherwise it will create a new compute cluster.\n",
    "\n",
    "**Note**: At the moment it is not possible to create a new compute cluster so please specify the name of an existing compute cluster.\n",
    "\n",
    "AML pipelines need to be run on a remote compute target and cannot be run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-03-25T06:14:03.766000+00:00', 'errors': None, 'creationTime': '2021-02-04T18:49:38.130943+00:00', 'modifiedTime': '2021-02-04T18:49:53.799036+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Configuration & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model training environment that has been registered as part of the `00_environment_setup` notebook and use it for the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline run configuration containing the retrieved environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PipelineData object to pass data between steps.\n",
    "\n",
    "While here the pipeline will consist of a single step only, a usual flow with multiple steps will include:\n",
    "- Using Dataset objects as inputs to fetch raw data, performing some transformations, then outputting a PipelineData object.\n",
    "- Use the previous step's PipelineData output object as an input object, repeated for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = PipelineData(\"pipeline_data\", datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PipelineParameter objects to be able to pass versatile arguments to the PythonScriptSteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_param = PipelineParameter(name=\"dataset_name\", default_value=\"stanford_dogs_dataset\")\n",
    "dataset_version_param = PipelineParameter(name=\"dataset_version\", default_value=1)\n",
    "data_file_path_param = PipelineParameter(name=\"data_file_path\", default_value=\"none\")\n",
    "model_name_param = PipelineParameter(name=\"model_name\", default_value=\"dog_clf_model\")\n",
    "caller_run_id_param = PipelineParameter(name=\"caller_run_id\", default_value=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a pipeline, the individual steps need to be created first.\n",
    "\n",
    "A pipeline step is an object that encapsulates everything that is needed for running a pipeline including:\n",
    "\n",
    "- environment and dependency settings\n",
    "- the compute target to run the pipeline on\n",
    "- input and output data, and any custom parameters\n",
    "- reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class PipelineStep to assist with building a step using certain frameworks and stacks. Here, the PythonScriptStep class is used to define the step logic using the train_model.py script.\n",
    "\n",
    "An object reference in the outputs array becomes available as an input for a subsequent pipeline step, for scenarios where there is more than one step.\n",
    "\n",
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/en-gb/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline training step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step has been created.\n"
     ]
    }
   ],
   "source": [
    "train_step = PythonScriptStep(name=\"Train Model\",\n",
    "                              script_name=\"pipeline/train_model_step.py\",\n",
    "                              compute_target=compute_target,\n",
    "                              source_directory=\"../src\",\n",
    "                              outputs=[pipeline_data],\n",
    "                              arguments=[\"--model_name\", model_name_param,\n",
    "                                         \"--step_output\", pipeline_data,\n",
    "                                         \"--dataset_version\", dataset_version_param,\n",
    "                                         \"--data_file_path\", data_file_path_param,\n",
    "                                         \"--caller_run_id\", caller_run_id_param,\n",
    "                                         \"--dataset_name\", dataset_name_param],\n",
    "                              runconfig=run_config,\n",
    "                              allow_reuse=True)\n",
    "\n",
    "print(\"Training step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline evaluate step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate step has been created.\n"
     ]
    }
   ],
   "source": [
    "evaluate_step = PythonScriptStep(name=\"Evaluate Model\",\n",
    "                                 script_name=\"pipeline/evaluate_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--allow_run_cancel\", True],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Evaluate step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline register step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register step has been created.\n"
     ]
    }
   ],
   "source": [
    "register_step = PythonScriptStep(name=\"Register Model \",\n",
    "                                 script_name=\"pipeline/register_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 inputs=[pipeline_data],\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--step_input\", pipeline_data],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Register step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch the three pipeline steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include evaluation step before register step.\n"
     ]
    }
   ],
   "source": [
    "# Check run_evaluation flag to include or exclude evaluation step.\n",
    "if run_evaluation == True:\n",
    "    print(\"Include evaluation step before register step.\")\n",
    "    evaluate_step.run_after(train_step)\n",
    "    register_step.run_after(evaluate_step)\n",
    "    steps = [train_step, evaluate_step, register_step]\n",
    "else:\n",
    "    print(\"Exclude evaluation step and directly run register step.\")\n",
    "    register_step.run_after(train_step)\n",
    "    steps = [train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and validate the pipeline based on the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Train Model is ready to be created [0bad7338]\n",
      "Step Evaluate Model is ready to be created [970d713e]\n",
      "Step Register Model  is ready to be created [aee6a099]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "train_pipeline._set_experiment_name\n",
    "train_pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish the pipeline to create a REST endpoint that allows to rerun the pipeline from any HTTP library on any platform. The published pipeline can also be run from the AML workspace where different metdata such as run history and duration are tracked as well. If a pipeline with the same version has already been published, retrieve the existing published pipeline instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Train Model [0bad7338][53dfba84-57e8-4d7a-8ef5-522938d3e807], (This step will run and generate new outputs)\n",
      "Created step Evaluate Model [970d713e][da441d12-0054-451a-9b67-ed2ac6df05c7], (This step will run and generate new outputs)\n",
      "Created step Register Model  [aee6a099][94fef048-2bf0-47b8-868f-47afffae3002], (This step will run and generate new outputs)\n",
      "Published pipeline 'dog_clf_model_training_pipeline' with version 1.ß.\n"
     ]
    }
   ],
   "source": [
    "pipelines = PublishedPipeline.list(ws)\n",
    "matched_pipes = []\n",
    "\n",
    "for p in pipelines:\n",
    "    if p.name == pipeline_name:\n",
    "        if p.version == pipeline_version:\n",
    "            matched_pipes.append(p)\n",
    "\n",
    "if(len(matched_pipes) == 0):\n",
    "    published_pipeline = train_pipeline.publish(name=pipeline_name,\n",
    "                                                description=\"Model training/retraining pipeline\",\n",
    "                                                version=pipeline_version)\n",
    "    \n",
    "    print(f\"Published pipeline '{published_pipeline.name}' with version {published_pipeline.version}.\")\n",
    "\n",
    "else:\n",
    "    published_pipeline = matched_pipes[0]\n",
    "    print(f\"Retrieved published pipeline with id {published_pipeline.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline run takes more time than subsequent runs, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. Total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 5dd4d202-711a-458a-8041-56effefc11c9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/5dd4d202-711a-458a-8041-56effefc11c9?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n"
     ]
    }
   ],
   "source": [
    "pipeline_parameters = {\"model_name\": model_name}\n",
    "tags = {\"trigger\": \"jupyter notebook\",\n",
    "        \"model_architecture\" : \"transfer-learning with ResNext-50\"}\n",
    "\n",
    "# Create an AML Experiment\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    \n",
    "# Submit an Experiment Run using the published pipeline and defined pipeline parameters\n",
    "run = experiment.submit(published_pipeline,\n",
    "                        tags=tags,\n",
    "                        pipeline_parameters=pipeline_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 5dd4d202-711a-458a-8041-56effefc11c9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/5dd4d202-711a-458a-8041-56effefc11c9?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 33d19741-c0d5-4b68-b744-7b05ba40cdea\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/33d19741-c0d5-4b68-b744-7b05ba40cdea?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n",
      "StepRun( Train Model ) Status: NotStarted\n",
      "StepRun( Train Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_6d6335035eba896d3f928691ee471f6865c8f286db5f99fb1c7d4642f915023a_p.txt\n",
      "========================================================================================================================\n",
      "2021-03-25T06:21:06Z Starting output-watcher...\n",
      "2021-03-25T06:21:06Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
      "2021-03-25T06:21:07Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-03-25T06:21:07Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      "2c11b7cecaa5: Pulling fs layer\n",
      "04637fa56252: Pulling fs layer\n",
      "d6e6af23a0f3: Pulling fs layer\n",
      "b4a424de92ad: Pulling fs layer\n",
      "3e5d9ee64909: Pulling fs layer\n",
      "3a846111ff22: Pulling fs layer\n",
      "93a5020c6e19: Pulling fs layer\n",
      "360b353e68fd: Pulling fs layer\n",
      "ea4e2e1810f8: Pulling fs layer\n",
      "def12cf7de15: Pulling fs layer\n",
      "3ae6adfbdb11: Pulling fs layer\n",
      "2a21fbf2232e: Pulling fs layer\n",
      "2ef655776049: Pulling fs layer\n",
      "2b2711cecc87: Pulling fs layer\n",
      "2468e9aec01e: Pulling fs layer\n",
      "92a4e88a49e9: Pulling fs layer\n",
      "a33b32b5286b: Pulling fs layer\n",
      "98c96e7c8036: Pulling fs layer\n",
      "0ca6e388c01d: Pulling fs layer\n",
      "cb0e780a2d44: Pulling fs layer\n",
      "00361b3fc45c: Pulling fs layer\n",
      "def12cf7de15: Waiting\n",
      "ea4e2e1810f8: Waiting\n",
      "3ae6adfbdb11: Waiting\n",
      "3a846111ff22: Waiting\n",
      "2a21fbf2232e: Waiting\n",
      "360b353e68fd: Waiting\n",
      "2ef655776049: Waiting\n",
      "93a5020c6e19: Waiting\n",
      "98c96e7c8036: Waiting\n",
      "2b2711cecc87: Waiting\n",
      "b4a424de92ad: Waiting\n",
      "0ca6e388c01d: Waiting\n",
      "2468e9aec01e: Waiting\n",
      "00361b3fc45c: Waiting\n",
      "92a4e88a49e9: Waiting\n",
      "cb0e780a2d44: Waiting\n",
      "3e5d9ee64909: Waiting\n",
      "a33b32b5286b: Waiting\n",
      "04637fa56252: Verifying Checksum\n",
      "04637fa56252: Download complete\n",
      "d6e6af23a0f3: Download complete\n",
      "b4a424de92ad: Verifying Checksum\n",
      "b4a424de92ad: Download complete\n",
      "2c11b7cecaa5: Verifying Checksum\n",
      "2c11b7cecaa5: Download complete\n",
      "3a846111ff22: Verifying Checksum\n",
      "3a846111ff22: Download complete\n",
      "93a5020c6e19: Verifying Checksum\n",
      "93a5020c6e19: Download complete\n",
      "3e5d9ee64909: Verifying Checksum\n",
      "3e5d9ee64909: Download complete\n",
      "2c11b7cecaa5: Pull complete\n",
      "360b353e68fd: Verifying Checksum\n",
      "360b353e68fd: Download complete\n",
      "04637fa56252: Pull complete\n",
      "d6e6af23a0f3: Pull complete\n",
      "b4a424de92ad: Pull complete\n",
      "3ae6adfbdb11: Verifying Checksum\n",
      "3ae6adfbdb11: Download complete\n",
      "def12cf7de15: Verifying Checksum\n",
      "def12cf7de15: Download complete\n",
      "2a21fbf2232e: Verifying Checksum\n",
      "2a21fbf2232e: Download complete\n",
      "2ef655776049: Verifying Checksum\n",
      "2ef655776049: Download complete\n",
      "ea4e2e1810f8: Verifying Checksum\n",
      "ea4e2e1810f8: Download complete\n",
      "2468e9aec01e: Verifying Checksum\n",
      "2468e9aec01e: Download complete\n",
      "92a4e88a49e9: Verifying Checksum\n",
      "92a4e88a49e9: Download complete\n",
      "2b2711cecc87: Verifying Checksum\n",
      "2b2711cecc87: Download complete\n",
      "98c96e7c8036: Verifying Checksum\n",
      "98c96e7c8036: Download complete\n",
      "0ca6e388c01d: Download complete\n",
      "00361b3fc45c: Verifying Checksum\n",
      "00361b3fc45c: Download complete\n",
      "cb0e780a2d44: Verifying Checksum\n",
      "cb0e780a2d44: Download complete\n",
      "3e5d9ee64909: Pull complete\n",
      "3a846111ff22: Pull complete\n",
      "93a5020c6e19: Pull complete\n",
      "360b353e68fd: Pull complete\n",
      "ea4e2e1810f8: Pull complete\n",
      "def12cf7de15: Pull complete\n",
      "3ae6adfbdb11: Pull complete\n",
      "2a21fbf2232e: Pull complete\n",
      "2ef655776049: Pull complete\n",
      "2b2711cecc87: Pull complete\n",
      "2468e9aec01e: Pull complete\n",
      "92a4e88a49e9: Pull complete\n",
      "a33b32b5286b: Download complete\n",
      "a33b32b5286b: Pull complete\n",
      "98c96e7c8036: Pull complete\n",
      "0ca6e388c01d: Pull complete\n",
      "cb0e780a2d44: Pull complete\n",
      "00361b3fc45c: Pull complete\n",
      "Digest: sha256:061c85dd8a554afd2ae1b47f6d3be6cabfa07429d7c0ff4f665a3ef43f33114b\n",
      "Status: Downloaded newer image for 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "2021-03-25T06:22:10Z Check if container 33d19741-c0d5-4b68-b744-7b05ba40cdea already exist exited with 0, \n",
      "\n",
      "45f4de3edcdf8cfe3444a50024b669e802e4e488d13f78b827d768eae92bcd7e\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/03/25 06:22:34 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
      "2021/03/25 06:22:34 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "[2021-03-25T06:22:36.185858] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['pipeline/train_model_step.py', '--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/mounts/workspaceblobstore/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 128\n",
      "[2021-03-25T06:22:37.958455] Entering Run History Context Manager.\n",
      "[2021-03-25T06:22:39.547825] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/mounts/workspaceblobstore/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea\n",
      "[2021-03-25T06:22:39.548160] Preparing to call script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/mounts/workspaceblobstore/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "[2021-03-25T06:22:39.548241] After variable expansion, calling script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/mounts/workspaceblobstore/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "\n",
      "2021/03/25 06:22:39 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "Running train_model_step.py\n",
      "Argument [caller_run_id]: none\n",
      "Argument [dataset_name]: stanford_dogs_dataset\n",
      "Argument [dataset_version]: 1\n",
      "Argument [data_file_path]: none\n",
      "Argument [model_name]: dog_clf_model\n",
      "Argument [step_output]: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/mounts/workspaceblobstore/azureml/33d19741-c0d5-4b68-b744-7b05ba40cdea/pipeline_data\n",
      "\n",
      "Getting training parameters\n",
      "Parameters: {'num_epochs': 1, 'batch_size': 8, 'learning_rate': 0.001, 'momentum': 0.9, 'num_frozen_layers': 7, 'num_neurons_fc_layer': 512, 'dropout_prob_fc_layer': 0.0, 'lr_scheduler_step_size': 7}\n",
      "--------------------\n",
      "START MODEL TRAINING\n",
      "--------------------\n",
      "Hyperparameter number of epochs: 1\n",
      "Hyperparameter batch size: 8\n",
      "Hyperparameter learning rate: 0.001\n",
      "Hyperparameter momentum: 0.9\n",
      "Hyperparameter number of frozen layers: 7\n",
      "Hyperparameter number of neurons fc layer: 512\n",
      "Hyperparameter dropout probability fc layer: 0\n",
      "Hyperparameter lr scheduler step size: 7\n",
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
      "\n",
      "  0%|          | 0.00/95.8M [00:00<?, ?B/s]\n",
      "  3%|▎         | 2.55M/95.8M [00:00<00:03, 26.7MB/s]\n",
      "  5%|▌         | 4.88M/95.8M [00:00<00:03, 25.9MB/s]\n",
      "  8%|▊         | 7.50M/95.8M [00:00<00:03, 26.3MB/s]\n",
      " 11%|█         | 10.3M/95.8M [00:00<00:03, 27.2MB/s]\n",
      " 13%|█▎        | 12.9M/95.8M [00:00<00:03, 27.1MB/s]\n",
      " 16%|█▋        | 15.6M/95.8M [00:00<00:03, 27.4MB/s]\n",
      " 19%|█▉        | 18.0M/95.8M [00:00<00:03, 26.5MB/s]\n",
      " 22%|██▏       | 20.8M/95.8M [00:00<00:02, 27.2MB/s]\n",
      " 25%|██▍       | 23.5M/95.8M [00:00<00:02, 27.4MB/s]\n",
      " 27%|██▋       | 26.2M/95.8M [00:01<00:02, 27.8MB/s]\n",
      " 30%|███       | 28.8M/95.8M [00:01<00:02, 27.6MB/s]\n",
      " 33%|███▎      | 31.6M/95.8M [00:01<00:02, 27.9MB/s]\n",
      " 36%|███▌      | 34.2M/95.8M [00:01<00:02, 25.6MB/s]\n",
      " 38%|███▊      | 36.9M/95.8M [00:01<00:02, 26.2MB/s]\n",
      " 41%|████▏     | 39.6M/95.8M [00:01<00:02, 26.6MB/s]\n",
      " 44%|████▍     | 42.2M/95.8M [00:01<00:02, 26.4MB/s]\n",
      " 47%|████▋     | 45.1M/95.8M [00:01<00:01, 27.5MB/s]\n",
      " 50%|████▉     | 47.8M/95.8M [00:01<00:01, 27.8MB/s]\n",
      " 53%|█████▎    | 50.6M/95.8M [00:01<00:01, 28.0MB/s]\n",
      " 56%|█████▌    | 53.2M/95.8M [00:02<00:01, 27.8MB/s]\n",
      " 58%|█████▊    | 55.9M/95.8M [00:02<00:01, 27.9MB/s]\n",
      " 61%|██████▏   | 58.7M/95.8M [00:02<00:01, 27.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 61.4M/95.8M [00:02<00:01, 27.5MB/s]\n",
      " 67%|██████▋   | 64.0M/95.8M [00:02<00:01, 27.6MB/s]\n",
      " 70%|██████▉   | 66.7M/95.8M [00:02<00:01, 26.5MB/s]\n",
      " 72%|███████▏  | 69.2M/95.8M [00:02<00:01, 26.3MB/s]\n",
      " 75%|███████▌  | 72.0M/95.8M [00:02<00:00, 27.1MB/s]\n",
      " 78%|███████▊  | 74.7M/95.8M [00:02<00:00, 27.5MB/s]\n",
      " 81%|████████  | 77.5M/95.8M [00:02<00:00, 27.7MB/s]\n",
      " 84%|████████▎ | 80.2M/95.8M [00:03<00:00, 27.5MB/s]\n",
      " 86%|████████▋ | 82.8M/95.8M [00:03<00:00, 27.5MB/s]\n",
      " 89%|████████▉ | 85.6M/95.8M [00:03<00:00, 27.9MB/s]\n",
      " 92%|█████████▏| 88.2M/95.8M [00:03<00:00, 27.6MB/s]\n",
      " 95%|█████████▍| 90.9M/95.8M [00:03<00:00, 27.7MB/s]\n",
      " 98%|█████████▊| 93.6M/95.8M [00:03<00:00, 26.9MB/s]\n",
      "100%|██████████| 95.8M/95.8M [00:03<00:00, 27.2MB/s]\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "--------------------\n",
      "Train Loss: 1.7562 Train Acc: 0.5297\n"
     ]
    }
   ],
   "source": [
    "# Wait for completion of the run and show output log\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the compute target.\n",
    "\n",
    "**Note**: At the moment the compute target can and should not be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford-dogs-dev-env",
   "language": "python",
   "name": "stanford-dogs-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
