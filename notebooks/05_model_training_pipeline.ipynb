{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Notebook Summary](#Notebook-Summary)\n",
    "* [Setup](#Setup)\n",
    "    * [Notebook Parameters](#Notebook-Parameters)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Pipeline Run Configuration & Environment](#Pipeline-Run-Configuration-&-Environment)\n",
    "* [Pipeline Inputs](#Pipeline-Inputs)\n",
    "* [Create Pipeline](#Create-Pipeline)\n",
    "    * [Training Step](#Training-Step)\n",
    "    * [Evaluate Step](#Evaluate-Step)\n",
    "    * [Register Step](#Register-Step)\n",
    "* [Publish Pipeline](#Publish-Pipeline)\n",
    "* [Run Pipeline](#Run-Pipeline)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, an Azure Machine Learning (AML) training / retraining pipeline will be built and published. After building and publishing the pipeline, a REST endpoint can be used to trigger the pipeline from any HTTP library on any platform. This pipeline will be used in the MLOps process for continuous model retraining, e.g. when data or model drift is detected or in general when the model should be retrained. A pipeline gives a more operationalizable way of training than a script run (which was used for original model training in the `02_model_training` notebook) as it can be easily automated and run based on triggers. It also allows for chaining of different steps that can then be executed sequentially. In general, machine learning pipelines help to optimize the workflow in terms of speed, portability and reuse.\n",
    "\n",
    "The training / retraining pipeline built in this notebook will consist of three different steps that are executed sequentially:\n",
    "- Model training using the same code for training as in the `02_model_training` notebook\n",
    "- Model evaluation (comparing the newly trained model with the model currently in production or with a manual threshold)\n",
    "- Model registration (registering the newly trained model to the AML workspace based on the outcomes of the model evaluation)\n",
    "\n",
    "Check out the [AML Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines) for more info on how to build pipelines in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.28.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import azureml\n",
    "from azureml.core import Dataset, Datastore, Environment, Experiment, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the notebook parameters which are used in the source code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the remote compute target cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "# Define the name of the training environment created in the 00_environment_setup notebook\n",
    "env_name = \"stanford-dogs-train-env\"\n",
    "\n",
    "# Determine whether the pipeline training run should be evaluated before model registration\n",
    "run_evaluation = True\n",
    "\n",
    "# Define the pipeline endpoint name\n",
    "pipeline_name = \"dog_clf_model_training_pipeline\"\n",
    "\n",
    "# Define the pipeline endpoint version\n",
    "# Make sure to update this every time you want to publish changes to your pipeline!!!\n",
    "pipeline_version = \"1.0\"\n",
    "\n",
    "# Define the model_name\n",
    "model_name = \"dog_clf_model\"\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"stanford_dogs_classifier_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the AML workspace, a workspace object needs to be instantiated using the AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a remote compute target to run the pipeline experiments on. The below code will first check whether a compute target with name **cluster_name** (defined in the [Notebook Parameters](#Notebook-Parameters) section) already exists and if it does, will retrieve it. Otherwise it will create a new compute cluster.\n",
    "\n",
    "AML pipelines need to be run on a remote compute target and cannot be run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-06-19T16:23:57.320000+00:00', 'errors': None, 'creationTime': '2021-02-04T18:49:38.130943+00:00', 'modifiedTime': '2021-02-04T18:49:53.799036+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Configuration & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model training environment that has been registered as part of the `00_environment_setup` notebook and use it for the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline run configuration containing the retrieved environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PipelineData object to pass data between steps.\n",
    "\n",
    "While here the pipeline will consist of a single step only, a usual flow with multiple steps will include:\n",
    "- Using Dataset objects as inputs to fetch raw data, performing some transformations, then outputting a PipelineData object.\n",
    "- Use the previous step's PipelineData output object as an input object, repeated for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = PipelineData(\"pipeline_data\", datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PipelineParameter objects to be able to pass versatile arguments to the PythonScriptSteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_param = PipelineParameter(name=\"dataset_name\", default_value=\"stanford_dogs_dataset\")\n",
    "dataset_version_param = PipelineParameter(name=\"dataset_version\", default_value=1)\n",
    "data_file_path_param = PipelineParameter(name=\"data_file_path\", default_value=\"none\")\n",
    "model_name_param = PipelineParameter(name=\"model_name\", default_value=\"dog_clf_model\")\n",
    "caller_run_id_param = PipelineParameter(name=\"caller_run_id\", default_value=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a pipeline, the individual steps need to be created first.\n",
    "\n",
    "A pipeline step is an object that encapsulates everything that is needed for running a pipeline including:\n",
    "\n",
    "- environment and dependency settings\n",
    "- the compute target to run the pipeline on\n",
    "- input and output data, and any custom parameters\n",
    "- reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class PipelineStep to assist with building a step using certain frameworks and stacks. Here, the PythonScriptStep class is used to define the step logic using the train_model.py script.\n",
    "\n",
    "An object reference in the outputs array becomes available as an input for a subsequent pipeline step, for scenarios where there is more than one step.\n",
    "\n",
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/en-gb/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline training step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step has been created.\n"
     ]
    }
   ],
   "source": [
    "train_step = PythonScriptStep(name=\"Train Model\",\n",
    "                              script_name=\"pipeline/train_model_step.py\",\n",
    "                              compute_target=compute_target,\n",
    "                              source_directory=\"../src\",\n",
    "                              outputs=[pipeline_data],\n",
    "                              arguments=[\"--model_name\", model_name_param,\n",
    "                                         \"--step_output\", pipeline_data,\n",
    "                                         \"--dataset_version\", dataset_version_param,\n",
    "                                         \"--data_file_path\", data_file_path_param,\n",
    "                                         \"--caller_run_id\", caller_run_id_param,\n",
    "                                         \"--dataset_name\", dataset_name_param],\n",
    "                              runconfig=run_config,\n",
    "                              allow_reuse=True)\n",
    "\n",
    "print(\"Training step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline evaluate step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate step has been created.\n"
     ]
    }
   ],
   "source": [
    "evaluate_step = PythonScriptStep(name=\"Evaluate Model\",\n",
    "                                 script_name=\"pipeline/evaluate_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--allow_run_cancel\", True],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Evaluate step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline register step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register step has been created.\n"
     ]
    }
   ],
   "source": [
    "register_step = PythonScriptStep(name=\"Register Model \",\n",
    "                                 script_name=\"pipeline/register_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 inputs=[pipeline_data],\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--step_input\", pipeline_data],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Register step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch the three pipeline steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include evaluation step before register step.\n"
     ]
    }
   ],
   "source": [
    "# Check run_evaluation flag to include or exclude evaluation step.\n",
    "if run_evaluation == True:\n",
    "    print(\"Include evaluation step before register step.\")\n",
    "    evaluate_step.run_after(train_step)\n",
    "    register_step.run_after(evaluate_step)\n",
    "    steps = [train_step, evaluate_step, register_step]\n",
    "else:\n",
    "    print(\"Exclude evaluation step and directly run register step.\")\n",
    "    register_step.run_after(train_step)\n",
    "    steps = [train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and validate the pipeline based on the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "train_pipeline._set_experiment_name\n",
    "train_pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish the pipeline to create a REST endpoint that allows to rerun the pipeline from any HTTP library on any platform. The published pipeline can also be run from the AML workspace where different metdata such as run history and duration are tracked as well. \n",
    "\n",
    "**Note**: If a pipeline with the same version has already been published, the code will retrieve the existing published pipeline instead. This means that whenever you make changes to the pipeline you need to specify a new pipeline version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Train Model [4666da05][b26b1509-d5ed-4c36-b60f-f79b23c53bb9], (This step is eligible to reuse a previous run's output)\n",
      "Created step Evaluate Model [f902a643][48b0959e-b6b0-4b6c-828e-85e2ba63fe5d], (This step will run and generate new outputs)Created step Register Model  [08e01182][90bfcc00-13eb-496a-93e7-ebdaee8a55ee], (This step will run and generate new outputs)\n",
      "\n",
      "Published pipeline 'dog_clf_model_training_pipeline' with version 1.0.\n"
     ]
    }
   ],
   "source": [
    "pipelines = PublishedPipeline.list(ws)\n",
    "matched_pipes = []\n",
    "\n",
    "for p in pipelines:\n",
    "    if p.name == pipeline_name:\n",
    "        if p.version == pipeline_version:\n",
    "            matched_pipes.append(p)\n",
    "\n",
    "if(len(matched_pipes) == 0):\n",
    "    published_pipeline = train_pipeline.publish(name=pipeline_name,\n",
    "                                                description=\"Model training/retraining pipeline\",\n",
    "                                                version=pipeline_version)\n",
    "    \n",
    "    print(f\"Published pipeline '{published_pipeline.name}' with version {published_pipeline.version}.\")\n",
    "\n",
    "else:\n",
    "    published_pipeline = matched_pipes[0]\n",
    "    print(f\"Retrieved published pipeline with id {published_pipeline.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline run takes more time than subsequent runs, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. Total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 27299434-dc04-402f-9295-eeef0d43a9c8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/27299434-dc04-402f-9295-eeef0d43a9c8?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews&tid=461e2020-109b-4c43-ad3f-eb9944f5dc44\n"
     ]
    }
   ],
   "source": [
    "pipeline_parameters = {\"model_name\": model_name}\n",
    "tags = {\"trigger\": \"jupyter notebook\",\n",
    "        \"model_architecture\" : \"transfer-learning with ResNext-50\"}\n",
    "\n",
    "# Create an AML Experiment\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    \n",
    "# Submit an Experiment Run using the published pipeline and defined pipeline parameters\n",
    "run = experiment.submit(published_pipeline,\n",
    "                        tags=tags,\n",
    "                        pipeline_parameters=pipeline_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 27299434-dc04-402f-9295-eeef0d43a9c8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/27299434-dc04-402f-9295-eeef0d43a9c8?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews&tid=461e2020-109b-4c43-ad3f-eb9944f5dc44\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 001c6067-c295-4a18-aee5-34aeae1bcc16\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/001c6067-c295-4a18-aee5-34aeae1bcc16?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews&tid=461e2020-109b-4c43-ad3f-eb9944f5dc44\n",
      "StepRun( Train Model ) Status: NotStarted\n",
      "StepRun( Train Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      "========================================================================================================================\n",
      "2021-06-20T15:41:37Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore\n",
      "2021-06-20T15:41:37Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2021-06-20T15:41:38Z Starting output-watcher...\n",
      "2021-06-20T15:41:38Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
      "2021-06-20T15:41:38Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-06-20T15:41:38Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      "2c11b7cecaa5: Pulling fs layer\n",
      "04637fa56252: Pulling fs layer\n",
      "d6e6af23a0f3: Pulling fs layer\n",
      "b4a424de92ad: Pulling fs layer\n",
      "3e5d9ee64909: Pulling fs layer\n",
      "3a846111ff22: Pulling fs layer\n",
      "93a5020c6e19: Pulling fs layer\n",
      "360b353e68fd: Pulling fs layer\n",
      "ea4e2e1810f8: Pulling fs layer\n",
      "def12cf7de15: Pulling fs layer\n",
      "3ae6adfbdb11: Pulling fs layer\n",
      "2a21fbf2232e: Pulling fs layer\n",
      "2ef655776049: Pulling fs layer\n",
      "2b2711cecc87: Pulling fs layer\n",
      "2468e9aec01e: Pulling fs layer\n",
      "92a4e88a49e9: Pulling fs layer\n",
      "a33b32b5286b: Pulling fs layer\n",
      "98c96e7c8036: Pulling fs layer\n",
      "0ca6e388c01d: Pulling fs layer\n",
      "cb0e780a2d44: Pulling fs layer\n",
      "00361b3fc45c: Pulling fs layer\n",
      "b4a424de92ad: Waiting\n",
      "3e5d9ee64909: Waiting\n",
      "3a846111ff22: Waiting\n",
      "93a5020c6e19: Waiting\n",
      "2468e9aec01e: Waiting\n",
      "92a4e88a49e9: Waiting\n",
      "a33b32b5286b: Waiting\n",
      "3ae6adfbdb11: Waiting\n",
      "98c96e7c8036: Waiting\n",
      "2a21fbf2232e: Waiting\n",
      "0ca6e388c01d: Waiting\n",
      "2ef655776049: Waiting\n",
      "cb0e780a2d44: Waiting\n",
      "2b2711cecc87: Waiting\n",
      "00361b3fc45c: Waiting\n",
      "360b353e68fd: Waiting\n",
      "ea4e2e1810f8: Waiting\n",
      "def12cf7de15: Waiting\n",
      "04637fa56252: Verifying Checksum\n",
      "04637fa56252: Download complete\n",
      "d6e6af23a0f3: Verifying Checksum\n",
      "d6e6af23a0f3: Download complete\n",
      "b4a424de92ad: Verifying Checksum\n",
      "b4a424de92ad: Download complete\n",
      "2c11b7cecaa5: Verifying Checksum\n",
      "2c11b7cecaa5: Download complete\n",
      "3a846111ff22: Verifying Checksum\n",
      "3a846111ff22: Download complete\n",
      "3e5d9ee64909: Verifying Checksum\n",
      "3e5d9ee64909: Download complete\n",
      "93a5020c6e19: Verifying Checksum\n",
      "93a5020c6e19: Download complete\n",
      "2c11b7cecaa5: Pull complete\n",
      "04637fa56252: Pull complete\n",
      "d6e6af23a0f3: Pull complete\n",
      "b4a424de92ad: Pull complete\n",
      "360b353e68fd: Download complete\n",
      "def12cf7de15: Verifying Checksum\n",
      "def12cf7de15: Download complete\n",
      "3ae6adfbdb11: Verifying Checksum\n",
      "3ae6adfbdb11: Download complete\n",
      "2ef655776049: Download complete\n",
      "2a21fbf2232e: Verifying Checksum\n",
      "2a21fbf2232e: Download complete\n",
      "2b2711cecc87: Verifying Checksum\n",
      "2b2711cecc87: Download complete\n",
      "2468e9aec01e: Verifying Checksum\n",
      "2468e9aec01e: Download complete\n",
      "ea4e2e1810f8: Verifying Checksum\n",
      "ea4e2e1810f8: Download complete\n",
      "92a4e88a49e9: Verifying Checksum\n",
      "92a4e88a49e9: Download complete\n",
      "98c96e7c8036: Verifying Checksum\n",
      "98c96e7c8036: Download complete\n",
      "0ca6e388c01d: Verifying Checksum\n",
      "0ca6e388c01d: Download complete\n",
      "cb0e780a2d44: Download complete\n",
      "00361b3fc45c: Verifying Checksum\n",
      "00361b3fc45c: Download complete\n",
      "3e5d9ee64909: Pull complete\n",
      "3a846111ff22: Pull complete\n",
      "93a5020c6e19: Pull complete\n",
      "360b353e68fd: Pull complete\n",
      "ea4e2e1810f8: Pull complete\n",
      "def12cf7de15: Pull complete\n",
      "3ae6adfbdb11: Pull complete\n",
      "2a21fbf2232e: Pull complete\n",
      "2ef655776049: Pull complete\n",
      "2b2711cecc87: Pull complete\n",
      "2468e9aec01e: Pull complete\n",
      "92a4e88a49e9: Pull complete\n",
      "a33b32b5286b: Verifying Checksum\n",
      "a33b32b5286b: Download complete\n",
      "a33b32b5286b: Pull complete\n",
      "98c96e7c8036: Pull complete\n",
      "0ca6e388c01d: Pull complete\n",
      "cb0e780a2d44: Pull complete\n",
      "00361b3fc45c: Pull complete\n",
      "Digest: sha256:061c85dd8a554afd2ae1b47f6d3be6cabfa07429d7c0ff4f665a3ef43f33114b\n",
      "Status: Downloaded newer image for 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "2021-06-20T15:42:39Z Check if container 001c6067-c295-4a18-aee5-34aeae1bcc16 already exist exited with 0, \n",
      "\n",
      "0c8d1a829b6477765eef40b5cd50edb72367a5bcd69bbf57fc9f93cf2206731f\n",
      "2021-06-20T15:42:52Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-06-20T15:42:52Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4b3c4383b988c8798beefa30dc4de490-3e1814f0956501fb-01 -sshRequired=false] \n",
      "2021/06/20 15:42:52 Starting App Insight Logger for task:  containerSetup\n",
      "2021/06/20 15:42:52 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/20 15:42:52 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/06/20 15:42:52 Starting infiniband setup\n",
      "2021/06/20 15:42:52 Python Version found is Python 3.7.1\n",
      "\n",
      "2021/06/20 15:42:52 Returning Python Version as 3.7\n",
      "2021/06/20 15:42:52 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/20 15:42:52 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021-06-20T15:42:52Z VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/20 15:42:52 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/06/20 15:42:52 Not setting up Infiniband in Container\n",
      "2021/06/20 15:42:52 Not setting up Infiniband in Container\n",
      "2021-06-20T15:42:52Z Not setting up Infiniband in Container\n",
      "2021/06/20 15:42:52 Python Version found is Python 3.7.1\n",
      "\n",
      "2021/06/20 15:42:52 Returning Python Version as 3.7\n",
      "2021/06/20 15:42:52 sshd inside container not required for job, skipping setup.\n",
      "2021/06/20 15:42:53 All App Insights Logs was send successfully\n",
      "2021/06/20 15:42:53 App Insight Client has already been closed\n",
      "2021/06/20 15:42:53 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-06-20T15:42:53Z Starting docker container succeeded.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/06/20 15:42:59 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/06/20 15:42:59 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/20 15:42:59 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2021/06/20 15:43:00 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2021-06-20T15:43:00.419295] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['pipeline/train_model_step.py', '--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset'])\n",
      "Script type = None\n",
      "[2021-06-20T15:43:02.755482] Entering Run History Context Manager.\n",
      "[2021-06-20T15:43:03.888122] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16\n",
      "[2021-06-20T15:43:03.888373] Preparing to call script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "[2021-06-20T15:43:03.888441] After variable expansion, calling script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "\n",
      "2021/06/20 15:43:04 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "Running train_model_step.py\n",
      "Argument [caller_run_id]: none\n",
      "Argument [dataset_name]: stanford_dogs_dataset\n",
      "Argument [dataset_version]: 1\n",
      "Argument [data_file_path]: none\n",
      "Argument [model_name]: dog_clf_model\n",
      "Argument [step_output]: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/mounts/workspaceblobstore/azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/pipeline_data\n",
      "\n",
      "Getting training parameters\n",
      "Parameters: {'num_epochs': 1, 'batch_size': 8, 'learning_rate': 0.001, 'momentum': 0.9, 'num_frozen_layers': 7, 'num_neurons_fc_layer': 512, 'dropout_prob_fc_layer': 0.0, 'lr_scheduler_step_size': 7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "START MODEL TRAINING\n",
      "--------------------\n",
      "Hyperparameter number of epochs: 1\n",
      "Hyperparameter batch size: 8\n",
      "Hyperparameter learning rate: 0.001\n",
      "Hyperparameter momentum: 0.9\n",
      "Hyperparameter number of frozen layers: 7\n",
      "Hyperparameter number of neurons fc layer: 512\n",
      "Hyperparameter dropout probability fc layer: 0\n",
      "Hyperparameter lr scheduler step size: 7\n",
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
      "\n",
      "  0%|          | 0.00/95.8M [00:00<?, ?B/s]\n",
      "  0%|          | 32.0k/95.8M [00:00<07:41, 217kB/s]\n",
      "  0%|          | 200k/95.8M [00:00<05:49, 287kB/s] \n",
      "  1%|          | 824k/95.8M [00:00<04:08, 401kB/s]\n",
      "  2%|▏         | 1.70M/95.8M [00:00<02:55, 562kB/s]\n",
      "  6%|▌         | 5.82M/95.8M [00:00<01:58, 797kB/s]\n",
      "  9%|▉         | 8.41M/95.8M [00:00<01:21, 1.12MB/s]\n",
      " 14%|█▍        | 13.4M/95.8M [00:00<00:54, 1.59MB/s]\n",
      " 17%|█▋        | 16.0M/95.8M [00:00<00:37, 2.21MB/s]\n",
      " 22%|██▏       | 21.0M/95.8M [00:01<00:25, 3.09MB/s]\n",
      " 25%|██▍       | 23.9M/95.8M [00:01<00:17, 4.21MB/s]\n",
      " 30%|██▉       | 28.5M/95.8M [00:01<00:12, 5.76MB/s]\n",
      " 33%|███▎      | 31.7M/95.8M [00:01<00:08, 7.59MB/s]\n",
      " 38%|███▊      | 36.1M/95.8M [00:01<00:06, 10.0MB/s]\n",
      " 41%|████      | 39.4M/95.8M [00:01<00:04, 12.5MB/s]\n",
      " 46%|████▌     | 43.6M/95.8M [00:01<00:03, 15.8MB/s]\n",
      " 49%|████▉     | 47.0M/95.8M [00:01<00:02, 18.4MB/s]\n",
      " 53%|█████▎    | 51.2M/95.8M [00:02<00:02, 21.9MB/s]\n",
      " 57%|█████▋    | 54.5M/95.8M [00:02<00:01, 23.9MB/s]\n",
      " 61%|██████▏   | 58.7M/95.8M [00:02<00:01, 27.1MB/s]\n",
      " 65%|██████▍   | 62.1M/95.8M [00:02<00:01, 27.9MB/s]\n",
      " 69%|██████▉   | 66.3M/95.8M [00:02<00:01, 30.6MB/s]\n",
      " 73%|███████▎  | 69.6M/95.8M [00:02<00:00, 30.5MB/s]\n",
      " 77%|███████▋  | 73.8M/95.8M [00:02<00:00, 31.5MB/s]\n",
      " 81%|████████  | 77.1M/95.8M [00:02<00:00, 32.4MB/s]\n",
      " 85%|████████▍ | 81.3M/95.8M [00:02<00:00, 34.3MB/s]\n",
      " 88%|████████▊ | 84.8M/95.8M [00:03<00:00, 33.1MB/s]\n",
      " 93%|█████████▎| 88.9M/95.8M [00:03<00:00, 34.7MB/s]\n",
      " 96%|█████████▋| 92.3M/95.8M [00:03<00:00, 33.1MB/s]\n",
      "100%|██████████| 95.8M/95.8M [00:03<00:00, 29.9MB/s]\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "--------------------\n",
      "Train Loss: 1.7348 Train Acc: 0.5328\n",
      "Val Loss: 0.5840 Val Acc: 0.8262\n",
      "--------------------\n",
      "Training completed in 6m 39s\n",
      "Best validation accuracy: 0.826151\n",
      "Logging model metrics\n",
      "Metrics {'test_acc': 0.8064102564102564}\n",
      "The following tags are now present: {'azureml.nodeid': '4666da05', 'azureml.pipeline': '27299434-dc04-402f-9295-eeef0d43a9c8', 'azureml.pipelinerunid': '27299434-dc04-402f-9295-eeef0d43a9c8', 'azureml.pipelineComponent': 'masterescloud', '_aml_system_ComputeTargetStatus': '{\"AllocationState\":\"steady\",\"PreparingNodeCount\":0,\"RunningNodeCount\":0,\"CurrentNodeCount\":0}', 'trigger': 'pipeline'}.\n",
      "\n",
      "\n",
      "[2021-06-20T15:56:18.794999] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "4 items cleaning up...\n",
      "Cleanup took 0.3210875988006592 seconds\n",
      "[2021-06-20T15:56:19.354177] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      "===============================================================================================================\n",
      "[2021-06-20T15:56:30.555142] Entering job release\n",
      "[2021-06-20T15:56:32.057732] Starting job release\n",
      "[2021-06-20T15:56:32.058226] Logging experiment finalizing status in history service.[2021-06-20T15:56:32.058410] job release stage : upload_datastore starting...\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 2263\n",
      "[2021-06-20T15:56:32.058683] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "\n",
      "[2021-06-20T15:56:32.059447] job release stage : execute_job_release starting...\n",
      "[2021-06-20T15:56:32.061502] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-06-20T15:56:32.061769] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-06-20T15:56:32.068706] Entering context manager injector.\n",
      "[2021-06-20T15:56:32.159218] job release stage : upload_datastore completed...\n",
      "[2021-06-20T15:56:32.221715] job release stage : send_run_telemetry starting...\n",
      "[2021-06-20T15:56:32.330422] job release stage : execute_job_release completed...\n",
      "[2021-06-20T15:56:32.420112] get vm size and vm region successfully.\n",
      "[2021-06-20T15:56:32.609532] get compute meta data successfully.\n",
      "[2021-06-20T15:56:32.937940] post artifact meta request successfully.\n",
      "[2021-06-20T15:56:32.973707] upload compute record artifact successfully.\n",
      "[2021-06-20T15:56:32.973826] job release stage : send_run_telemetry completed...\n",
      "[2021-06-20T15:56:32.974197] Job release is complete\n",
      "\n",
      "StepRun(Train Model) Execution Summary\n",
      "=======================================\n",
      "StepRun( Train Model ) Status: Finished\n",
      "{'runId': '001c6067-c295-4a18-aee5-34aeae1bcc16', 'target': 'gpu-cluster', 'status': 'Completed', 'startTimeUtc': '2021-06-20T15:41:36.751784Z', 'endTimeUtc': '2021-06-20T15:56:45.428112Z', 'properties': {'azureml.git.repository_uri': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'mlflow.source.git.repoURL': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'mlflow.source.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'azureml.git.dirty': 'True', 'ContentSnapshotId': '7297ec36-dfc8-463a-ac09-ce18cbdda450', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'b26b1509-d5ed-4c36-b60f-f79b23c53bb9', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '4666da05', 'azureml.pipelinerunid': '27299434-dc04-402f-9295-eeef0d43a9c8', 'azureml.pipelineid': '49915db5-5176-4292-9c1f-e017c4d68753', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '818032c8-9681-4678-bd7f-710db63b9a95'}, 'consumptionDetails': {'type': 'Reference'}}], 'outputDatasets': [], 'runDefinition': {'script': 'pipeline/train_model_step.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--model_name', '$AML_PARAMETER_model_name', '--step_output', '$AZUREML_DATAREFERENCE_pipeline_data', '--dataset_version', '$AML_PARAMETER_dataset_version', '--data_file_path', '$AML_PARAMETER_data_file_path', '--caller_run_id', '$AML_PARAMETER_caller_run_id', '--dataset_name', '$AML_PARAMETER_dataset_name'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster', 'dataReferences': {'pipeline_data': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/001c6067-c295-4a18-aee5-34aeae1bcc16/pipeline_data', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'stanford-dogs-train-env', 'version': '1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge', 'pytorch'], 'dependencies': ['joblib=0.13.2', 'matplotlib=3.3.3', 'python=3.7.1', 'pytorch::pytorch=1.7.0', 'pytorch::torchvision=0.8.1', 'scipy=1.6.0', 'tqdm=4.38.0', {'pip': ['azure-cli', 'azureml-core==1.20.0', 'azureml-defaults', 'azureml-sdk', 'azureml-widgets', 'ipykernel', 'python-dotenv==0.15.0']}], 'name': 'azureml_3a65fd49f096be6cbe5921e5dcc78936'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20201113.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {'AML_PARAMETER_model_name': 'dog_clf_model', 'AML_PARAMETER_dataset_version': '1', 'AML_PARAMETER_data_file_path': 'none', 'AML_PARAMETER_caller_run_id': 'none', 'AML_PARAMETER_dataset_name': 'stanford_dogs_dataset'}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=KIUOdx4MvBj3l%2BDTe0LE8febn4rHO4x1ngXiriLiFMA%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'azureml-logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=RnVTwK3lI0GxkEHxgC4y3w5%2F2ZFArKptaDzfeS%2BP780%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=xL60cqH13Do%2FuDElkJcDT%2B3Mpm3pQI4Od7IUzqA98mM%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=%2BSOb6Pdio8J8f%2F4IqdZX%2F38T2BeyOnRQxz%2Bn2n%2FdKTE%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'azureml-logs/process_info.json': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=3Z0itp6ls2yLeY%2BgpymzL0XTWPKWwRKrPMfnUt%2FMA%2B0%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'azureml-logs/process_status.json': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=xDDqYBzX0ih6c0O9SC2bULuWZv8fojGIZz1vpCYKqlE%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/119_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/119_azureml.log?sv=2019-02-02&sr=b&sig=rD27WipiVri6OT%2Brw64cC3q27zSoQjyOSJuBgDPhdaw%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=wKRg4YrTgSbaj88%2FjyDGRqhCaljsl74rstk696zmeuo%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.0.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/backgroundProcess_Telemetry.0.log?sv=2019-02-02&sr=b&sig=UMnUoaINkF3MCaSDtp89a2h752MJBeARSQv4rG64wag%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=w0yD%2F2DD%2BzbngWjqPS3y4v1rWBpCxw5AirjMFI7MvXw%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/engine_spans_02afed3d-e45d-49c1-99ce-ff71d1ce90fc.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/engine_spans_02afed3d-e45d-49c1-99ce-ff71d1ce90fc.jsonl?sv=2019-02-02&sr=b&sig=qmAYT2OKw6Ej1AxHBwo3VR0Jd8je3tC7PbBv5WDmiys%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/python_span_02afed3d-e45d-49c1-99ce-ff71d1ce90fc.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/python_span_02afed3d-e45d-49c1-99ce-ff71d1ce90fc.jsonl?sv=2019-02-02&sr=b&sig=XsMz1w4ECzuTjSTaaYclXePkXtZKn8ud06XZisii12I%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/python_span_431ae6a8-cd28-4a82-9b07-55c4f0bd3df5.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/python_span_431ae6a8-cd28-4a82-9b07-55c4f0bd3df5.jsonl?sv=2019-02-02&sr=b&sig=ekW0dus0ay6qsh0UFKHd%2BxiVAhRwPjsoWyJEH0Mx30w%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/dataprep/python_span_9e986bce-96f5-4339-ad69-d584fce5850b.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/dataprep/python_span_9e986bce-96f5-4339-ad69-d584fce5850b.jsonl?sv=2019-02-02&sr=b&sig=AoUWHRXckzwIEnKI%2FVY7xJEVY5y%2BR8zGaWvKMwhefZM%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=CsPVt1ehpnU7GirT46JMvyS0dwVUPL0YxrFqX5jKIbQ%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=e9frJswdDSN5B1Z2hWU7Jt8nrZoQGWJSCGMxO4y4%2BgU%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=Ne6v3%2FQE9trUug4c%2FAt8m3aIx5Td4v5%2Ba4tqgK2aVP0%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=HacNLT6R1os8daG%2FcGstx3qvW6GzevJvdJTCWIpazGg%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.001c6067-c295-4a18-aee5-34aeae1bcc16/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=WHVdCFpGWvGZQAdQjCWbzYzxwOD8UkPeoWw4PeIQ%2Bao%3D&st=2021-06-20T15%3A46%3A38Z&se=2021-06-20T23%3A56%3A38Z&sp=r'}, 'submittedBy': 's.birk@avanade.com Birk'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: 77b422de-66fc-47eb-8387-5f339eaa2ee5\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/77b422de-66fc-47eb-8387-5f339eaa2ee5?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews&tid=461e2020-109b-4c43-ad3f-eb9944f5dc44\n",
      "StepRun( Evaluate Model ) Status: NotStarted\n",
      "StepRun( Evaluate Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      "========================================================================================================================\n",
      "2021-06-20T15:57:00Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore\n",
      "2021-06-20T15:57:00Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2021-06-20T15:57:01Z Starting output-watcher...\n",
      "2021-06-20T15:57:01Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
      "2021-06-20T15:57:02Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-06-20T15:57:02Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      "Digest: sha256:061c85dd8a554afd2ae1b47f6d3be6cabfa07429d7c0ff4f665a3ef43f33114b\n",
      "Status: Image is up to date for 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "2021-06-20T15:57:02Z Check if container 77b422de-66fc-47eb-8387-5f339eaa2ee5 already exist exited with 0, \n",
      "\n",
      "65666184e1a9de2dbfdcc35ae82f9976a7cf4ad392746e7a79e7fbe54512a66b\n",
      "2021-06-20T15:57:03Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-06-20T15:57:03Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-28f91bb140097ccb53423839f4b8fac4-02ecc253e9555fd7-01 -sshRequired=false] \n",
      "2021/06/20 15:57:03 Starting App Insight Logger for task:  containerSetup\n",
      "2021/06/20 15:57:03 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/20 15:57:03 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/06/20 15:57:03 Starting infiniband setup\n",
      "2021/06/20 15:57:03 Python Version found is Python 3.7.1\n",
      "\n",
      "2021/06/20 15:57:03 Returning Python Version as 3.7\n",
      "2021/06/20 15:57:03 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/20 15:57:03 VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021-06-20T15:57:03Z VMSize: standard_nc6, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
      "2021/06/20 15:57:03 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/06/20 15:57:03 Not setting up Infiniband in Container\n",
      "2021/06/20 15:57:03 Not setting up Infiniband in Container\n",
      "2021-06-20T15:57:03Z Not setting up Infiniband in Container\n",
      "2021/06/20 15:57:03 Python Version found is Python 3.7.1\n",
      "\n",
      "2021/06/20 15:57:03 Returning Python Version as 3.7\n",
      "2021/06/20 15:57:03 sshd inside container not required for job, skipping setup.\n",
      "2021/06/20 15:57:03 All App Insights Logs was send successfully\n",
      "2021/06/20 15:57:03 App Insight Client has already been closed\n",
      "2021/06/20 15:57:03 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-06-20T15:57:03Z Starting docker container succeeded.\n",
      "2021-06-20T15:57:08Z Job environment preparation succeeded on 10.0.0.5. Output: \n",
      ">>>   2021/06/20 15:56:59 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/06/20 15:56:59 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      ">>>   2021/06/20 15:56:59 runtime.GOOS linux\n",
      ">>>   2021/06/20 15:56:59 Checking if '/tmp' exists\n",
      ">>>   2021/06/20 15:56:59 Reading dyanamic configs\n",
      ">>>   2021/06/20 15:56:59 Container sas url: https://baiscriptswesteuropeprod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=9UBH7ig8b9NIeIkNQpNxDmP7wUMtSqFoIE5AY22cheE%3D\n",
      ">>>   2021/06/20 15:56:59 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: no such file or directory\n",
      ">>>   2021/06/20 15:56:59 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installer on host: false. Is Azsecpack installation enabled: false,\n",
      ">>>   2021/06/20 15:56:59 Starting Azsecpack installation on machine: 18a58f8aff9142a283bf1d18fcf66aa8000001#461e2020-109b-4c43-ad3f-eb9944f5dc44#bf088f59-f015-4332-bd36-54b988be7c90#amlbrikserg#amlbriksews#gpu-cluster\n",
      ">>>   2021/06/20 15:56:59 Is Azsecpack enabled: false, GetDisableVsatlsscan: true\n",
      ">>>   2021/06/20 15:56:59 Turning off azsecpack, if it is already running\n",
      ">>>   2021/06/20 15:56:59 [doTurnOffAzsecpack] output:Unit mdsd.service could not be found.\n",
      ">>>   ,err:exit status 1.\n",
      ">>>   2021/06/20 15:56:59 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/06/20 15:56:59 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/06/20 15:56:59 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/06/20 15:56:59 GPU : GPU 0: Tesla K80 (UUID: GPU-574265d4-d829-7e0a-e8f0-bf4f84173460)\n",
      ">>>   2021/06/20 15:56:59 GPU count found on the node: 1\n",
      ">>>   2021/06/20 15:56:59 Mellanox Inbox drivers found (implying presence of SR-IOV)?: false\n",
      ">>>   2021/06/20 15:56:59 Disabling IB for NCCL.\n",
      ">>>   2021/06/20 15:56:59 AMLComputeXDSEndpoint:  https://westeurope-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/06/20 15:56:59 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/06/20 15:56:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config\n",
      ">>>   2021/06/20 15:56:59 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/06/20 15:56:59 Starting identity responder.\n",
      ">>>   2021/06/20 15:56:59 Starting identity responder.\n",
      ">>>   2021/06/20 15:56:59 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/06/20 15:56:59 Logfile used for identity responder: /mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/IdentityResponderLog-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      ">>>   2021/06/20 15:56:59 Logfile used for identity responder: /mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/IdentityResponderLog-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      ">>>   2021/06/20 15:56:59 Started Identity Responder for job.\n",
      ">>>   2021/06/20 15:56:59 Started Identity Responder for job.\n",
      ">>>   2021/06/20 15:56:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd\n",
      ">>>   2021/06/20 15:56:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/shared\n",
      ">>>   2021/06/20 15:56:59 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/20 15:56:59 Mounting job level file systems\n",
      ">>>   2021/06/20 15:56:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts\n",
      ">>>   2021/06/20 15:56:59 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/06/20 15:56:59 Datastore credentials file not found, skipping.\n",
      ">>>   2021/06/20 15:56:59 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.master.runtimesastokens\n",
      ">>>   2021/06/20 15:56:59 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/06/20 15:56:59 No NFS configured\n",
      ">>>   2021/06/20 15:56:59 No Azure File Shares configured\n",
      ">>>   2021/06/20 15:56:59 Mounting blob file systems\n",
      ">>>   2021/06/20 15:56:59 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/06/20 15:56:59 Mounting azureml-blobstore-3d5545b1-5c4c-4954-8d38-23156fa90536 container from amlbriksews9265001959 account at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore\n",
      ">>>   2021/06/20 15:56:59 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/20 15:56:59 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/20 15:56:59 Blobfuse cache size set to 310607 MB.\n",
      ">>>   2021/06/20 15:56:59 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=310607 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/06/20 15:56:59 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore\n",
      ">>>   2021/06/20 15:57:00 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore\n",
      ">>>   2021/06/20 15:57:00 Successfully mounted azureml-blobstore-3d5545b1-5c4c-4954-8d38-23156fa90536 container from amlbriksews9265001959 account at /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore\n",
      ">>>   2021/06/20 15:57:00 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5\n",
      ">>>   2021/06/20 15:57:00 No unmanaged file systems configured\n",
      ">>>   2021/06/20 15:57:00 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/06/20 15:57:00 GPU : GPU 0: Tesla K80 (UUID: GPU-574265d4-d829-7e0a-e8f0-bf4f84173460)\n",
      ">>>   2021/06/20 15:57:00 Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ">>>   . Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      ">>>   2021/06/20 15:57:00 Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ">>>   . Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      ">>>   2021/06/20 15:57:01 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/20 15:57:01 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs\n",
      ">>>   2021/06/20 15:57:01 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/logs\n",
      ">>>   2021/06/20 15:57:01 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/outputs\n",
      ">>>   2021/06/20 15:57:01 Starting output-watcher...\n",
      ">>>   2021/06/20 15:57:01 Single file input dataset is enabled.\n",
      ">>>   2021/06/20 15:57:01 Start to pulling docker image: 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      ">>>   2021/06/20 15:57:01 Start pull docker image: 3d5545b15c4c49548d3823156fa90536.azurecr.io\n",
      ">>>   2021/06/20 15:57:01 Getting credentials for image 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043 with url 3d5545b15c4c49548d3823156fa90536.azurecr.io\n",
      ">>>   2021/06/20 15:57:01 Container registry is ACR.\n",
      ">>>   2021/06/20 15:57:01 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/06/20 15:57:01 Getting ACR Credentials from EMS for environment stanford-dogs-train-env:1\n",
      ">>>   2021/06/20 15:57:01 Requesting XDS for registry details.\n",
      ">>>   2021/06/20 15:57:01 Attempt 1 of http call to https://westeurope-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourceGroups/amlbrikserg/workspaces/amlbriksews/clusters/gpu-cluster/nodes/tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p?api-version=2018-02-01\n",
      ">>>   2021/06/20 15:57:02 Got container registry details from credentials service for registry address: 3d5545b15c4c49548d3823156fa90536.azurecr.io.\n",
      ">>>   2021/06/20 15:57:02 Writing ACR Details to file...\n",
      ">>>   2021/06/20 15:57:02 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/06/20 15:57:02 Executing 'Copy ACR Details file' on 10.0.0.5\n",
      ">>>   2021/06/20 15:57:02 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/06/20 15:57:02 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/06/20 15:57:02 Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/06/20 15:57:02 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/06/20 15:57:02 EMS returned 3d5545b15c4c49548d3823156fa90536.azurecr.io for environment stanford-dogs-train-env\n",
      ">>>   2021/06/20 15:57:02 Save docker credentials for image 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043 in /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd/docker_login_6F797B3945147B96\n",
      ">>>   2021/06/20 15:57:02 Start login to the docker registry\n",
      ">>>   2021/06/20 15:57:02 Successfully logged into the docker registry.\n",
      ">>>   2021/06/20 15:57:02 Start run pull docker image command\n",
      ">>>   2021/06/20 15:57:02 Pull docker image succeeded.\n",
      ">>>   2021/06/20 15:57:02 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd/docker_login_6F797B3945147B96\n",
      ">>>   2021/06/20 15:57:02 Pull docker image time: 1.242059886s\n",
      ">>>   \n",
      ">>>   2021/06/20 15:57:02 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/06/20 15:57:02 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/06/20 15:57:02 GPU : GPU 0: Tesla K80 (UUID: GPU-574265d4-d829-7e0a-e8f0-bf4f84173460)\n",
      ">>>   2021/06/20 15:57:02 Setting the memory limit for docker container to be 55987 MB\n",
      ">>>   2021/06/20 15:57:02 The env variable file size is 39499 bytes\n",
      ">>>   2021/06/20 15:57:02 Creating parent cgroup '77b422de-66fc-47eb-8387-5f339eaa2ee5' for Containers used in Job\n",
      ">>>   2021/06/20 15:57:02 Add parent cgroup '77b422de-66fc-47eb-8387-5f339eaa2ee5' to container '77b422de-66fc-47eb-8387-5f339eaa2ee5'\n",
      ">>>   2021/06/20 15:57:02 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/06/20 15:57:02 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,77b422de-66fc-47eb-8387-5f339eaa2ee5,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,--gpus,all,-m,55987m,-v,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.batchai.envlist,--cgroup-parent=/77b422de-66fc-47eb-8387-5f339eaa2ee5/,--shm-size,2g\n",
      ">>>   2021/06/20 15:57:02 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/06/20 15:57:02 the binding /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5 \n",
      ">>>   2021/06/20 15:57:02 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,77b422de-66fc-47eb-8387-5f339eaa2ee5,--gpus,all,-m,55987m,-w,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.batchai.envlist,--cgroup-parent=/77b422de-66fc-47eb-8387-5f339eaa2ee5/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5,-v,/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd,-v,/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs\n",
      ">>>   2021/06/20 15:57:02 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name 77b422de-66fc-47eb-8387-5f339eaa2ee5 --gpus all -m 55987m -w /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/config/.batchai.envlist --cgroup-parent=/77b422de-66fc-47eb-8387-5f339eaa2ee5/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5:/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5 -v /mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd -v /mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs:/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/certs -d -it --privileged --net=host 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      ">>>   2021/06/20 15:57:02 Check if container 77b422de-66fc-47eb-8387-5f339eaa2ee5 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/20 15:57:02 Check if container 77b422de-66fc-47eb-8387-5f339eaa2ee5 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/20 15:57:03 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/06/20 15:57:03 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/06/20 15:57:03 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-28f91bb140097ccb53423839f4b8fac4-02ecc253e9555fd7-01 -sshRequired=false] \n",
      ">>>   2021/06/20 15:57:03 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-28f91bb140097ccb53423839f4b8fac4-02ecc253e9555fd7-01 -sshRequired=false] \n",
      ">>>   2021/06/20 15:57:03 Container ssh is not required for job type.\n",
      ">>>   2021/06/20 15:57:03 Starting docker container succeeded.\n",
      ">>>   2021/06/20 15:57:03 Starting docker container succeeded.\n",
      ">>>   2021/06/20 15:57:03 Disk space after starting docker container: 317941MB\n",
      ">>>   2021/06/20 15:57:03 Begin execution of runSpecialJobTask\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5-setup/job_prep.py --snapshots '[{\"Id\":\"7297ec36-dfc8-463a-ac09-ce18cbdda450\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml_compute_logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      ">>>   2021/06/20 15:57:03 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5;/azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5-setup/job_prep.py --snapshots '[{\"Id\":\"7297ec36-dfc8-463a-ac09-ce18cbdda450\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/06/20 15:57:03 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-28f91bb140097ccb53423839f4b8fac4-b7da883ecd25e59d-01 -t 77b422de-66fc-47eb-8387-5f339eaa2ee5 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/d2fb774b-d8b7-47da-b405-b8ef3a4e7369/job-1/77b422de-66fc-47eb-8_9f705bb8-bc7f-4199-9334-c9bd56257ac1/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5;/azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5/mounts/workspaceblobstore/azureml/77b422de-66fc-47eb-8387-5f339eaa2ee5-setup/job_prep.py --snapshots '[{\"Id\":\"7297ec36-dfc8-463a-ac09-ce18cbdda450\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/20 15:57:04 Attempt 1 of http call to https://westeurope.api.azureml.ms/history/v1.0/private/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourceGroups/amlbrikserg/providers/Microsoft.MachineLearningServices/workspaces/amlbriksews/runs/77b422de-66fc-47eb-8387-5f339eaa2ee5/spans\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:03.866578] Entering job preparation.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.355874] Starting job preparation.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.355913] Extracting the control code.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.373494] fetching and extracting the control code on master node.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.373518] Starting extract_project.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.373553] Starting to extract zip file.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:05.847605] Finished extracting zip file.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:06.001977] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:06.002027] Start fetching snapshots.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:06.002064] Start fetching snapshot.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:06.002077] Retrieving project from snapshot: 7297ec36-dfc8-463a-ac09-ce18cbdda450\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 53\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.638590] Finished fetching snapshot.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.638618] Finished fetching snapshots.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.638674] Finished extract_project.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.651074] Finished fetching and extracting the control code.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.655379] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.656736] Start run_history_prep.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.697665] Entering context manager injector.\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.742373] downloadDataStore completed\n",
      ">>>   2021/06/20 15:57:07 runSpecialJobTask: preparation: [2021-06-20T15:57:07.743926] Job preparation is complete.\n",
      ">>>   2021/06/20 15:57:07 Execution of runSpecialJobTask completed\n",
      ">>>   2021/06/20 15:57:07 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/06/20 15:57:07 Process Exiting with Code:  0\n",
      ">>>   2021/06/20 15:57:08 All App Insights Logs was send successfully\n",
      ">>>   \n",
      "2021-06-20T15:57:08Z 127.0.0.1 slots=1 max-slots=1\n",
      "2021-06-20T15:57:08Z launching Custom job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt\n",
      "===============================================================================================================\n",
      "[2021-06-20T15:57:24.231521] Entering job release\n",
      "[2021-06-20T15:57:25.740453] Starting job release\n",
      "[2021-06-20T15:57:25.746777] Logging experiment finalizing status in history service.[2021-06-20T15:57:25.747372] job release stage : upload_datastore starting...\n",
      "[2021-06-20T15:57:25.747541] job release stage : start importing azureml.history._tracking in run_history_release.Starting the daemon thread to refresh tokens in background for process with pid = 197\n",
      "\n",
      "\n",
      "[2021-06-20T15:57:25.747888] job release stage : execute_job_release starting...[2021-06-20T15:57:25.747977] job release stage : copy_batchai_cached_logs starting...\n",
      "\n",
      "[2021-06-20T15:57:25.749915] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-06-20T15:57:25.750483] Entering context manager injector.\n",
      "[2021-06-20T15:57:25.758712] job release stage : upload_datastore completed...\n",
      "[2021-06-20T15:57:25.913249] job release stage : send_run_telemetry starting...\n",
      "[2021-06-20T15:57:26.002259] job release stage : execute_job_release completed...\n",
      "[2021-06-20T15:57:26.183657] get vm size and vm region successfully.\n",
      "[2021-06-20T15:57:26.473623] get compute meta data successfully.\n",
      "[2021-06-20T15:57:26.704282] post artifact meta request successfully.\n",
      "[2021-06-20T15:57:26.729254] upload compute record artifact successfully.\n",
      "[2021-06-20T15:57:26.729330] job release stage : send_run_telemetry completed...\n",
      "[2021-06-20T15:57:26.729563] Job release is complete\n",
      "\n",
      "StepRun(Evaluate Model) Execution Summary\n",
      "==========================================\n",
      "StepRun( Evaluate Model ) Status: Finished\n",
      "{'runId': '77b422de-66fc-47eb-8387-5f339eaa2ee5', 'target': 'gpu-cluster', 'status': 'Completed', 'startTimeUtc': '2021-06-20T15:56:59.144757Z', 'endTimeUtc': '2021-06-20T15:57:37.815029Z', 'properties': {'azureml.git.repository_uri': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'mlflow.source.git.repoURL': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'mlflow.source.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'azureml.git.dirty': 'True', 'ContentSnapshotId': '7297ec36-dfc8-463a-ac09-ce18cbdda450', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '48b0959e-b6b0-4b6c-828e-85e2ba63fe5d', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'f902a643', 'azureml.pipelinerunid': '27299434-dc04-402f-9295-eeef0d43a9c8', 'azureml.pipelineid': '49915db5-5176-4292-9c1f-e017c4d68753', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [], 'runDefinition': {'script': 'pipeline/evaluate_model_step.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--model_name', '$AML_PARAMETER_model_name', '--allow_run_cancel', 'True'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'stanford-dogs-train-env', 'version': '1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge', 'pytorch'], 'dependencies': ['joblib=0.13.2', 'matplotlib=3.3.3', 'python=3.7.1', 'pytorch::pytorch=1.7.0', 'pytorch::torchvision=0.8.1', 'scipy=1.6.0', 'tqdm=4.38.0', {'pip': ['azure-cli', 'azureml-core==1.20.0', 'azureml-defaults', 'azureml-sdk', 'azureml-widgets', 'ipykernel', 'python-dotenv==0.15.0']}], 'name': 'azureml_3a65fd49f096be6cbe5921e5dcc78936'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20201113.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {'AML_PARAMETER_model_name': 'dog_clf_model'}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/55_azureml-execution-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=XMt5%2FkCsiOxjzm%2F1b5F0GG1LtUmllu1wodtPdQGk%2BXA%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'azureml-logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/65_job_prep-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=ZOvWBm8uSXtteLRrVnjey0k3D3m5YVAlmPy757OIQy4%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=Kl8fX33jKYiBa%2Fc5a%2FrWEsrGTPDttcri4RW8G2x8npY%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/75_job_post-tvmps_6f91b9a188a1747840567a950ceb1809ea6559d0865cd6a251557efbbada5fea_p.txt?sv=2019-02-02&sr=b&sig=saXHTHp0TLQICEAcOXSPtaayjValz3B4aGG6PFKB7XI%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'azureml-logs/process_info.json': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=r4P4zRqy3mzsKcfJpHE3%2B6OzKfShOJBJA%2B1n8Wc2OAI%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'azureml-logs/process_status.json': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=iPAT4aqTlX6bbscGLArFpZ%2BUmjQVK7Gng7i9XgtWZS8%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/108_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/108_azureml.log?sv=2019-02-02&sr=b&sig=nzzNFZ6%2BLl9bzCLh%2FjlPiUzKO8Z%2FLZKuWtzlxQg4LTI%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/dataprep/python_span_136d3757-c993-48b1-980b-a7c8ff45e8e9.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/dataprep/python_span_136d3757-c993-48b1-980b-a7c8ff45e8e9.jsonl?sv=2019-02-02&sr=b&sig=iwp%2FXhR81F%2FDyegp4WX%2BQW4zVT9US0rkhamiecTQel4%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/dataprep/python_span_4b6c0cc5-3e94-4c05-8c8b-9ea57da1ebf3.jsonl': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/dataprep/python_span_4b6c0cc5-3e94-4c05-8c8b-9ea57da1ebf3.jsonl?sv=2019-02-02&sr=b&sig=smUeuQgFm7SqEZ3xejLof%2BsYTT7MTVFiM4rgYJ0YgLY%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=N9vOrzeSCPWQc5G4pG6S9ZHV%2BhpkOb0a0Wq9mdPJR4U%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=sazqDpyW4MdHCir%2BCEGU%2Bm4Kk4%2B6FjU%2B1wuX%2F%2FWf24Q%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=kVbtvxMQrLM5rZAqBzv8X%2FVJbNjFs6wyqKDikzruZ30%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=4EMCex0YP9llqpy4WqcmmmnvqbkGl8kUsHqbmGEbdls%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.77b422de-66fc-47eb-8387-5f339eaa2ee5/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=%2FhxfeJNKL9DcFEKR8p1LKo0GNAwaNjiMYvxz2kaic00%3D&st=2021-06-20T15%3A47%3A31Z&se=2021-06-20T23%3A57%3A31Z&sp=r'}, 'submittedBy': 's.birk@avanade.com Birk'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Canceled\n",
      "{'runId': '27299434-dc04-402f-9295-eeef0d43a9c8', 'status': 'Canceled', 'startTimeUtc': '2021-06-20T15:37:34.350358Z', 'endTimeUtc': '2021-06-20T15:57:39.771153Z', 'properties': {'azureml.git.repository_uri': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'mlflow.source.git.repoURL': 'https://github.com/sebastianbirk/pytorch-mlops-template-azure-ml.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'mlflow.source.git.commit': 'a95bf69e33a851055c6928f725556ea44b42f5d7', 'azureml.git.dirty': 'True', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"model_name\":\"dog_clf_model\",\"dataset_version\":\"1\",\"data_file_path\":\"none\",\"caller_run_id\":\"none\",\"dataset_name\":\"stanford_dogs_dataset\"}', 'azureml.pipelineid': '49915db5-5176-4292-9c1f-e017c4d68753'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.27299434-dc04-402f-9295-eeef0d43a9c8/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=mDm%2FOUm0TiW1XFMTn7%2F%2FwrVhqtRg6%2BkEBFbK01uNF9c%3D&st=2021-06-20T15%3A41%3A08Z&se=2021-06-20T23%3A51%3A08Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.27299434-dc04-402f-9295-eeef0d43a9c8/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=b5PEx8nLLX60mLTOaj12wJTNHLKBAn4S9HH%2BQBvcNWg%3D&st=2021-06-20T15%3A41%3A08Z&se=2021-06-20T23%3A51%3A08Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlbriksews9265001959.blob.core.windows.net/azureml/ExperimentRun/dcid.27299434-dc04-402f-9295-eeef0d43a9c8/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=3ilfms0kTNkHq0r2bZ%2BrDQBtXW%2FH101Wyn%2B20O6l%2F%2Fc%3D&st=2021-06-20T15%3A41%3A08Z&se=2021-06-20T23%3A51%3A08Z&sp=r'}, 'submittedBy': 's.birk@avanade.com Birk'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Canceled'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for completion of the run and show output log\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to delete the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
