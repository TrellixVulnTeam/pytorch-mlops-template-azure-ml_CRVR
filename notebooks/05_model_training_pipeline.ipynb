{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Notebook Summary](#Notebook-Summary)\n",
    "* [Setup](#Setup)\n",
    "    * [Notebook Parameters](#Notebook-Parameters)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Pipeline Run Configuration & Environment](#Pipeline-Run-Configuration-&-Environment)\n",
    "* [Pipeline Inputs](#Pipeline-Inputs)\n",
    "* [Create Pipeline](#Create-Pipeline)\n",
    "    * [Training Step](#Training-Step)\n",
    "    * [Evaluate Step](#Evaluate-Step)\n",
    "    * [Register Step](#Register-Step)\n",
    "* [Publish Pipeline](#Publish-Pipeline)\n",
    "* [Run Pipeline](#Run-Pipeline)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, an Azure Machine Learning (AML) training / retraining pipeline will be built and published. After building and publishing the pipeline, a REST endpoint can be used to trigger the pipeline from any HTTP library on any platform. This pipeline will be used in the MlOps process for continuous model retraining, e.g. when data or model drift is detected or in general when the model should be retrained. A pipeline gives a more operationalizable way of training than a script run (which was used for original model training in the `02_model_training` notebook) as it can be easily automated and run based on triggers. It also allows for chaining of different steps that can then be executed sequentially. In general, machine learning pipelines help to optimize the workflow in terms of speed, portability and reuse.\n",
    "\n",
    "The training / retraining pipeline built in this notebook will consist of three different steps that are executed sequentially:\n",
    "- Model training using the same code for training as in the `02_model_training` notebook\n",
    "- Model evaluation (comparing the newly trained model with the model currently in production or with a manual threshold)\n",
    "- Model registration (registering the newly trained model to the AML workspace based on the outcomes of the model evaluation)\n",
    "\n",
    "Check out the [AML Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines) for more info on how to build pipelines in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import azureml\n",
    "from azureml.core import Dataset, Datastore, Environment, Experiment, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the remote compute target cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "# Define the name of the training environment created in the 00_environment_setup notebook\n",
    "env_name = \"stanford-dogs-train-env\"\n",
    "\n",
    "# Determine whether the pipeline training run should be evaluated before model registration\n",
    "run_evaluation = True\n",
    "\n",
    "# Define the pipeline endpoint name\n",
    "pipeline_name = \"dog classification model training pipeline\"\n",
    "\n",
    "# Define the pipeline endpoint version\n",
    "pipeline_version = \"1.1\"\n",
    "\n",
    "# Define the model_name\n",
    "model_name = \"dog-classification-model\"\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"stanford-dogs-classifier-train-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the AML workspace, a workspace object needs to be instantiated using the AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a remote compute target to run the pipeline experiments on. The below code will first check whether a compute target with name **cluster_name** (defined in the [Notebook Parameters](#Notebook-Parameters) section) already exists and if it does, will retrieve it. Otherwise it will create a new compute cluster.\n",
    "\n",
    "**Note**: At the moment it is not possible to create a new compute cluster so please specify the name of an existing compute cluster.\n",
    "\n",
    "AML pipelines need to be run on a remote compute target and cannot be run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 4, 'targetNodeCount': 4, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 4, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-03-14T07:57:15.668000+00:00', 'errors': None, 'creationTime': '2021-02-28T09:03:28.414676+00:00', 'modifiedTime': '2021-02-28T09:03:44.147067+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT2400S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Configuration & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model training environment that has been registered as part of the `00_environment_setup` notebook and use it for the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline run configuration containing the retrieved environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PipelineData object to pass data between steps.\n",
    "\n",
    "While here the pipeline will consist of a single step only, a usual flow with multiple steps will include:\n",
    "- Using Dataset objects as inputs to fetch raw data, performing some transformations, then outputting a PipelineData object.\n",
    "- Use the previous step's PipelineData output object as an input object, repeated for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = PipelineData(\"pipeline_data\", datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PipelineParameter objects to be able to pass versatile arguments to the PythonScriptSteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_param = PipelineParameter(name=\"dataset_name\", default_value=\"fowl-dataset\")\n",
    "dataset_version_param = PipelineParameter(name=\"dataset_version\", default_value=1)\n",
    "data_file_path_param = PipelineParameter(name=\"data_file_path\", default_value=\"none\")\n",
    "model_name_param = PipelineParameter(name=\"model_name\", default_value=\"fowl-model\")\n",
    "caller_run_id_param = PipelineParameter(name=\"caller_run_id\", default_value=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a pipeline, the individual steps need to be created first.\n",
    "\n",
    "A pipeline step is an object that encapsulates everything that is needed for running a pipeline including:\n",
    "\n",
    "- environment and dependency settings\n",
    "- the compute target to run the pipeline on\n",
    "- input and output data, and any custom parameters\n",
    "- reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class PipelineStep to assist with building a step using certain frameworks and stacks. Here, the PythonScriptStep class is used to define the step logic using the train_model.py script.\n",
    "\n",
    "An object reference in the outputs array becomes available as an input for a subsequent pipeline step, for scenarios where there is more than one step.\n",
    "\n",
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/en-gb/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline training step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step has been created.\n"
     ]
    }
   ],
   "source": [
    "train_step = PythonScriptStep(name=\"Train Model\",\n",
    "                              script_name=\"pipeline/train_model_step.py\",\n",
    "                              compute_target=compute_target,\n",
    "                              source_directory=\"../src\",\n",
    "                              outputs=[pipeline_data],\n",
    "                              arguments=[\"--model_name\", model_name_param,\n",
    "                                         \"--step_output\", pipeline_data,\n",
    "                                         \"--dataset_version\", dataset_version_param,\n",
    "                                         \"--data_file_path\", data_file_path_param,\n",
    "                                         \"--caller_run_id\", caller_run_id_param,\n",
    "                                         \"--dataset_name\", dataset_name_param],\n",
    "                              runconfig=run_config,\n",
    "                              allow_reuse=True)\n",
    "\n",
    "print(\"Training step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline evaluate step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate step has been created.\n"
     ]
    }
   ],
   "source": [
    "evaluate_step = PythonScriptStep(name=\"Evaluate Model\",\n",
    "                                 script_name=\"pipeline/evaluate_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--allow_run_cancel\", True],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Evaluate step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline register step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register step has been created.\n"
     ]
    }
   ],
   "source": [
    "register_step = PythonScriptStep(name=\"Register Model \",\n",
    "                                 script_name=\"pipeline/register_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 inputs=[pipeline_data],\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--step_input\", pipeline_data],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Register step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch the three pipeline steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include evaluation step before register step.\n"
     ]
    }
   ],
   "source": [
    "# Check run_evaluation flag to include or exclude evaluation step.\n",
    "if run_evaluation == True:\n",
    "    print(\"Include evaluation step before register step.\")\n",
    "    evaluate_step.run_after(train_step)\n",
    "    register_step.run_after(evaluate_step)\n",
    "    steps = [train_step, evaluate_step, register_step]\n",
    "else:\n",
    "    print(\"Exclude evaluation step and directly run register step.\")\n",
    "    register_step.run_after(train_step)\n",
    "    steps = [train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and validate the pipeline based on the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Train Model is ready to be created [86731611]\n",
      "Step Evaluate Model is ready to be created [66f6a28d]\n",
      "Step Register Model  is ready to be created [9ecd6e27]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "train_pipeline._set_experiment_name\n",
    "train_pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish the pipeline to create a REST endpoint that allows to rerun the pipeline from any HTTP library on any platform. The published pipeline can also be run from the AML workspace where different metdata such as run history and duration are tracked as well. If a pipeline with the same version has already been published, retrieve the existing published pipeline instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Train Model [86731611][66d7041e-178e-45f5-be3c-58ba03b4cafa], (This step will run and generate new outputs)\n",
      "Created step Evaluate Model [66f6a28d][ed4c7b44-125a-4c7d-a42e-08faea8cee9e], (This step will run and generate new outputs)\n",
      "Created step Register Model  [9ecd6e27][8fb51e98-ecf7-4398-94a4-3d370573f2be], (This step will run and generate new outputs)\n",
      "Published pipeline 'dog classification model training pipeline' with version 1.1.\n"
     ]
    }
   ],
   "source": [
    "pipelines = PublishedPipeline.list(ws)\n",
    "matched_pipes = []\n",
    "\n",
    "for p in pipelines:\n",
    "    if p.name == pipeline_name:\n",
    "        if p.version == pipeline_version:\n",
    "            matched_pipes.append(p)\n",
    "\n",
    "if(len(matched_pipes) == 0):\n",
    "    published_pipeline = train_pipeline.publish(name=pipeline_name,\n",
    "                                                description=\"Model training/retraining pipeline\",\n",
    "                                                version=pipeline_version)\n",
    "    \n",
    "    print(f\"Published pipeline '{published_pipeline.name}' with version {published_pipeline.version}.\")\n",
    "\n",
    "else:\n",
    "    published_pipeline = matched_pipes[0]\n",
    "    print(f\"Retrieved published pipeline with id {published_pipeline.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline run takes more time than subsequent runs, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. Total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 08c5cb8b-9409-4778-8a5a-1fd52f3c53f3\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford-dogs-classifier-train-pipeline/runs/08c5cb8b-9409-4778-8a5a-1fd52f3c53f3?wsid=/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourcegroups/sbirkamlrg/workspaces/sbirkamlws\n"
     ]
    }
   ],
   "source": [
    "pipeline_parameters = {\"model_name\": model_name}\n",
    "tags = {\"trigger\": \"jupyter notebook\",\n",
    "        \"model_architecture\" : \"transfer-learning with ResNext-50\"}\n",
    "\n",
    "# Create an AML Experiment\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    \n",
    "# Submit an Experiment Run using the published pipeline and defined pipeline parameters\n",
    "run = experiment.submit(published_pipeline,\n",
    "                        tags=tags,\n",
    "                        pipeline_parameters=pipeline_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 08c5cb8b-9409-4778-8a5a-1fd52f3c53f3\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford-dogs-classifier-train-pipeline/runs/08c5cb8b-9409-4778-8a5a-1fd52f3c53f3?wsid=/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourcegroups/sbirkamlrg/workspaces/sbirkamlws\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: aaf25f12-7063-4874-bc7f-2b9525d91804\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford-dogs-classifier-train-pipeline/runs/aaf25f12-7063-4874-bc7f-2b9525d91804?wsid=/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourcegroups/sbirkamlrg/workspaces/sbirkamlws\n",
      "StepRun( Train Model ) Status: NotStarted\n",
      "StepRun( Train Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      "========================================================================================================================\n",
      "2021-03-14T11:31:32Z Starting output-watcher...\n",
      "2021-03-14T11:31:32Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-03-14T11:31:35Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-03-14T11:31:35Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      "Digest: sha256:a0dda384f049847b440e051dae44ca912efbaddfe6c82f1ae2f19d71479bbab5\n",
      "Status: Image is up to date for e3ec47206e69417c9ef00185ff1aae0c.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "e3ec47206e69417c9ef00185ff1aae0c.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "2021-03-14T11:31:36Z Check if container aaf25f12-7063-4874-bc7f-2b9525d91804 already exist exited with 0, \n",
      "\n",
      "de3d8ad18f88b9b25bd53d6c3354187d11a120c61de4882b03cfc3e49283bc38\n",
      "2021/03/14 11:31:37 Starting App Insight Logger for task:  containerSetup\n",
      "2021/03/14 11:31:37 Version: 3.0.01524.0008 Branch: .SourceBranch Commit: eba29f9\n",
      "2021/03/14 11:31:37 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/03/14 11:31:37 Starting infiniband setup\n",
      "2021/03/14 11:31:37 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/03/14 11:31:37 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/03/14 11:31:37 Starting setupPasswordLessSSH setup\n",
      "2021/03/14 11:31:37 sshd runtime has already been installed in the container\n",
      "2021/03/14 11:31:38 All App Insights Logs was send successfully\n",
      "2021/03/14 11:31:38 App Insight Client has already been closed\n",
      "2021/03/14 11:31:38 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-03-14T11:31:38Z Starting docker container succeeded.\n",
      "2021-03-14T11:31:47Z Job environment preparation succeeded on 10.0.0.5. Output: \n",
      ">>>   2021/03/14 11:31:29 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/03/14 11:31:29 Version: 3.0.01524.0008 Branch: .SourceBranch Commit: eba29f9\n",
      ">>>   2021/03/14 11:31:29 runtime.GOOS linux\n",
      ">>>   2021/03/14 11:31:29 Reading dyanamic configs\n",
      ">>>   2021/03/14 11:31:29 Container sas url: https://baiscriptswesteuropeprod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=9UBH7ig8b9NIeIkNQpNxDmP7wUMtSqFoIE5AY22cheE%3D\n",
      ">>>   2021/03/14 11:31:29 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: no such file or directory\n",
      ">>>   2021/03/14 11:31:29 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installed false, isEnable false,\n",
      ">>>   2021/03/14 11:31:29 azsecpack isEnable:false,GetDisableVsatlsscan:true\n",
      ">>>   2021/03/14 11:31:29 [doTurnOffAzsecpack] output:   Active: inactive (dead)\n",
      ">>>   ,err:<nil>.\n",
      ">>>   2021/03/14 11:31:29 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/03/14 11:31:29 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/03/14 11:31:29 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/03/14 11:31:29 GPU : GPU 0: Tesla K80 (UUID: GPU-520664d3-a929-bb9a-662c-bb566d01235b)\n",
      ">>>   2021/03/14 11:31:29 GPU count found on the node: 1\n",
      ">>>   2021/03/14 11:31:29 Mellanox Inbox drivers found (implying presence of SR-IOV)?: false\n",
      ">>>   2021/03/14 11:31:29 Disabling IB for NCCL.\n",
      ">>>   2021/03/14 11:31:29 AMLComputeXDSEndpoint:  https://westeurope-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/03/14 11:31:29 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/03/14 11:31:29 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config\n",
      ">>>   2021/03/14 11:31:29 This is not a aml-workstation (compute instance), current offer type: azureml. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/03/14 11:31:29 Starting identity responder.\n",
      ">>>   2021/03/14 11:31:29 Starting identity responder.\n",
      ">>>   2021/03/14 11:31:29 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/03/14 11:31:29 Logfile used for identity responder: /mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/IdentityResponderLog-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      ">>>   2021/03/14 11:31:29 Logfile used for identity responder: /mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/IdentityResponderLog-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      ">>>   2021/03/14 11:31:29 Started Identity Responder for job.\n",
      ">>>   2021/03/14 11:31:29 Started Identity Responder for job.\n",
      ">>>   2021/03/14 11:31:29 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/wd\n",
      ">>>   2021/03/14 11:31:29 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/shared\n",
      ">>>   2021/03/14 11:31:29 Mounting job level file systems\n",
      ">>>   2021/03/14 11:31:29 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts\n",
      ">>>   2021/03/14 11:31:29 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/03/14 11:31:29 Datastore credentials file not found, skipping.\n",
      ">>>   2021/03/14 11:31:29 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.master.runtimesastokens\n",
      ">>>   2021/03/14 11:31:29 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/03/14 11:31:29 No NFS configured\n",
      ">>>   2021/03/14 11:31:29 No Azure File Shares configured\n",
      ">>>   2021/03/14 11:31:29 Mounting blob file systems\n",
      ">>>   2021/03/14 11:31:30 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/03/14 11:31:30 Mounting azureml-blobstore-e3ec4720-6e69-417c-9ef0-0185ff1aae0c container from sbirkamlws2968444863 account at /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore\n",
      ">>>   2021/03/14 11:31:30 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/03/14 11:31:30 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/03/14 11:31:30 Blobfuse cache size set to 310298 MB.\n",
      ">>>   2021/03/14 11:31:30 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=310298 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/03/14 11:31:30 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore\n",
      ">>>   2021/03/14 11:31:30 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore\n",
      ">>>   2021/03/14 11:31:30 Successfully mounted azureml-blobstore-e3ec4720-6e69-417c-9ef0-0185ff1aae0c container from sbirkamlws2968444863 account at /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore\n",
      ">>>   2021/03/14 11:31:30 No unmanaged file systems configured\n",
      ">>>   2021/03/14 11:31:30 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/03/14 11:31:30 GPU : GPU 0: Tesla K80 (UUID: GPU-520664d3-a929-bb9a-662c-bb566d01235b)\n",
      ">>>   2021/03/14 11:31:30 Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ">>>   . Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      ">>>   2021/03/14 11:31:30 Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ">>>   . Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      ">>>   2021/03/14 11:31:30 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/03/14 11:31:30 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs\n",
      ">>>   2021/03/14 11:31:31 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/logs\n",
      ">>>   2021/03/14 11:31:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/outputs\n",
      ">>>   2021/03/14 11:31:32 Starting output-watcher...\n",
      ">>>   2021/03/14 11:31:33 Single file input dataset is enabled.\n",
      ">>>   2021/03/14 11:31:33 Start to pulling docker image: e3ec47206e69417c9ef00185ff1aae0c.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      ">>>   2021/03/14 11:31:33 Start pull docker image: e3ec47206e69417c9ef00185ff1aae0c.azurecr.io\n",
      ">>>   2021/03/14 11:31:33 Getting credentials for image e3ec47206e69417c9ef00185ff1aae0c.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043 with url e3ec47206e69417c9ef00185ff1aae0c.azurecr.io\n",
      ">>>   2021/03/14 11:31:33 Container registry is ACR.\n",
      ">>>   2021/03/14 11:31:33 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/03/14 11:31:33 Getting ACR Credentials from EMS for environment stanford-dogs-train-env:Autosave_2021-03-13T22:11:54Z_71a9aa94\n",
      ">>>   2021/03/14 11:31:33 Requesting XDS for registry details.\n",
      ">>>   2021/03/14 11:31:33 Attempt 1 of http call to https://westeurope-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourceGroups/sbirkamlrg/workspaces/sbirkamlws/clusters/gpu-cluster/nodes/tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d?api-version=2018-02-01\n",
      ">>>   2021/03/14 11:31:34 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 15\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/03/14 11:31:35 Got container registry details from credentials service for registry address: e3ec47206e69417c9ef00185ff1aae0c.azurecr.io.\n",
      ">>>   2021/03/14 11:31:35 Writing ACR Details to file...\n",
      ">>>   2021/03/14 11:31:35 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/03/14 11:31:35 Executing 'Copy ACR Details file' on 10.0.0.5\n",
      ">>>   2021/03/14 11:31:35 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/03/14 11:31:35 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/03/14 11:31:35 Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/03/14 11:31:35 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/03/14 11:31:35 EMS returned e3ec47206e69417c9ef00185ff1aae0c.azurecr.io for environment stanford-dogs-train-env\n",
      ">>>   2021/03/14 11:31:35 start login to the docker registry\n",
      ">>>   2021/03/14 11:31:35 Successfully logged into the docker registry.\n",
      ">>>   2021/03/14 11:31:35 Start run pull docker image command\n",
      ">>>   2021/03/14 11:31:36 Pull docker image succeeded.\n",
      ">>>   2021/03/14 11:31:36 Pull docker image time: 3.115689435s\n",
      ">>>   \n",
      ">>>   2021/03/14 11:31:36 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/03/14 11:31:36 Start to getting gpu count by running nvidia-smi command\n",
      ">>>   2021/03/14 11:31:36 GPU : GPU 0: Tesla K80 (UUID: GPU-520664d3-a929-bb9a-662c-bb566d01235b)\n",
      ">>>   2021/03/14 11:31:36 Setting the memory limit for docker container to be 55988 MB\n",
      ">>>   2021/03/14 11:31:36 The env variable file size is 38291 bytes\n",
      ">>>   2021/03/14 11:31:36 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/03/14 11:31:36 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,aaf25f12-7063-4874-bc7f-2b9525d91804,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,--gpus,all,-m,55988m,-v,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/aaf25f12-7063-4874-bc7f-2b9525d91804/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/aaf25f12-7063-4874-bc7f-2b9525d91804/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.batchai.envlist,--shm-size,2g\n",
      ">>>   2021/03/14 11:31:36 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/aaf25f12-7063-4874-bc7f-2b9525d91804/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/aaf25f12-7063-4874-bc7f-2b9525d91804/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/03/14 11:31:36 the binding /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804 \n",
      ">>>   2021/03/14 11:31:36 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,aaf25f12-7063-4874-bc7f-2b9525d91804,--gpus,all,-m,55988m,-w,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.batchai.envlist,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804,-v,/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd,-v,/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs\n",
      ">>>   2021/03/14 11:31:36 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name aaf25f12-7063-4874-bc7f-2b9525d91804 --gpus all -m 55988m -w /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/config/.batchai.envlist --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804:/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804 -v /mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd -v /mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs:/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/certs -d -it --privileged --net=host e3ec47206e69417c9ef00185ff1aae0c.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      ">>>   2021/03/14 11:31:36 Check if container aaf25f12-7063-4874-bc7f-2b9525d91804 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/03/14 11:31:36 Check if container aaf25f12-7063-4874-bc7f-2b9525d91804 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/03/14 11:31:38 Container ssh is not required for job type.\n",
      ">>>   2021/03/14 11:31:38 Starting docker container succeeded.\n",
      ">>>   2021/03/14 11:31:38 Starting docker container succeeded.\n",
      ">>>   2021/03/14 11:31:38 Disk space after starting docker container: 317649MB\n",
      ">>>   2021/03/14 11:31:38 Begin execution of runSpecialJobTask\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"d1e5bfbd-d04c-4008-bf8e-61222d583026\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs/65_job_prep-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/azureml_compute_logs/65_job_prep-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      ">>>   2021/03/14 11:31:38 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804;/azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"d1e5bfbd-d04c-4008-bf8e-61222d583026\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/03/14 11:31:38 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-f9476e99869ac9b17ae8cf60e40f55ba-bf2ae2336fca9675-01 -t aaf25f12-7063-4874-bc7f-2b9525d91804 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/70e2e318-f3ce-4193-ac54-1ece9b42e34a/job-1/aaf25f12-7063-4874-b_2fdbdfe3-4583-41c8-a8c9-ae0ae8a085d6/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804;/azureml-envs/azureml_3a65fd49f096be6cbe5921e5dcc78936/bin/python /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"d1e5bfbd-d04c-4008-bf8e-61222d583026\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/03/14 11:31:39 Attempt 1 of http call to https://westeurope.experiments.azureml.net/history/v1.0/private/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourceGroups/sbirkamlrg/providers/Microsoft.MachineLearningServices/workspaces/sbirkamlws/runs/aaf25f12-7063-4874-bc7f-2b9525d91804/spans\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:39.161112] Entering job preparation.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.083180] Starting job preparation.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.083221] Extracting the control code.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.108163] fetching and extracting the control code on master node.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.108189] Starting extract_project.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.108225] Starting to extract zip file.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:41.951112] Finished extracting zip file.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:42.141787] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:42.141852] Start fetching snapshots.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:42.141890] Start fetching snapshot.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:42.141907] Retrieving project from snapshot: d1e5bfbd-d04c-4008-bf8e-61222d583026\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 62\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.434743] Finished fetching snapshot.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.434776] Finished fetching snapshots.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.434786] Finished extract_project.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.451362] Finished fetching and extracting the control code.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.461962] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.463324] Start run_history_prep.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:45.554839] Entering context manager injector.\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: Already registered authentication for run id: aaf25f12-7063-4874-bc7f-2b9525d91804\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: Already registered authentication for run id: aaf25f12-7063-4874-bc7f-2b9525d91804\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: Acquired lockfile /tmp/aaf25f12-7063-4874-bc7f-2b9525d91804-datastore.lock to downloading input data references\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:46.625750] downloadDataStore completed\n",
      ">>>   2021/03/14 11:31:47 runSpecialJobTask: preparation: [2021-03-14T11:31:47.077021] Job preparation is complete.\n",
      ">>>   2021/03/14 11:31:47 Execution of runSpecialJobTask completed\n",
      ">>>   2021/03/14 11:31:47 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/03/14 11:31:47 Process Exiting with Code:  0\n",
      ">>>   2021/03/14 11:31:47 All App Insights Logs was send successfully\n",
      ">>>   \n",
      "2021-03-14T11:31:47Z 127.0.0.1 slots=1 max-slots=1\n",
      "2021-03-14T11:31:48Z launching Custom job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/03/14 11:31:48 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2021/03/14 11:31:48 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2021-03-14T11:31:49.902251] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['pipeline/train_model_step.py', '--model_name', 'dog-classification-model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'fowl-dataset'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 128\n",
      "[2021-03-14T11:31:51.922560] Entering Run History Context Manager.\n",
      "2021/03/14 11:31:53 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "[2021-03-14T11:31:53.432632] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804\n",
      "[2021-03-14T11:31:53.432966] Preparing to call script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog-classification-model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'fowl-dataset']\n",
      "[2021-03-14T11:31:53.433039] After variable expansion, calling script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog-classification-model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'fowl-dataset']\n",
      "\n",
      "Running train_model_step.py\n",
      "Argument [caller_run_id]: none\n",
      "Argument [dataset_name]: fowl-dataset\n",
      "Argument [dataset_version]: 1\n",
      "Argument [data_file_path]: none\n",
      "Argument [model_name]: dog-classification-model\n",
      "Argument [step_output]: /mnt/batch/tasks/shared/LS_root/jobs/sbirkamlws/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/mounts/workspaceblobstore/azureml/aaf25f12-7063-4874-bc7f-2b9525d91804/pipeline_data\n",
      "Getting training parameters\n",
      "Parameters: {'num_epochs': 30, 'learning_rate': 0.1, 'momentum': 0.9, 'num_frozen_layers': 7, 'num_neurons_fc_layer': 512, 'dropout_prob_fc_layer': 0.0, 'lr_scheduler_step_size': 7}\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 128\n",
      "\n",
      "\n",
      "[2021-03-14T11:31:56.494903] The experiment failed. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "4 items cleaning up...\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_0558bcc487f318756d216e6c99bce70e09e1f1d84d5568070eef3b7b78669fed_d.txt\n",
      "===============================================================================================================\n",
      "[2021-03-14T11:32:08.041286] Entering job release\n",
      "[2021-03-14T11:32:09.568746] Starting job release\n",
      "[2021-03-14T11:32:09.575203] Logging experiment finalizing status in history service.\n",
      "[2021-03-14T11:32:09.575783] job release stage : upload_datastore starting...Starting the daemon thread to refresh tokens in background for process with pid = 274\n",
      "\n",
      "[2021-03-14T11:32:09.576212] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-03-14T11:32:09.578321] job release stage : copy_batchai_cached_logs starting...[2021-03-14T11:32:09.578360] job release stage : execute_job_release starting...\n",
      "\n",
      "[2021-03-14T11:32:09.578680] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-03-14T11:32:09.585008] Entering context manager injector.\n",
      "[2021-03-14T11:32:09.886136] job release stage : upload_datastore completed...[2021-03-14T11:32:09.886742] job release stage : send_run_telemetry starting...\n",
      "\n",
      "[2021-03-14T11:32:09.943756] job release stage : execute_job_release completed...\n",
      "[2021-03-14T11:32:10.199048] get vm size and vm region successfully.\n",
      "[2021-03-14T11:32:10.466293] get compute meta data successfully.\n",
      "[2021-03-14T11:32:10.635701] post artifact meta request successfully.\n",
      "[2021-03-14T11:32:10.667734] upload compute record artifact successfully.\n",
      "[2021-03-14T11:32:10.849621] job release stage : send_run_telemetry completed...\n",
      "[2021-03-14T11:32:10.850001] Job release is complete\n",
      "\n",
      "StepRun(Train Model) Execution Summary\n",
      "=======================================\n",
      "StepRun( Train Model ) Status: Failed\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",\n        \"details\": [],\n        \"innerError\": {\n            \"code\": \"BadArgument\",\n            \"innerError\": {\n                \"code\": \"AmlComputeBadRequest\"\n            }\n        },\n        \"messageFormat\": \"{Message}\",\n        \"messageParameters\": {\n            \"Message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\"\n        }\n    },\n    \"correlation\": {\n        \"operation\": null,\n        \"request\": \"7ee61f6e9c0e7b56\"\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2021-03-14T11:32:44.667053Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\",\\n        \\\"details\\\": [],\\n        \\\"innerError\\\": {\\n            \\\"code\\\": \\\"BadArgument\\\",\\n            \\\"innerError\\\": {\\n                \\\"code\\\": \\\"AmlComputeBadRequest\\\"\\n            }\\n        },\\n        \\\"messageFormat\\\": \\\"{Message}\\\",\\n        \\\"messageParameters\\\": {\\n            \\\"Message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\"\\n        }\\n    },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n        \\\"request\\\": \\\"7ee61f6e9c0e7b56\\\"\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2021-03-14T11:32:44.667053Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9f4a1a008ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Wait for completion of the run and show output log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/pytorch-aml-env/lib/python3.7/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0;32m--> 295\u001b[0;31m                                                              raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    296\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                                 \u001b[0;31m# If there are package conflicts in the user's environment, the run rehydration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch-aml-env/lib/python3.7/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0;32m--> 737\u001b[0;31m                                                raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The output streaming for the run interrupted.\\n\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch-aml-env/lib/python3.7/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",\n        \"details\": [],\n        \"innerError\": {\n            \"code\": \"BadArgument\",\n            \"innerError\": {\n                \"code\": \"AmlComputeBadRequest\"\n            }\n        },\n        \"messageFormat\": \"{Message}\",\n        \"messageParameters\": {\n            \"Message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\"\n        }\n    },\n    \"correlation\": {\n        \"operation\": null,\n        \"request\": \"7ee61f6e9c0e7b56\"\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2021-03-14T11:32:44.667053Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\",\\n        \\\"details\\\": [],\\n        \\\"innerError\\\": {\\n            \\\"code\\\": \\\"BadArgument\\\",\\n            \\\"innerError\\\": {\\n                \\\"code\\\": \\\"AmlComputeBadRequest\\\"\\n            }\\n        },\\n        \\\"messageFormat\\\": \\\"{Message}\\\",\\n        \\\"messageParameters\\\": {\\n            \\\"Message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\"\\n        }\\n    },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n        \\\"request\\\": \\\"7ee61f6e9c0e7b56\\\"\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2021-03-14T11:32:44.667053Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Wait for completion of the run and show output log\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the compute target.\n",
    "\n",
    "**Note**: At the moment the compute target can and should not be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-aml-env",
   "language": "python",
   "name": "pytorch-aml-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
