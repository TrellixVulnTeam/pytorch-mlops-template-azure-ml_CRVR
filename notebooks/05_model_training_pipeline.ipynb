{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Notebook Summary](#Notebook-Summary)\n",
    "* [Setup](#Setup)\n",
    "    * [Notebook Parameters](#Notebook-Parameters)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Pipeline Run Configuration & Environment](#Pipeline-Run-Configuration-&-Environment)\n",
    "* [Pipeline Inputs](#Pipeline-Inputs)\n",
    "* [Create Pipeline](#Create-Pipeline)\n",
    "    * [Training Step](#Training-Step)\n",
    "    * [Evaluate Step](#Evaluate-Step)\n",
    "    * [Register Step](#Register-Step)\n",
    "* [Publish Pipeline](#Publish-Pipeline)\n",
    "* [Run Pipeline](#Run-Pipeline)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, an Azure Machine Learning (AML) training / retraining pipeline will be built and published. After building and publishing the pipeline, a REST endpoint can be used to trigger the pipeline from any HTTP library on any platform. This pipeline will be used in the MlOps process for continuous model retraining, e.g. when data or model drift is detected or in general when the model should be retrained. A pipeline gives a more operationalizable way of training than a script run (which was used for original model training in the `02_model_training` notebook) as it can be easily automated and run based on triggers. It also allows for chaining of different steps that can then be executed sequentially. In general, machine learning pipelines help to optimize the workflow in terms of speed, portability and reuse.\n",
    "\n",
    "The training / retraining pipeline built in this notebook will consist of three different steps that are executed sequentially:\n",
    "- Model training using the same code for training as in the `02_model_training` notebook\n",
    "- Model evaluation (comparing the newly trained model with the model currently in production or with a manual threshold)\n",
    "- Model registration (registering the newly trained model to the AML workspace based on the outcomes of the model evaluation)\n",
    "\n",
    "Check out the [AML Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines) for more info on how to build pipelines in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import azureml\n",
    "from azureml.core import Dataset, Datastore, Environment, Experiment, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the remote compute target cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "# Define the name of the training environment created in the 00_environment_setup notebook\n",
    "env_name = \"stanford-dogs-train-env\"\n",
    "\n",
    "# Determine whether the pipeline training run should be evaluated before model registration\n",
    "run_evaluation = True\n",
    "\n",
    "# Define the pipeline endpoint name\n",
    "pipeline_name = \"dog_clf_model_training_pipeline\"\n",
    "\n",
    "# Define the pipeline endpoint version\n",
    "pipeline_version = \"1.0\"\n",
    "\n",
    "# Define the model_name\n",
    "model_name = \"dog_clf_model\"\n",
    "\n",
    "# Define the experiment name\n",
    "experiment_name = \"stanford_dogs_classifier_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the AML workspace, a workspace object needs to be instantiated using the AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a remote compute target to run the pipeline experiments on. The below code will first check whether a compute target with name **cluster_name** (defined in the [Notebook Parameters](#Notebook-Parameters) section) already exists and if it does, will retrieve it. Otherwise it will create a new compute cluster.\n",
    "\n",
    "**Note**: At the moment it is not possible to create a new compute cluster so please specify the name of an existing compute cluster.\n",
    "\n",
    "AML pipelines need to be run on a remote compute target and cannot be run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 1, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2021-03-25T06:41:52.952000+00:00', 'errors': None, 'creationTime': '2021-02-04T18:49:38.130943+00:00', 'modifiedTime': '2021-02-04T18:49:53.799036+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Configuration & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model training environment that has been registered as part of the `00_environment_setup` notebook and use it for the pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline run configuration containing the retrieved environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PipelineData object to pass data between steps.\n",
    "\n",
    "While here the pipeline will consist of a single step only, a usual flow with multiple steps will include:\n",
    "- Using Dataset objects as inputs to fetch raw data, performing some transformations, then outputting a PipelineData object.\n",
    "- Use the previous step's PipelineData output object as an input object, repeated for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = PipelineData(\"pipeline_data\", datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PipelineParameter objects to be able to pass versatile arguments to the PythonScriptSteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_param = PipelineParameter(name=\"dataset_name\", default_value=\"stanford_dogs_dataset\")\n",
    "dataset_version_param = PipelineParameter(name=\"dataset_version\", default_value=1)\n",
    "data_file_path_param = PipelineParameter(name=\"data_file_path\", default_value=\"none\")\n",
    "model_name_param = PipelineParameter(name=\"model_name\", default_value=\"dog_clf_model\")\n",
    "caller_run_id_param = PipelineParameter(name=\"caller_run_id\", default_value=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a pipeline, the individual steps need to be created first.\n",
    "\n",
    "A pipeline step is an object that encapsulates everything that is needed for running a pipeline including:\n",
    "\n",
    "- environment and dependency settings\n",
    "- the compute target to run the pipeline on\n",
    "- input and output data, and any custom parameters\n",
    "- reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class PipelineStep to assist with building a step using certain frameworks and stacks. Here, the PythonScriptStep class is used to define the step logic using the train_model.py script.\n",
    "\n",
    "An object reference in the outputs array becomes available as an input for a subsequent pipeline step, for scenarios where there is more than one step.\n",
    "\n",
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/en-gb/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline training step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step has been created.\n"
     ]
    }
   ],
   "source": [
    "train_step = PythonScriptStep(name=\"Train Model\",\n",
    "                              script_name=\"pipeline/train_model_step.py\",\n",
    "                              compute_target=compute_target,\n",
    "                              source_directory=\"../src\",\n",
    "                              outputs=[pipeline_data],\n",
    "                              arguments=[\"--model_name\", model_name_param,\n",
    "                                         \"--step_output\", pipeline_data,\n",
    "                                         \"--dataset_version\", dataset_version_param,\n",
    "                                         \"--data_file_path\", data_file_path_param,\n",
    "                                         \"--caller_run_id\", caller_run_id_param,\n",
    "                                         \"--dataset_name\", dataset_name_param],\n",
    "                              runconfig=run_config,\n",
    "                              allow_reuse=True)\n",
    "\n",
    "print(\"Training step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline evaluate step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate step has been created.\n"
     ]
    }
   ],
   "source": [
    "evaluate_step = PythonScriptStep(name=\"Evaluate Model\",\n",
    "                                 script_name=\"pipeline/evaluate_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--allow_run_cancel\", True],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Evaluate step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline register step using the PipelineParameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register step has been created.\n"
     ]
    }
   ],
   "source": [
    "register_step = PythonScriptStep(name=\"Register Model \",\n",
    "                                 script_name=\"pipeline/register_model_step.py\",\n",
    "                                 compute_target=compute_target,\n",
    "                                 source_directory=\"../src\",\n",
    "                                 inputs=[pipeline_data],\n",
    "                                 arguments=[\"--model_name\", model_name_param,\n",
    "                                            \"--step_input\", pipeline_data],\n",
    "                                 runconfig=run_config,\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "print(\"Register step has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch the three pipeline steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include evaluation step before register step.\n"
     ]
    }
   ],
   "source": [
    "# Check run_evaluation flag to include or exclude evaluation step.\n",
    "if run_evaluation == True:\n",
    "    print(\"Include evaluation step before register step.\")\n",
    "    evaluate_step.run_after(train_step)\n",
    "    register_step.run_after(evaluate_step)\n",
    "    steps = [train_step, evaluate_step, register_step]\n",
    "else:\n",
    "    print(\"Exclude evaluation step and directly run register step.\")\n",
    "    register_step.run_after(train_step)\n",
    "    steps = [train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and validate the pipeline based on the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Train Model is ready to be created [e07b2ec1]\n",
      "Step Evaluate Model is ready to be created [99965b81]\n",
      "Step Register Model  is ready to be created [e74c99f3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "train_pipeline._set_experiment_name\n",
    "train_pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish the pipeline to create a REST endpoint that allows to rerun the pipeline from any HTTP library on any platform. The published pipeline can also be run from the AML workspace where different metdata such as run history and duration are tracked as well. If a pipeline with the same version has already been published, retrieve the existing published pipeline instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Train Model [e07b2ec1][2e23b59a-f2b0-445e-81a1-9c76a3a76039], (This step will run and generate new outputs)\n",
      "Created step Evaluate Model [99965b81][a50388ed-fbe0-4763-a0cd-9fe343b58eef], (This step will run and generate new outputs)\n",
      "Created step Register Model  [e74c99f3][47e24927-5eb0-4393-9b2a-ad88f82121af], (This step will run and generate new outputs)\n",
      "Published pipeline 'dog_clf_model_training_pipeline' with version 1.0.\n"
     ]
    }
   ],
   "source": [
    "pipelines = PublishedPipeline.list(ws)\n",
    "matched_pipes = []\n",
    "\n",
    "for p in pipelines:\n",
    "    if p.name == pipeline_name:\n",
    "        if p.version == pipeline_version:\n",
    "            matched_pipes.append(p)\n",
    "\n",
    "if(len(matched_pipes) == 0):\n",
    "    published_pipeline = train_pipeline.publish(name=pipeline_name,\n",
    "                                                description=\"Model training/retraining pipeline\",\n",
    "                                                version=pipeline_version)\n",
    "    \n",
    "    print(f\"Published pipeline '{published_pipeline.name}' with version {published_pipeline.version}.\")\n",
    "\n",
    "else:\n",
    "    published_pipeline = matched_pipes[0]\n",
    "    print(f\"Retrieved published pipeline with id {published_pipeline.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline run takes more time than subsequent runs, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. Total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 8c637b32-8bab-4b76-8ed5-a93410e8ce8e\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/8c637b32-8bab-4b76-8ed5-a93410e8ce8e?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n"
     ]
    }
   ],
   "source": [
    "pipeline_parameters = {\"model_name\": model_name}\n",
    "tags = {\"trigger\": \"jupyter notebook\",\n",
    "        \"model_architecture\" : \"transfer-learning with ResNext-50\"}\n",
    "\n",
    "# Create an AML Experiment\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    \n",
    "# Submit an Experiment Run using the published pipeline and defined pipeline parameters\n",
    "run = experiment.submit(published_pipeline,\n",
    "                        tags=tags,\n",
    "                        pipeline_parameters=pipeline_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 8c637b32-8bab-4b76-8ed5-a93410e8ce8e\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/8c637b32-8bab-4b76-8ed5-a93410e8ce8e?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: c0e01d11-1a5c-4ea2-9136-1b50bb254608\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/stanford_dogs_classifier_train/runs/c0e01d11-1a5c-4ea2-9136-1b50bb254608?wsid=/subscriptions/bf088f59-f015-4332-bd36-54b988be7c90/resourcegroups/amlbrikserg/workspaces/amlbriksews\n",
      "StepRun( Train Model ) Status: NotStarted\n",
      "StepRun( Train Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_d69b064d3d98d381deab45293e1c53c9903ffa834b3bf727e821875eca0bf663_p.txt\n",
      "========================================================================================================================\n",
      "2021-03-25T06:48:55Z Starting output-watcher...\n",
      "2021-03-25T06:48:55Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
      "2021-03-25T06:48:56Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-03-25T06:48:56Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_6dbbd78e255a8138d44f837e9481b043\n",
      "2c11b7cecaa5: Pulling fs layer\n",
      "04637fa56252: Pulling fs layer\n",
      "d6e6af23a0f3: Pulling fs layer\n",
      "b4a424de92ad: Pulling fs layer\n",
      "3e5d9ee64909: Pulling fs layer\n",
      "3a846111ff22: Pulling fs layer\n",
      "93a5020c6e19: Pulling fs layer\n",
      "360b353e68fd: Pulling fs layer\n",
      "ea4e2e1810f8: Pulling fs layer\n",
      "def12cf7de15: Pulling fs layer\n",
      "3ae6adfbdb11: Pulling fs layer\n",
      "2a21fbf2232e: Pulling fs layer\n",
      "2ef655776049: Pulling fs layer\n",
      "2b2711cecc87: Pulling fs layer\n",
      "2468e9aec01e: Pulling fs layer\n",
      "92a4e88a49e9: Pulling fs layer\n",
      "a33b32b5286b: Pulling fs layer\n",
      "98c96e7c8036: Pulling fs layer\n",
      "0ca6e388c01d: Pulling fs layer\n",
      "360b353e68fd: Waiting\n",
      "cb0e780a2d44: Pulling fs layer\n",
      "00361b3fc45c: Pulling fs layer\n",
      "ea4e2e1810f8: Waiting\n",
      "def12cf7de15: Waiting\n",
      "2a21fbf2232e: Waiting\n",
      "3ae6adfbdb11: Waiting\n",
      "b4a424de92ad: Waiting\n",
      "a33b32b5286b: Waiting\n",
      "3e5d9ee64909: Waiting\n",
      "98c96e7c8036: Waiting\n",
      "3a846111ff22: Waiting\n",
      "93a5020c6e19: Waiting\n",
      "2ef655776049: Waiting\n",
      "2b2711cecc87: Waiting\n",
      "2468e9aec01e: Waiting\n",
      "92a4e88a49e9: Waiting\n",
      "0ca6e388c01d: Waiting\n",
      "cb0e780a2d44: Waiting\n",
      "00361b3fc45c: Waiting\n",
      "d6e6af23a0f3: Verifying Checksum\n",
      "d6e6af23a0f3: Download complete\n",
      "04637fa56252: Verifying Checksum\n",
      "04637fa56252: Download complete\n",
      "b4a424de92ad: Verifying Checksum\n",
      "b4a424de92ad: Download complete\n",
      "2c11b7cecaa5: Verifying Checksum\n",
      "2c11b7cecaa5: Download complete\n",
      "3a846111ff22: Verifying Checksum\n",
      "3a846111ff22: Download complete\n",
      "93a5020c6e19: Verifying Checksum\n",
      "93a5020c6e19: Download complete\n",
      "3e5d9ee64909: Verifying Checksum\n",
      "3e5d9ee64909: Download complete\n",
      "2c11b7cecaa5: Pull complete\n",
      "04637fa56252: Pull complete\n",
      "360b353e68fd: Verifying Checksum\n",
      "360b353e68fd: Download complete\n",
      "d6e6af23a0f3: Pull complete\n",
      "b4a424de92ad: Pull complete\n",
      "def12cf7de15: Download complete\n",
      "3ae6adfbdb11: Verifying Checksum\n",
      "3ae6adfbdb11: Download complete\n",
      "2a21fbf2232e: Verifying Checksum\n",
      "2a21fbf2232e: Download complete\n",
      "2ef655776049: Verifying Checksum\n",
      "2ef655776049: Download complete\n",
      "2b2711cecc87: Verifying Checksum\n",
      "2b2711cecc87: Download complete\n",
      "2468e9aec01e: Download complete\n",
      "92a4e88a49e9: Verifying Checksum\n",
      "92a4e88a49e9: Download complete\n",
      "98c96e7c8036: Verifying Checksum\n",
      "98c96e7c8036: Download complete\n",
      "ea4e2e1810f8: Verifying Checksum\n",
      "ea4e2e1810f8: Download complete\n",
      "0ca6e388c01d: Verifying Checksum\n",
      "0ca6e388c01d: Download complete\n",
      "cb0e780a2d44: Download complete\n",
      "00361b3fc45c: Verifying Checksum\n",
      "00361b3fc45c: Download complete\n",
      "3e5d9ee64909: Pull complete\n",
      "3a846111ff22: Pull complete\n",
      "93a5020c6e19: Pull complete\n",
      "360b353e68fd: Pull complete\n",
      "ea4e2e1810f8: Pull complete\n",
      "def12cf7de15: Pull complete\n",
      "3ae6adfbdb11: Pull complete\n",
      "2a21fbf2232e: Pull complete\n",
      "2ef655776049: Pull complete\n",
      "2b2711cecc87: Pull complete\n",
      "2468e9aec01e: Pull complete\n",
      "92a4e88a49e9: Pull complete\n",
      "a33b32b5286b: Download complete\n",
      "a33b32b5286b: Pull complete\n",
      "98c96e7c8036: Pull complete\n",
      "0ca6e388c01d: Pull complete\n",
      "cb0e780a2d44: Pull complete\n",
      "00361b3fc45c: Pull complete\n",
      "Digest: sha256:061c85dd8a554afd2ae1b47f6d3be6cabfa07429d7c0ff4f665a3ef43f33114b\n",
      "Status: Downloaded newer image for 3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "3d5545b15c4c49548d3823156fa90536.azurecr.io/azureml/azureml_6dbbd78e255a8138d44f837e9481b043:latest\n",
      "2021-03-25T06:50:04Z Check if container c0e01d11-1a5c-4ea2-9136-1b50bb254608 already exist exited with 0, \n",
      "\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_d69b064d3d98d381deab45293e1c53c9903ffa834b3bf727e821875eca0bf663_p.txt\n",
      "===============================================================================================================\n",
      "[2021-03-25T06:50:19.768752] Entering job preparation.\n",
      "[2021-03-25T06:50:21.092310] Starting job preparation.\n",
      "[2021-03-25T06:50:21.092346] Extracting the control code.\n",
      "[2021-03-25T06:50:21.112908] fetching and extracting the control code on master node.\n",
      "[2021-03-25T06:50:21.112938] Starting extract_project.\n",
      "[2021-03-25T06:50:21.112978] Starting to extract zip file.\n",
      "[2021-03-25T06:50:21.811964] Finished extracting zip file.\n",
      "[2021-03-25T06:50:22.174628] Using urllib.request Python 3.0 or later\n",
      "[2021-03-25T06:50:22.174690] Start fetching snapshots.\n",
      "[2021-03-25T06:50:22.174730] Start fetching snapshot.\n",
      "[2021-03-25T06:50:22.174750] Retrieving project from snapshot: 55180866-c7e8-4773-a5a7-f6508e00e534\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 62\n",
      "[2021-03-25T06:50:24.830558] Finished fetching snapshot.\n",
      "[2021-03-25T06:50:24.830589] Finished fetching snapshots.\n",
      "[2021-03-25T06:50:24.830605] Finished extract_project.\n",
      "[2021-03-25T06:50:24.841524] Finished fetching and extracting the control code.\n",
      "[2021-03-25T06:50:24.845920] downloadDataStore - Download from datastores if requested.\n",
      "[2021-03-25T06:50:24.847219] Start run_history_prep.\n",
      "[2021-03-25T06:50:24.898312] Entering context manager injector.\n",
      "Already registered authentication for run id: c0e01d11-1a5c-4ea2-9136-1b50bb254608\n",
      "Acquired lockfile /tmp/c0e01d11-1a5c-4ea2-9136-1b50bb254608-datastore.lock to downloading input data references\n",
      "[2021-03-25T06:50:25.928107] downloadDataStore completed\n",
      "[2021-03-25T06:50:26.411616] Job preparation is complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/03/25 06:50:27 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
      "2021/03/25 06:50:27 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
      "[2021-03-25T06:50:29.479546] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['pipeline/train_model_step.py', '--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/mounts/workspaceblobstore/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 128\n",
      "[2021-03-25T06:50:31.435966] Entering Run History Context Manager.\n",
      "2021/03/25 06:50:32 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "[2021-03-25T06:50:33.126658] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/mounts/workspaceblobstore/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608\n",
      "[2021-03-25T06:50:33.127016] Preparing to call script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/mounts/workspaceblobstore/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "[2021-03-25T06:50:33.127102] After variable expansion, calling script [pipeline/train_model_step.py] with arguments:['--model_name', 'dog_clf_model', '--step_output', '/mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/mounts/workspaceblobstore/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/pipeline_data', '--dataset_version', '1', '--data_file_path', 'none', '--caller_run_id', 'none', '--dataset_name', 'stanford_dogs_dataset']\n",
      "\n",
      "Running train_model_step.py\n",
      "Argument [caller_run_id]: none\n",
      "Argument [dataset_name]: stanford_dogs_dataset\n",
      "Argument [dataset_version]: 1\n",
      "Argument [data_file_path]: none\n",
      "Argument [model_name]: dog_clf_model\n",
      "Argument [step_output]: /mnt/batch/tasks/shared/LS_root/jobs/amlbriksews/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/mounts/workspaceblobstore/azureml/c0e01d11-1a5c-4ea2-9136-1b50bb254608/pipeline_data\n",
      "\n",
      "Getting training parameters\n",
      "Parameters: {'num_epochs': 1, 'batch_size': 8, 'learning_rate': 0.001, 'momentum': 0.9, 'num_frozen_layers': 7, 'num_neurons_fc_layer': 512, 'dropout_prob_fc_layer': 0.0, 'lr_scheduler_step_size': 7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "START MODEL TRAINING\n",
      "--------------------\n",
      "Hyperparameter number of epochs: 1\n",
      "Hyperparameter batch size: 8\n",
      "Hyperparameter learning rate: 0.001\n",
      "Hyperparameter momentum: 0.9\n",
      "Hyperparameter number of frozen layers: 7\n",
      "Hyperparameter number of neurons fc layer: 512\n",
      "Hyperparameter dropout probability fc layer: 0\n",
      "Hyperparameter lr scheduler step size: 7\n",
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
      "\n",
      "  0%|          | 0.00/95.8M [00:00<?, ?B/s]\n",
      " 27%|██▋       | 25.6M/95.8M [00:00<00:00, 268MB/s]\n",
      " 54%|█████▍    | 51.9M/95.8M [00:00<00:00, 271MB/s]\n",
      " 80%|███████▉  | 76.6M/95.8M [00:00<00:00, 267MB/s]\n",
      "100%|██████████| 95.8M/95.8M [00:00<00:00, 268MB/s]\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Wait for completion of the run and show output log\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the compute target.\n",
    "\n",
    "**Note**: At the moment the compute target can and should not be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford-dogs-dev-env",
   "language": "python",
   "name": "stanford-dogs-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
