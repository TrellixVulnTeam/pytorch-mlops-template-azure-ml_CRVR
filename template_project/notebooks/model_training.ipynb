{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Workspace](#Workspace)\n",
    "* [Data](#Data)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Training Artifacts](#Training-Artifacts)\n",
    "* [Development Environment](#Development-Environment)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Development Environment](#Development-Environment)\n",
    "* [Experiment & Run Configuration](#Experiment-&-Run-Configuration)\n",
    "* [Model Registration](#Model-Registration)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Append parent directory to sys path to be able to import from src directory\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml.core version: 1.19.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from azureml.core import Dataset, Environment, Experiment, Keyvault, Model, ScriptRunConfig, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from src.training.download_utils import download_file, extract_stanford_dogs_archive\n",
    "\n",
    "print(f\"azureml.core version: {azureml.core.VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the Azure Machine Learning (AML) workspace, a workspace object needs to be instantiated using the azureml SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Create Workspace Object from Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option requires a config.json file containing the subscription id, resource group and workspace name. On an AML Compute Instance, this is available by default. For any other compute, the file can be downloaded from the workspace and put in the same directory as the calling file, a subdirectory named .azureml, or in a parent directory. Alternatively, the path can also be manually specified using the `path` argument.\n",
    "\n",
    "<img src=\"../images/config_file.png\" alt=\"config_file\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: amlbriksews\n",
      "Azure region: westeurope\n",
      "Subscription id: bf088f59-f015-4332-bd36-54b988be7c90\n",
      "Resource group: amlbrikserg\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config() \n",
    "print(\"Workspace name: \" + ws.name, \n",
    "      \"Azure region: \" + ws.location, \n",
    "      \"Subscription id: \" + ws.subscription_id, \n",
    "      \"Resource group: \" + ws.resource_group, sep=\"\\n\")\n",
    "\n",
    "# Retrieve workspace details\n",
    "# ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Create Workspace from Connection Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws = Workspace.get(name=\"sbirk-aml-ws\",\n",
    "#                    subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "#                    resource_group=\"sbirk-aml-rg\") \n",
    "\n",
    "# # Retrieve workspace details\n",
    "# # ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this template, the CIFAR-10 image dataset is used for multiclass classification. It has the classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The images in CIFAR-10 are three-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[dataset](http://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = \"../src/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/training/download_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/download_utils.py\n",
    "import os\n",
    "import scipy.io\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def download_file(download_url: str,\n",
    "                  file_dir_path: str,\n",
    "                  file_name: str,\n",
    "                  skip_if_dir_exists: bool = False,\n",
    "                  force_dir_deletion: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Download a file\n",
    "    :param download_url: url from where to download\n",
    "    :param file_dir_path: directory to which to download\n",
    "    :param file_name: name of the file\n",
    "    :param skip_if_dir_exists: flag that indicates whether to skip the download if the directory already exists\n",
    "    :param force_dir_deletion: flag that indicates whether to delete the existing directory before the download\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove file directory if it exists\n",
    "    if force_dir_deletion:\n",
    "        shutil.rmtree(file_dir_path)\n",
    "    \n",
    "    # Check if download should be triggered\n",
    "    if not os.path.exists(file_dir_path) or not skip_if_dir_exists:\n",
    "    \n",
    "        # Create file directory if it does not exist\n",
    "        os.makedirs(file_dir_path, exist_ok=True)\n",
    "    \n",
    "        # Download the file\n",
    "        file_path = os.path.join(file_dir_path, file_name)\n",
    "        print(\"Downloading \" + download_url + \" to \" + file_path)\n",
    "        urllib.request.urlretrieve(download_url, filename=file_path, reporthook=generate_bar_updater())\n",
    "        \n",
    "\n",
    "def extract_stanford_dogs_archive(archive_dir_path: str = \"../data\",\n",
    "                                  target_dir_path: str = \"../data\",\n",
    "                                  remove_archives: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Extract the stanford dogs image archive and separate the images into training,\n",
    "    validation and test set\n",
    "    :param archive_dir_path: path of the \"image.tar\" and \"lists.tar\" files to be extracted\n",
    "    :param target_dir_path: path of the target directory where the file should be extracted to\n",
    "    :param remove_archives: flag that indicates whether the archives are removed after extraction\n",
    "    \"\"\"\n",
    " \n",
    "    # Specify directory paths\n",
    "    training_dir = os.path.join(target_dir_path, \"train\")\n",
    "    validation_dir = os.path.join(target_dir_path, \"val\")\n",
    "    test_dir = os.path.join(target_dir_path, \"test\")                            \n",
    "\n",
    "    # Extract lists.tar archive\n",
    "    with tarfile.open(os.path.join(archive_dir_path, \"lists.tar\"), \"r\") as lists_tar:\n",
    "        lists_tar.extractall(path=archive_dir_path)\n",
    "                             \n",
    "    print(\"Lists.tar archive has been extracted successfully.\")\n",
    "    \n",
    "    # Load list.mat files\n",
    "    train_list_mat = scipy.io.loadmat(os.path.join(archive_dir_path, \"train_list.mat\"))\n",
    "    test_list_mat = scipy.io.loadmat(os.path.join(archive_dir_path, \"test_list.mat\"))\n",
    "    \n",
    "    training_files = []\n",
    "    test_and_val_files = []\n",
    "    \n",
    "    # Extract training data file names\n",
    "    for array in train_list_mat[\"file_list\"]:\n",
    "        training_files.append(array[0][0])\n",
    "\n",
    "    # Extract test data file names\n",
    "    for array in test_list_mat[\"file_list\"]:\n",
    "        test_and_val_files.append(array[0][0])\n",
    "                             \n",
    "    print(\"File lists have been read successfully.\")\n",
    "    print(\"Extracting images.tar archive...\")\n",
    "                             \n",
    "    # Extract images.tar archive\n",
    "    with tarfile.open(os.path.join(archive_dir_path, \"images.tar\"), \"r\") as images_tar:\n",
    "        test_val_idx = 0\n",
    "        for member in tqdm.tqdm(images_tar.getmembers()):\n",
    "            if member.isreg(): # Skip if TarInfo is not files\n",
    "                member.name = member.name.split(\"/\", 1)[1] # Retrieve only relevant part of file name\n",
    "                \n",
    "                # Extract files to corresponding directories\n",
    "                if member.name in training_files:\n",
    "                    images_tar.extract(member, training_dir)\n",
    "                    \n",
    "                elif member.name in test_and_val_files: # Every 2nd file goes to the validation data\n",
    "                    test_val_idx+=1\n",
    "                    if test_val_idx % 2 != 0:\n",
    "                        images_tar.extract(member, validation_dir)\n",
    "                    else:\n",
    "                        images_tar.extract(member, test_dir)\n",
    "                             \n",
    "    print(\"Images.tar archive has been extracted successfully.\")\n",
    "\n",
    "    # Remove list.mat files\n",
    "    os.remove(os.path.join(archive_dir_path, \"file_list.mat\"))\n",
    "    os.remove(os.path.join(archive_dir_path, \"test_list.mat\"))\n",
    "    os.remove(os.path.join(archive_dir_path, \"train_list.mat\"))\n",
    "    \n",
    "    # Remove archive files if flag is set to true\n",
    "    if remove_archives:\n",
    "        print(\"Removing archive files.\")\n",
    "        os.remove(os.path.join(archive_dir_path, \"lists.tar\"))\n",
    "        os.remove(os.path.join(archive_dir_path, \"images.tar\"))\n",
    "\n",
    "                             \n",
    "def generate_bar_updater():\n",
    "    \"\"\"\n",
    "    Create a tqdm reporthook function for urlretrieve\n",
    "    :returns: bar_update function which can be used by urlretrieve \n",
    "              to display and update a progress bar\n",
    "    \"\"\"\n",
    "    \n",
    "    pbar = tqdm.tqdm(total=None)\n",
    "\n",
    "    # Define progress bar update function\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar to ../data/images.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 793059328/793579520 [00:48<00:00, 16677417.75it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar to ../data/lists.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/481280 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 16384/481280 [00:00<00:03, 137912.27it/s]\u001b[A\n",
      "  7%|▋         | 32768/481280 [00:00<00:03, 123796.62it/s]\u001b[A\n",
      " 12%|█▏        | 57344/481280 [00:00<00:03, 133610.98it/s]\u001b[A\n",
      " 15%|█▌        | 73728/481280 [00:00<00:03, 129464.38it/s]\u001b[A\n",
      " 22%|██▏       | 106496/481280 [00:01<00:02, 147503.85it/s]\u001b[A\n",
      " 29%|██▉       | 139264/481280 [00:01<00:02, 163467.60it/s]\u001b[A\n",
      " 39%|███▉      | 188416/481280 [00:01<00:01, 192387.56it/s]\u001b[A\n",
      " 51%|█████     | 245760/481280 [00:01<00:01, 226125.51it/s]\u001b[A\n",
      " 65%|██████▍   | 311296/481280 [00:01<00:00, 264402.49it/s]\u001b[A\n",
      "793583616it [00:59, 16677417.75it/s]                               \n",
      "483328it [00:18, 312845.54it/s]                            \u001b[A"
     ]
    }
   ],
   "source": [
    "archive_file_list = [\"images.tar\", \"lists.tar\"]\n",
    "\n",
    "# Download archive files from the stanford vision website\n",
    "for archive_file in archive_file_list:\n",
    "    download_file(download_url=\"http://vision.stanford.edu/aditya86/ImageNetDogs/\" + archive_file,\n",
    "                  file_dir_path=\"../data\",\n",
    "                  file_name=archive_file,\n",
    "                  skip_if_dir_exists=False,\n",
    "                  force_dir_deletion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists.tar archive has been extracted successfully.\n",
      "File lists have been read successfully.\n",
      "Extracting images.tar archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "793583616it [05:15, 2514719.57it/s] \n",
      "483328it [04:24, 1826.80it/s]  \n",
      "100%|██████████| 20701/20701 [34:35<00:00,  9.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images.tar archive has been extracted successfully.\n",
      "Removing archive files.\n"
     ]
    }
   ],
   "source": [
    "# Extract archive files\n",
    "extract_stanford_dogs_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(\"../data/Images\")\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Pytorch dataset and dataloader classes to download the CIFAR-10 data to the AML Compute Instance / local compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation steps\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # mean, variance of channels\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"../data\", train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "testset = torchvision.datasets.CIFAR10(root=\"../data\", train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define classes\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first batch of 4 CIFAR-10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transpose dimensions from Pytorch format to default numpy format\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(\" \".join(\"%11s\" % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to the default AML datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=\"../data\", target_path=\"data/cifar-10\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Register AML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object from datastore location\n",
    "dataset = Dataset.File.from_files(path=(datastore, \"data/cifar-10\"))\n",
    "\n",
    "# Register the dataset\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name=\"cifar-10-dataset\",\n",
    "                           description=\"cifar-10 training dataset\",\n",
    "                           create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a remote compute target to run the experiment on. The below code will first check whether a compute target with name `cpu_cluster_name` already exists and if it does it will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the CPU cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to create your training script. In this tutorial, the training script is already provided for you at `pytorch_train.py`. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "However, if you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the training artifacts\n",
    "training_folder = os.path.join(os.getcwd(), \"../src/training\")\n",
    "os.makedirs(training_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training script is created in the aml_training folder. This folder will contain all artifacts needed for aml remote training and these will be copied to the remote compute at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import urllib\n",
    "\n",
    "from azureml.core import Run\n",
    "from model import Net\n",
    "from utils import download_and_extract_data\n",
    "from zipfile import ZipFile\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load the training and validation data.\"\"\"\n",
    "\n",
    "    # Apply data augmentation and normalization for training data\n",
    "    # Apply just normalization for validation data\n",
    "    data_transforms = {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.RandomResizedCrop(24), # Crop 24x24 image\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'val']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                  shuffle=True, num_workers=4)\n",
    "                   for x in ['train', 'val']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "\n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, data_dir):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    # load training/validation data\n",
    "    dataloaders, dataset_sizes, class_names = load_data(data_dir)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # log the best val accuracy to AML run\n",
    "            run.log('best_val_acc', np.float(best_acc))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(num_epochs, data_dir, learning_rate, momentum):\n",
    "    \"\"\"Load a pretrained model and reset the final fully connected layer.\"\"\"\n",
    "\n",
    "    # log the hyperparameter metrics to the AML run\n",
    "    run.log('lr', np.float(learning_rate))\n",
    "    run.log('momentum', np.float(momentum))\n",
    "\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)  # only 2 classes to predict\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(),\n",
    "                             lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "        optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    model = train_model(model_ft, criterion, optimizer_ft,\n",
    "                        exp_lr_scheduler, num_epochs, data_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "    # get command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_epochs', type=int, default=25,\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--output_dir', type=str, help='output directory')\n",
    "    parser.add_argument('--learning_rate', type=float,\n",
    "                        default=0.001, help='learning rate')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_dir = download_data()\n",
    "    print(\"data directory is: \" + data_dir)\n",
    "    model = fine_tune_model(args.num_epochs, data_dir,\n",
    "                            args.learning_rate, args.momentum)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    torch.save(model, os.path.join(args.output_dir, 'model.pt'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Path to the training data\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Learning rate for SGD\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9,help=\"Momentum for SGD\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"========== DATA ==========\")\n",
    "    print(\"Data Location: \" + args.data_path)\n",
    "    print(\"Available Files:\", os.listdir(args.data_path))\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Create dataloader for CIFAR-10 training data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=args.data_path, train=True, \n",
    "                                            download=False, transform=transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    # Leverage GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define convolutional network\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    # Set up pytorch cross entropy loss and SGD optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
    "\n",
    "    print(\"===== MODEL TRAINING =====\")\n",
    "    \n",
    "    # Train the network\n",
    "    for epoch in range(2):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Unpack the data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "            # Run forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                loss = running_loss / 2000\n",
    "                run.log(\"loss\", loss) # log loss metric to AML\n",
    "                print(f\"epoch={epoch + 1}, batch={i + 1:5}: loss {loss:.2f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    file_path = \"outputs/cifar_net.pt\"\n",
    "    torch.save(net.state_dict(), file_path) # Anything written to the outputs folder on remote compute is automatically uploaded to the run outputs \n",
    "    # run.upload_file(name=file_path, path_or_stream=file_path)\n",
    "    \n",
    "    print(\"Saved and uploaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from azureml.core import Run\n",
    "from model import Net\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Path to the training data\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Learning rate for SGD\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9,help=\"Momentum for SGD\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"========== DATA ==========\")\n",
    "    print(\"Data Location: \" + args.data_path)\n",
    "    print(\"Available Files:\", os.listdir(args.data_path))\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Create dataloader for CIFAR-10 training data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=args.data_path, train=True, \n",
    "                                            download=False, transform=transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    # Leverage GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define convolutional network\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    # Set up pytorch cross entropy loss and SGD optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
    "\n",
    "    print(\"===== MODEL TRAINING =====\")\n",
    "    \n",
    "    # Train the network\n",
    "    for epoch in range(2):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Unpack the data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "            # Run forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                loss = running_loss / 2000\n",
    "                run.log(\"loss\", loss) # log loss metric to AML\n",
    "                print(f\"epoch={epoch + 1}, batch={i + 1:5}: loss {loss:.2f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    file_path = \"outputs/cifar_net.pt\"\n",
    "    torch.save(net.state_dict(), file_path) # Anything written to the outputs folder on remote compute is automatically uploaded to the run outputs \n",
    "    # run.upload_file(name=file_path, path_or_stream=file_path)\n",
    "    \n",
    "    print(\"Saved and uploaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model file which contains the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training script locally for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/training/train.py --data_path ../data --learning_rate 0.003 --momentum 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../environments/conda/environment.yml\n",
    "\n",
    "name: pytorch-aml-env\n",
    "dependencies:\n",
    "- python=3.7.1\n",
    "- pytorch::pytorch=1.7.0\n",
    "- pytorch::torchvision=0.8.1\n",
    "- pip:\n",
    "    - azureml-defaults\n",
    "    - azureml-sdk\n",
    "    - azureml-widgets\n",
    "channels:\n",
    "- pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A yml file with conda environment specification is provided in `../environments/conda`. By instantiating an environment object, this conda environment can be used for the remote training run. Alternatively, AML curated environments can also be used. AML curated environments cover common ML scenarios and are backed by cached Docker images. Cached Docker images make the first remote run preparation faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display AML Curated Environments\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "#     if env.startswith(\"AzureML\"):\n",
    "#         print(\"Name\", env)\n",
    "#         print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List workspace environments\n",
    "# for name, env in ws.environments.items():\n",
    "#     print(f\"Name {name} \\t version {env.version}\")\n",
    "\n",
    "# # Retrieve an environment\n",
    "# env = Environment.get(workspace=ws, name=\"AzureML-PyTorch-1.3-CPU\", version=\"1\")\n",
    "\n",
    "# # Get base image of retrieved environment\n",
    "# print(env.docker.base_image)\n",
    "\n",
    "# print(\"\\n Attributes of retrieved environment:\")\n",
    "# env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first run in a given environment, Azure ML spends some time building the environment. On the subsequent runs, Azure ML keeps track of changes and uses the existing environment, resulting in faster run completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name=\"pytorch-aml-env\",\n",
    "                                           file_path=\"../environments/conda/environment.yml\")\n",
    "\n",
    "# Attribute docker.enabled controls whether to use Docker container or host OS for execution.\n",
    "# This is only relevant for local execution as execution on AML Compute Cluster will always use Docker container.\n",
    "# env.docker.enabled = True\n",
    "\n",
    "# Use Python dependencies from your Docker image (as opposed to from conda specification)\n",
    "# env.python.user_managed_dependencies=True\n",
    "\n",
    "## Only uncomment one of the three below options\n",
    "# OPTION 1: Use mcr base image\n",
    "#env.docker.base_image = \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20201113.v1\"\n",
    "#env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04' # GPU base image\n",
    "\n",
    "# Option 2: Use custom base image from workspace-native ACR\n",
    "#env.docker.base_image = \"eafc0c3ef9714c74a4fa655ee90531ba.azurecr.io/base/pytorch\"\n",
    "\n",
    "# OPTION 3: Use custom base image from standalone ACR and use admin user credentials. For this you need to enable admin user in the ACR.\n",
    "env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "env.docker.base_image_registry.address = \"sbirkacr.azurecr.io\"\n",
    "env.docker.base_image_registry.username = \"sbirkacr\"\n",
    "env.docker.base_image_registry.password = \"HqAu5Y2We0gZ42IunR5MBXkKc+shf2uj\" # replace with Key Vault\n",
    "\n",
    "# Option 4: Use custom base image from standalone ACR and use service principal authentication. \n",
    "#           The service principal needs the AcrPull permission on the standalone ACR.\n",
    "env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "env.docker.base_image_registry.address = \"sbirkacr.azurecr.io\"\n",
    "env.docker.base_image_registry.username = keyvault.get_secret(name=\"sbirk-acr-sp-username\")\n",
    "env.docker.base_image_registry.password = keyvault.get_secret(name=\"sbirk-acr-sp-password\")\n",
    "\n",
    "# Option 5: Use custom base image from standalone ACR with anonymous access preview feature.\n",
    "# env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "\n",
    "# Create an environment variable.\n",
    "# This can be retrieved in the training script with os.environ.get(\"MESSAGE\").\n",
    "# env.environment_variables = {\"MESSAGE\": \"Hello from Azure Machine Learning\"}\n",
    "\n",
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment & Run Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training artifacts are prepared, a model can be trained on the remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: \"Normal\" Script Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "experiment = Experiment(workspace=ws, \n",
    "                        name=\"cifar-image-classification-pytorch\")\n",
    "\n",
    "# Create the script run configuration\n",
    "config = ScriptRunConfig(source_directory=\"../src/training\",\n",
    "                         script=\"train.py\",\n",
    "                         compute_target=cpu_cluster_name,\n",
    "                         arguments=[\n",
    "                             \"--data_path\", dataset.as_named_input(\"input\").as_mount(),\n",
    "                             \"--learning_rate\", 0.003,\n",
    "                             \"--momentum\", 0.92])\n",
    "\n",
    "config.run_config.environment = env\n",
    "\n",
    "# Submit the run\n",
    "run = experiment.submit(config)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Hyperdrive Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get portal URL\n",
    "run.get_portal_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run metrics, details and file names\n",
    "print(run.get_metrics())\n",
    "print(run.get_details())\n",
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/cifar_net.pt\"\n",
    "\n",
    "model = run.register_model(model_name=\"cifar10-model\",\n",
    "                           model_path=model_path,\n",
    "                           model_framework=Model.Framework.PYTORCH,\n",
    "                           description=\"cifar10 model\")\n",
    "\n",
    "print(model.name, model.id, model.version, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "run.download_file(name=os.path.join(\"../\", model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu_cluster.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/get-started-day1/day1-part4-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"your-tenant-id\")\n",
    "Additional details on authentication can be found here: https://aka.ms/aml-notebook-auth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The very first run will take 5-10minutes to complete. This is because in the background a docker image is built in the cloud, the compute cluster is resized from 0 to 1 node, and the docker image is downloaded to the compute. Subsequent runs are much quicker (~15 seconds) as the docker image is cached on the compute - you can test this by resubmitting the code below after the first run has completed.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The first time you run this script, Azure Machine Learning will build a new docker image from your PyTorch environment. The whole run could take 5-10 minutes to complete. You can see the docker build logs in the widget by selecting the `20_image_build_log.txt` in the log files dropdown. This image will be reused in future runs making them run much quicker.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies in the Azure ML environment don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need to create an Azure Machine Learning _compute instance_. The benefits of a compute instance over a local machine (e.g. laptop) or cloud VM are as follows:\n",
    "\n",
    "* It is a pre-configured with all the latest data science libaries (e.g. panads, scikit, TensorFlow, PyTorch) and tools (Jupyter, RStudio). In this tutorial we make extensive use of PyTorch, AzureML SDK, matplotlib and we do not need to install these components on a compute instance.\n",
    "* Notebooks are seperate from the compute instance - this means that you can develop your notebook on a small VM size, and then seamlessly scale up (and/or use a GPU-enabled) the machine when needed to train a model.\n",
    "* You can easily turn on/off the instance to control costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "samkemp"
   }
  ],
  "categories": [
   "tutorials",
   "get-started-day1"
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
