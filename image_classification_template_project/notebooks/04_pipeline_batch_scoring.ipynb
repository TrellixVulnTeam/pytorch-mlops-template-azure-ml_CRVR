{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Setup](#Setup)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Data](#Data)\n",
    "    * [Create and Register Input Dataset](#Create-and-Register-Input-Dataset)\n",
    "    * [Create Output PipelineData](#Create-Output-PipelineData)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Pipeline Run Environment](#Pipeline-Run-Environment)\n",
    "* [Pipeline Artifacts & Configuration](#Pipeline-Artifacts-&-Configuration)\n",
    "    * [Scoring Script](#Scoring-Script)\n",
    "    * [Parallel Run Configuration](#Parallel-Run-Configuration)\n",
    "    * [Parallel Run Step](#Parallel-Run-Step)\n",
    "* [Pipeline Run](#Pipeline-Run)\n",
    "    * [Submit Experiment](#Submit-Experiment)\n",
    "    * [Download & Inspect Pipeline Output](#Download-&-Inspect-Pipeline-Output)\n",
    "* [Publish the Pipeline](#Publish-the-Pipeline)\n",
    "    * [Configure Service Principal Secret](#Configure-Service-Principal-Secret)\n",
    "    * [Retrieve Authentication Header](#Retrieve-Authentication-Header)\n",
    "* [Trigger a Published Pipeline Run](#Trigger-a-Published-Pipeline-Run)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a batch scoring pipeline is published and triggered which uses the PyTorch model trained in the **02_model_training** notebook to run inference on mini-batches of images in a parallel manner on a compute cluster. In general, machine learning pipelines help to optimize the workflow in terms of speed, portability and reuse. After building and publishing the pipeline, a REST endpoint can be used to trigger the pipeline from any HTTP library on any platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import azureml.core\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tempfile\n",
    "from azureml.core import Environment, Experiment, Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.core.run import PipelineRun\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the Azure Machine Learning (AML) workspace, a workspace object needs to be instantiated using the Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace.\n",
    "# For alternative connection options (e.g. for automated workloads) see the aml_snippets directory.\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Register Input Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset object which will be used to read data from the workspace default datastore. \n",
    "\n",
    "**Note**: To transfer intermediate data between pipeline steps, a PipelineData object should be used instead.\n",
    "While here the pipeline will consist of a single step only, a usual flow with multiple steps will include:\n",
    "* Using Dataset objects as inputs to fetch raw data, performing some transformations, then outputting a PipelineData object.\n",
    "* Use the previous step's PipelineData output object as an input object, repeated for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset using the default datastore\n",
    "# Here the validation set will be used for batch scoring (to exemplify the process;\n",
    "# in reality a folder with new data that you want to score should be used instead -> data/batchscoring)\n",
    "datastore = ws.get_default_datastore()\n",
    "input_images = Dataset.File.from_files((datastore, \"data/fowl_data/val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset to the AML workspace\n",
    "input_images = input_images.register(workspace=ws, name=\"fowl-dataset-batch-scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output PipelineData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PipelineData object which will be used as a pointer to the datastore and will determine where to output the batch scoring results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name=\"batch_scoring_results\", datastore=datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a remote compute target to run experiments on. The below code will first check whether a compute target with name `cluster_name` already exists and if it does, it will use that instead of creating a new one.\n",
    "\n",
    "AML pipelines cannot be run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the CPU cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a registered environment from the AML workspace to run the pipeline in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"pytorch-aml-env\"\n",
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Artifacts & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the pipeline to use the model, a batch scoring script is created (which is different from the live inference scoring script). \n",
    "\n",
    "This script takes a minibatch of input images, applies the classification model, and outputs the predictions to a results file.\n",
    "\n",
    "The script batch_scoring.py takes the following parameters, which get passed from the ParallelRunStep that is created later in the notebook:\n",
    "\n",
    "* --model_name: the name of the model being used\n",
    "\n",
    "The pipelines infrastructure uses the ArgumentParser class to pass parameters into pipeline steps. For example, in the code below the first argument --model_name is given the property identifier model_name. In the main() function, this property is accessed using Model.get_model_path(args.model_name).\n",
    "\n",
    "The pipeline here only has one step and writes the output to a file, but for multi-step pipelines, ArgumentParser is also used to define a directory to write output data for input to subsequent steps. An example for such a pattern can be found [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/batch_pipeline_deployment/batch_score.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from azureml.core.model import Model\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def init():\n",
    "    \n",
    "    global model\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Start the Pytorch model serving\")\n",
    "    parser.add_argument(\"--model_name\", dest=\"model_name\", required=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    model_path = Model.get_model_path(args.model_name)\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "def run(mini_batch):\n",
    "    \n",
    "    result_list = []\n",
    "    for file_path in mini_batch:\n",
    "        image = preprocess_image(file_path)\n",
    "        \n",
    "        # get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            classes = [\"chicken\", \"turkey\"]\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            pred_probs = softmax(output).numpy()[0]\n",
    "            index = torch.argmax(output, 1)\n",
    "            result = os.path.basename(file_path) + \", \" + str(classes[index]) + \", \" + str(pred_probs[index])\n",
    "            result_list.append(result)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "\n",
    "def preprocess_image(image_file):\n",
    "    \"\"\"\n",
    "    Preprocess an input image.\n",
    "    :param image_file: Path to the input image\n",
    "    :return image: preprocessed image as torch tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image)\n",
    "    image = image[np.newaxis, ...]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Run Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parallel run configuration to wrap the inference script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run_config = ParallelRunConfig(\n",
    "    environment=env,\n",
    "    entry_script=\"batch_score.py\",\n",
    "    source_directory=\"../src/batch_pipeline_deployment\",\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"parallel_run_step.txt\",\n",
    "    mini_batch_size=\"10\",\n",
    "    error_threshold=1,\n",
    "    compute_target=compute_target,\n",
    "    process_count_per_node=2,\n",
    "    node_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Run Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline step. \n",
    "\n",
    "A pipeline step is an object that encapsulates everything that is needed for running a pipeline including:\n",
    "* environment and dependency settings\n",
    "* the compute target to run the pipeline on\n",
    "* input and output data, and any custom parameters\n",
    "* reference to a script or SDK-logic to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class PipelineStep to assist with building a step using certain frameworks and stacks. Here, a ParallelRunStep class is used to define the step logic using a scoring script.\n",
    "\n",
    "An object reference in the outputs array becomes available as an input for a subsequent pipeline step, for scenarios where there is more than one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_step_name = \"batchscoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "batch_score_step = ParallelRunStep(\n",
    "    name=parallel_step_name,\n",
    "    inputs=[input_images.as_named_input(\"input_images\")],\n",
    "    output=output_dir,\n",
    "    arguments=[\"--model_name\", \"fowl-model\"],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/en-gb/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pipeline object with all the configured pipeline steps as well as an experiment object to submit the pipeline for execution.\n",
    "\n",
    "Note: The first pipeline run takes roughly 15 minutes, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. However, total run time depends on the workload of your scripts and processes running in each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[batch_score_step])\n",
    "pipeline_run = Experiment(ws, \"pytorch-fowl-batch-scoring\").submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for completion of the run and show output log\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Inspect Pipeline Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_run = pipeline_run.find_step_run(batch_score_step.name)[0]\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\n",
    "\n",
    "target_dir = tempfile.mkdtemp()\n",
    "batch_output.download(local_path=target_dir)\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\n",
    "\n",
    "df = pd.read_csv(result_file, delimiter=\",\", header=None)\n",
    "df.columns = [\"Filename\", \"Prediction\", \"Probability\"]\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\n",
    "\n",
    "# Show first 30 rows of output file\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check path on datastore\n",
    "batch_output.path_on_datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish the pipeline to create a REST endpoint that allows to rerun the pipeline from any HTTP library on any platform. The published pipeline can also be run from the AML workspace where different metdata such as run history and duration are tracked as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"fowl-pytorch-scoring\",\n",
    "    description=\"Batch scoring using fowl pytorch model\",\n",
    "    version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the pipeline using the REST endpoint, an OAuth2 Bearer-type authentication header is needed. For an automated workflow in a production scenario, a service principal (App Registration) should be created to retrieve this authentication header. For more information on how to create a service principal from the Azure Portal, check [this](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal). A secret for the service principal needs to be created afterwards and the service principal needs to be granted role access to the AML workspace. The service principal password will be retrieved from Azure Key Vault."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Service Principal Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells only need to be run once. They will retrieve the service principal secret from the .env configuration file and write it to the workspace keyvault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "env_path = Path(\"../config/\") / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keyvault object\n",
    "keyvault = ws.get_default_keyvault()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set keyvault secret\n",
    "svc_pr_pw = os.environ.get(\"AML_SERVICE_PRINCIPAL_PASSWORD\")\n",
    "keyvault.set_secret(name=\"svc-pr-pw\", value = svc_pr_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Authentication Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the service principal authentication to retrieve an OAuth2 Bearer-type authentication header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pr_pw = keyvault.get_secret(name=\"svc-pr-pw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=\"461e2020-109b-4c43-ad3f-eb9944f5dc44\",\n",
    "    service_principal_id=\"8a0b5ebf-55c7-4dfa-a49c-37b0acd9c3ce\",\n",
    "    service_principal_password=svc_pr_pw)\n",
    "\n",
    "auth_header = svc_pr.get_authentication_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger a Published Pipeline Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the REST url from the endpoint property of the published pipeline object. The REST url can also be found in the AML workspace in the portal.\n",
    "\n",
    "Build an HTTP POST request to the endpoint, specifying the authentication header. Additionally, add a JSON payload object with the experiment name and the configuration parameter assignments. Here, the process_count_per_node is used as an example parameter, which is passed through to ParallelRunStep because it is defined in the step configuration.\n",
    "\n",
    "Make the request to trigger the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"pytorch-fowl-batch-scoring-request\",\n",
    "                               \"ParameterAssignments\": {\"process_count_per_node\": 6}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the Id key from the response dict to get the value of the run id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
    "                    \"Response Code: {}\\n\"\n",
    "                    \"Headers: {}\\n\"\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get(\"Id\")\n",
    "print(\"Submitted pipeline run: \", run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, use the run id to monitor the status of the new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run = PipelineRun(ws.experiments[\"pytorch-fowl-batch-scoring-request\"], run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed info about the run\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-aml-env",
   "language": "python",
   "name": "pytorch-aml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
