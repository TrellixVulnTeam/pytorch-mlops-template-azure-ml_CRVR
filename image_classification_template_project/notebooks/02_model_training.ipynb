{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Setup](#Setup)\n",
    "    * [Connect to Workspace](#Connect-to-Workspace)\n",
    "* [Data](#Data)\n",
    "    * [Download Data](#Download-Data)\n",
    "    * [Explore Data](#Explore-Data)\n",
    "    * [Upload Data](#Upload-Data)\n",
    "    * [Create and Register AML Datasets](#Create-and-Register-AML-Datasets)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Training Artifacts](#Training-Artifacts)\n",
    "* [Training Environment](#Development-Environment)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Development Environment](#Development-Environment)\n",
    "* [Experiment & Run Configuration](#Experiment-&-Run-Configuration)\n",
    "    * [Option 1: Normal Script Run](#Option-1:-Normal-Script-Run)\n",
    "    * [Option 2: Hyperdrive Run](#Option-2:-Hyperdrive-Run)\n",
    "* [Model Registration](#Model-Registration)\n",
    "    * [Model Download](#Model-Download)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append parent directory to sys path to be able to import modules from src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import azureml.core\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import uuid\n",
    "\n",
    "from azureml.core.authentication import MsiAuthentication\n",
    "from azureml.core import Dataset, Environment, Experiment, Keyvault, Model, ScriptRunConfig, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal, RandomParameterSampling\n",
    "from azureml.train.hyperdrive import choice, uniform\n",
    "from azureml.widgets import RunDetails\n",
    "from torchvision import datasets\n",
    "\n",
    "from src.training.data_utils import load_data, show_image\n",
    "from src.training.download_utils import download_file, extract_stanford_dogs_archive\n",
    "\n",
    "print(f\"azureml.core version: {azureml.core.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically reload modules when changes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training directory. This directory will contain all artifacts needed for model training. For AML remote training this directory will be copied to the remote compute at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = os.path.join(os.getcwd(), \"../src/training\")\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "print(f\"Training folder {training_folder} has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(), \"../data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "print(f\"Data folder {data_folder} has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the Azure Machine Learning (AML) workspace, a workspace object needs to be instantiated using the Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AML workspace. MsiAuthenthication only works out of the box on the AML Compute Instance.\n",
    "# For alternative connection options see the aml_snippets directory.\n",
    "msi_auth = MsiAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "               resource_group=\"amlbrikserg\",\n",
    "               workspace_name=\"amlbriksews\",\n",
    "               auth=msi_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this template, the CIFAR-10 image dataset is used for multiclass classification. It has the classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The images in CIFAR-10 are three-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Pytorch dataset and dataloader classes to download the CIFAR-10 data to the AML Compute Instance / local compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation steps\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # mean, variance of channels\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"../data\", train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "testset = torchvision.datasets.CIFAR10(root=\"../data\", train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define classes\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first batch of 4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transpose dimensions from Pytorch format to default numpy format\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(\" \".join(\"%10s\" % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to the default AML datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=\"../data\", target_path=\"data/cifar-10\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Register AML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the data as a dataset in the AML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object from datastore location\n",
    "dataset = Dataset.File.from_files(path=(datastore, \"data/cifar-10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name=\"cifar-10-dataset\",\n",
    "                           description=\"cifar-10 training dataset\",\n",
    "                           create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a remote compute target to run experiments on. The below code will first check whether a compute target with name `cluster_name` already exists and if it does, it will use that instead of creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the CPU cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training script is created in the aml_training folder. This script will be executed by the remote compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from azureml.core import Run\n",
    "from model import Net\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Path to the training data\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Learning rate for SGD\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9,help=\"Momentum for SGD\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"========== DATA ==========\")\n",
    "    print(\"Data Location: \" + args.data_path)\n",
    "    print(\"Available Files:\", os.listdir(args.data_path))\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Create dataloader for CIFAR-10 training data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=args.data_path, train=True, \n",
    "                                            download=False, transform=transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    # Leverage GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define convolutional network\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    # Set up pytorch cross entropy loss and SGD optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.learning_rate, momentum=args.momentum)\n",
    "\n",
    "    print(\"===== MODEL TRAINING =====\")\n",
    "    \n",
    "    # Train the network\n",
    "    for epoch in range(2):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Unpack the data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "            # Run forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                loss = running_loss / 2000\n",
    "                run.log(\"loss\", loss) # log loss metric to AML\n",
    "                print(f\"epoch={epoch + 1}, batch={i + 1:5}: loss {loss:.2f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    file_path = \"outputs/cifar_net.pt\"\n",
    "    torch.save(net.state_dict(), file_path) # Anything written to the outputs folder on remote compute is automatically uploaded to the run outputs \n",
    "    # run.upload_file(name=file_path, path_or_stream=file_path)\n",
    "    \n",
    "    print(\"Saved and uploaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model file which contains the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training script locally for 1 epoch for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/training/train.py --data_path ../data --learning_rate 0.003 --momentum 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment that has been registered as part of the **01_environment_creation** notebook and use it for remote training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"pytorch-aml-env\"\n",
    "env = Environment.get(workspace=ws, name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment & Run Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training artifacts are prepared, a model can be trained on the remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Normal Script Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "experiment = Experiment(workspace=ws, \n",
    "                        name=\"cifar-image-classification-pytorch\")\n",
    "\n",
    "# Create the script run configuration\n",
    "src_config = ScriptRunConfig(source_directory=\"../src/training\",\n",
    "                             script=\"train.py\",\n",
    "                             compute_target=compute_target,\n",
    "                             arguments=[\n",
    "                                 \"--data_path\", dataset.as_named_input(\"input\").as_mount(),\n",
    "                                 \"--learning_rate\", 0.003,\n",
    "                                 \"--momentum\", 0.92])\n",
    "\n",
    "src_config.run_config.environment = env\n",
    "\n",
    "# Start the Script Run\n",
    "run = experiment.submit(src_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Hyperdrive Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be tuned using AML's hyperdrive capability.\n",
    "\n",
    "The initial learning rate is tuned. The training script can contain a LR schedule to decay the learning rate every several epochs starting from that initial learning rate.\n",
    "\n",
    "Random sampling is used to try different configuration sets of hyperparameters to maximize the primary metric, the best validation accuracy (best_val_acc).\n",
    "\n",
    "An early termination policy is specified to early terminate poorly performing runs. The BanditPolicy is used, which will terminate any run that doesn't fall within the slack factor of the primary evaluation metric. In this template, this policy will be applied every epoch (since the best_val_acc metric is reported every epoch and evaluation_interval=1). The first policy evaluation will be delayed until after the first 10 epochs (delay_evaluation=10). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = RandomParameterSampling({\n",
    "    \"learning_rate\": uniform(0.0005, 0.005),\n",
    "    \"momentum\": uniform(0.9, 0.99)}\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "# Create the script run configuration\n",
    "src_config = ScriptRunConfig(source_directory=\"../src/training\",\n",
    "                             script=\"train.py\",\n",
    "                             compute_target=compute_target,\n",
    "                             arguments=[\n",
    "                                 \"--data_path\", dataset.as_named_input(\"input\").as_mount(),\n",
    "                                 \"--learning_rate\", 0.003,\n",
    "                                 \"--momentum\", 0.92])\n",
    "\n",
    "src_config.run_config.environment = env\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=src_config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name=\"best_val_acc\",\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=4,\n",
    "                                     max_concurrent_runs=2)\n",
    "\n",
    "# Start the Hyperdrive Run\n",
    "run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the HyperDrive or Script run\n",
    "run = experiment.submit(hyperdrive_config)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get portal URL\n",
    "run.get_portal_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run metrics, details and file names\n",
    "print(run.get_metrics())\n",
    "print(\"==========================\")\n",
    "print(run.get_details())\n",
    "print(\"==========================\")\n",
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "try:\n",
    "    run = hyperdrive_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/cifar_net.pt\"\n",
    "\n",
    "model = run.register_model(model_name=\"cifar10-model\",\n",
    "                           model_path=model_path,\n",
    "                           model_framework=Model.Framework.PYTORCH,\n",
    "                           description=\"cifar10 model\")\n",
    "\n",
    "print(model.name, model.id, model.version, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model=True\n",
    "\n",
    "if download_model:\n",
    "    \n",
    "    # Create directory\n",
    "    outputs_folder = os.path.join(os.getcwd(), \"../outputs\")\n",
    "    os.makedirs(outputs_folder, exist_ok=True)\n",
    "    print(f\"Outputs folder {outputs_folder} has been created.\")\n",
    "    \n",
    "    # Download model artifact\n",
    "    run.download_file(name=model_path, output_file_path=\"../outputs/cifar_net.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "samkemp"
   }
  ],
  "categories": [
   "tutorials",
   "get-started-day1"
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
