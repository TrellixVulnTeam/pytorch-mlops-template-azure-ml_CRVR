{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS:\n",
    "---\n",
    "* [Setup](#Setup)\n",
    "* [Workspace](#Workspace)\n",
    "* [Key Vault](#Key-Vault)\n",
    "* [Data](#Data)\n",
    "* [Compute Targets](#Compute-Targets)\n",
    "* [Training Artifacts](#Training-Artifacts)\n",
    "* [Development Environment](#Development-Environment)\n",
    "* [Compute Target](#Compute-Target)\n",
    "* [Development Environment](#Development-Environment)\n",
    "* [Experiment & Run Configuration](#Experiment-&-Run-Configuration)\n",
    "    * [Option 1: Normal Script Run](#Option-1-Script-Run)\n",
    "    * [Option 2: Hyperdrive Run](#Option-1-Script-Run)\n",
    "* [Model Registration](#Model-Registration)\n",
    "* [Resource Clean Up](#Resource-Clean-Up)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = env.get_image_details(ws)\n",
    "print(details['ingredients']['dockerfile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append parent directory to sys path to be able to import modules from src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml.core version: 1.20.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms4\n",
    "import uuid\n",
    "\n",
    "from azureml.core.authentication import AzureCliAuthentication, InteractiveLoginAuthentication, MsiAuthentication, ServicePrincipalAuthentication\n",
    "from azureml.core import Dataset, Environment, Experiment, Keyvault, Model, ScriptRunConfig, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal, RandomParameterSampling\n",
    "from azureml.train.hyperdrive import choice, uniform\n",
    "from azureml.widgets import RunDetails\n",
    "from torchvision import datasets\n",
    "\n",
    "from src.training.data_utils import load_data, show_image\n",
    "from src.training.download_utils import download_file, extract_stanford_dogs_archive\n",
    "\n",
    "print(f\"azureml.core version: {azureml.core.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect and communicate with the Azure Machine Learning (AML) workspace, a workspace object needs to be instantiated using the Azure ML SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Create Workspace Object from Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option requires a config.json file containing the subscription id, resource group and workspace name. On an AML Compute Instance, this is available by default. For any other compute, the file can be downloaded from the workspace (see image below) and put in the same directory as the calling file, a subdirectory named .azureml, or in a parent directory. Alternatively, the path to the file can also be manually specified using the `path` argument. This option will prompt the user for interactive authentication with an AD user using the browser (at the first time).\n",
    "\n",
    "<img src=\"../docs/images/config_file.png\" alt=\"config_file\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: amlbriksews\n",
      "Azure region: westeurope\n",
      "Subscription id: bf088f59-f015-4332-bd36-54b988be7c90\n",
      "Resource group: amlbrikserg\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config() \n",
    "print(\"Workspace name: \" + ws.name, \n",
    "      \"Azure region: \" + ws.location, \n",
    "      \"Subscription id: \" + ws.subscription_id, \n",
    "      \"Resource group: \" + ws.resource_group, sep=\"\\n\")\n",
    "\n",
    "# Retrieve workspace details\n",
    "# ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Create Workspace from Connection Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option will prompt the user for interactive authentication with an AD user using the browser (at the first time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws = Workspace.get(name=\"amlbriksews\",\n",
    "#                    subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "#                    resource_group=\"amlbrikserg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Use Explicit Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 explicit authentication methods:\n",
    "- Interactive Authentication\n",
    "- Azure CLI Authentication\n",
    "- Managed Service Identity (MSI) Authentication\n",
    "- Service Principal Authentication\n",
    "\n",
    "The interactive authentication is suitable for local experimentation on your own computer. Azure CLI authentication is suitable if you are already using Azure CLI for managing Azure resources, and want to sign in only once. The MSI and Service Principal authentication are suitable for automated workflows, for example as part of Azure Devops build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive authentication is the default mode when using Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_auth = InteractiveLoginAuthentication(tenant_id=\"461e2020-109b-4c43-ad3f-eb9944f5dc44\", force=True)\n",
    "# # Force can be set to True to not use cached authentication token\n",
    "\n",
    "# ws = Workspace(subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "#                resource_group=\"amlbrikserg\",\n",
    "#                workspace_name=\"amlbriksews\",\n",
    "#                auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure CLI Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the azure-cli package is installed, and **az login** command has been used to log in to an Azure Subscription, the AzureCliAuthentication class can be used. For testing run the below command in the terminal and follow the login instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`az login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cli_auth = AzureCliAuthentication()\n",
    "\n",
    "# ws = Workspace(subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "#                resource_group=\"amlbrikserg\",\n",
    "#                workspace_name=\"amlbriksews\",\n",
    "#                auth=cli_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSI Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: MSI authentication is supported only when using the Azure ML SDK from an Azure VM. As a prerequisite, enable System-assigned Managed Identity for your VM. Then, assign the VM access to your Workspace. On the AML Compute Instance this works out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msi_auth = MsiAuthentication()\n",
    "\n",
    "# ws = Workspace(subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "#                resource_group=\"amlbrikserg\",\n",
    "#                workspace_name=\"amlbriksews\",\n",
    "#                auth=msi_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Service Principal Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up a machine learning workflow as an automated process, it is recommended to use Service Principal Authentication. This approach decouples the authentication from any specific user login, and allows managed access control. As a prerequisite, create a Service Principal (App Registration). Then, assign the Service Principal access to your Workspace. It is strongly recommended that the secrets are not hardcoded. Instead, an environment variable can be used to pass secrets to the code, for example through Azure Key Vault, or through secret build variables in Azure DevOps. For local testing, the following bash command or the dotenv python library can be used to set environment variables.\n",
    "\n",
    "`$ export AZUREML_SP_PASSWORD=\"mypassword\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n",
    "\n",
    "svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=\"461e2020-109b-4c43-ad3f-eb9944f5dc44\",\n",
    "    service_principal_id=\"my-application-id\",\n",
    "    service_principal_password=svc_pr_password)\n",
    "\n",
    "\n",
    "ws = Workspace(subscription_id=\"bf088f59-f015-4332-bd36-54b988be7c90\",\n",
    "               resource_group=\"amlbrikserg\",\n",
    "               workspace_name=\"amlbriksews\",\n",
    "               auth=svc_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Vault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, there might be a need to pass a secret to either a remote run, for example username and password to authenticate against an external data source, or to an experiment control script (like this one), for example service principal id and password to authenticate with a Service Principal against the workspace. The Azure ML SDK enables these use cases through the Key Vault associated with the workspace where secrets can be stored in a safe way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store a secret in the Key Vault, an environment variable can be used to temporarily hold that secret. In a bash shell, an environment variable **LOCAL_SECRET** holding the value **12345** can be created as follows:\n",
    "\n",
    "`$ export LOCAL_SECRET=\"12345\"`\n",
    "\n",
    "The environment variable can be printed to the terminal in the following way:\n",
    "\n",
    "`$ echo $LOCAL_SECRET`\n",
    "\n",
    "It can be passed into a Python variable using the os library and set as a secret using the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8e01b088ea68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlocal_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOCAL_SECRET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use random UUID as a substitute for real secret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkeyvault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_keyvault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkeyvault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"secret_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ws' is not defined"
     ]
    }
   ],
   "source": [
    "local_secret = os.environ.get(\"LOCAL_SECRET\", default = str(uuid.uuid4())) # Use random UUID as a substitute for real secret\n",
    "keyvault = ws.get_default_keyvault()\n",
    "keyvault.set_secret(name=\"secret_name\", value = local_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault.list_secrets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_secret = keyvault.get_secret(name=\"secret_name\")\n",
    "local_secret==retrieved_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In submitted runs on local and remote compute, you can use the get_secret method of Run instance to get the secret value from Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_secret.py\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context()\n",
    "secret_value = run.get_secret(name=\"secret-name\")\n",
    "print(\"Got secret value {} , but don't write it out!\".format(len(secret_value) * \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) is an image dataset that will be used to train a multiclass dog breed classification model. In total there are 120 different dog breeds/classes and 20580 images. The dataset has been built using images and annotations from ImageNet for the task of fine-grained image categorization. The images are three-channel color images of variable pixels in size. While a file with a given train/test split can be downloaded from the website, the test dataset will be further split into a validation and real test set (50:50). This will ultimately lead into a data distribution as follows:\n",
    "- 12013 training images (58.34%)\n",
    "- 4290 validation images (20.83%)\n",
    "- 4290 test images (20.83%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a utility file with functions to download the dogs dataset archive files from the stanford vision website and extract the archive into a format expected by the [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/amlbrikseci/code/Users/BRIKSE/pytorch-use-cases-azure-ml/template_project/notebooks/../src/training/download_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/download_utils.py\n",
    "import os\n",
    "import scipy.io\n",
    "import shutil\n",
    "import tarfile\n",
    "import tqdm\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_file(download_url: str,\n",
    "                  file_dir: str,\n",
    "                  file_name: str,\n",
    "                  skip_if_dir_exists: bool = False,\n",
    "                  force_dir_deletion: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Download a file\n",
    "    :param download_url: url from where to download\n",
    "    :param file_dir: directory to which to download\n",
    "    :param file_name: name of the file\n",
    "    :param skip_if_dir_exists: flag that indicates whether to skip the download if the directory already exists\n",
    "    :param force_dir_deletion: flag that indicates whether to delete the existing directory before the download\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove file directory if it exists\n",
    "    if force_dir_deletion and os.path.exists(file_dir):\n",
    "        shutil.rmtree(file_dir)\n",
    "        print(f\"Directory {file_dir} has been removed.\")\n",
    "    \n",
    "    # Check if download should be triggered\n",
    "    if not os.path.exists(file_dir) or not skip_if_dir_exists:\n",
    "    \n",
    "        # Create file directory if it does not exist\n",
    "        os.makedirs(file_dir, exist_ok=True)\n",
    "    \n",
    "        # Download the file\n",
    "        file_path = os.path.join(file_dir, file_name)\n",
    "        print(\"Downloading \" + download_url + \" to \" + file_path + \".\")\n",
    "        urllib.request.urlretrieve(download_url, filename=file_path, reporthook=generate_bar_updater())\n",
    "        \n",
    "\n",
    "def extract_stanford_dogs_archive(archive_dir_path: str = \"../data\",\n",
    "                                  target_dir_path: str = \"../data\",\n",
    "                                  remove_archives: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Extract the stanford dogs image archive and separate the images into training,\n",
    "    validation and test set\n",
    "    :param archive_dir_path: path of the \"image.tar\" and \"lists.tar\" files to be extracted\n",
    "    :param target_dir_path: path of the target directory where the files should be extracted to\n",
    "    :param remove_archives: flag that indicates whether the archives are removed after extraction\n",
    "    \"\"\"\n",
    " \n",
    "    # Specify directory paths\n",
    "    training_dir = os.path.join(target_dir_path, \"train\")\n",
    "    validation_dir = os.path.join(target_dir_path, \"val\")\n",
    "    test_dir = os.path.join(target_dir_path, \"test\")    \n",
    "    \n",
    "    # Remove directories if they exist\n",
    "    for directory in [training_dir, validation_dir, test_dir]:\n",
    "        if os.path.exists(directory):\n",
    "            shutil.rmtree(directory)\n",
    "            print(f\"Directory {directory} has been removed.\")\n",
    "\n",
    "    # Extract lists.tar archive\n",
    "    with tarfile.open(os.path.join(archive_dir_path, \"lists.tar\"), \"r\") as lists_tar:\n",
    "        lists_tar.extractall(path=archive_dir_path)\n",
    "                             \n",
    "    print(\"Lists.tar archive has been extracted successfully.\")\n",
    "    \n",
    "    # Load list.mat files\n",
    "    train_list_mat = scipy.io.loadmat(os.path.join(archive_dir_path, \"train_list.mat\"))\n",
    "    test_list_mat = scipy.io.loadmat(os.path.join(archive_dir_path, \"test_list.mat\"))\n",
    "    \n",
    "    training_files = []\n",
    "    test_and_val_files = []\n",
    "    \n",
    "    # Extract training data file names\n",
    "    for array in train_list_mat[\"file_list\"]:\n",
    "        training_files.append(array[0][0])\n",
    "\n",
    "    # Extract test data file names\n",
    "    for array in test_list_mat[\"file_list\"]:\n",
    "        test_and_val_files.append(array[0][0])\n",
    "                             \n",
    "    print(\"File lists have been read successfully.\")\n",
    "    print(\"Extracting images.tar archive...\")\n",
    "                             \n",
    "    # Extract images.tar archive\n",
    "    with tarfile.open(os.path.join(archive_dir_path, \"images.tar\"), \"r\") as images_tar:\n",
    "        test_val_idx = 0\n",
    "        for member in tqdm.tqdm(images_tar.getmembers()):\n",
    "            if member.isreg(): # Skip if TarInfo is not files\n",
    "                member.name = member.name.split(\"/\", 1)[1] # Retrieve only relevant part of file name\n",
    "                \n",
    "                # Extract files to corresponding directories\n",
    "                if member.name in training_files:\n",
    "                    images_tar.extract(member, training_dir)\n",
    "                    \n",
    "                elif member.name in test_and_val_files: # Every 2nd file goes to the validation data\n",
    "                    test_val_idx+=1\n",
    "                    if test_val_idx % 2 != 0:\n",
    "                        images_tar.extract(member, validation_dir)\n",
    "                    else:\n",
    "                        images_tar.extract(member, test_dir)\n",
    "                             \n",
    "    print(\"Images.tar archive has been extracted successfully.\")\n",
    "\n",
    "    # Remove list.mat files\n",
    "    os.remove(os.path.join(archive_dir_path, \"file_list.mat\"))\n",
    "    os.remove(os.path.join(archive_dir_path, \"test_list.mat\"))\n",
    "    os.remove(os.path.join(archive_dir_path, \"train_list.mat\"))\n",
    "    \n",
    "    # Remove archive files if flag is set to true\n",
    "    if remove_archives:\n",
    "        print(\"Removing archive files.\")\n",
    "        os.remove(os.path.join(archive_dir_path, \"lists.tar\"))\n",
    "        os.remove(os.path.join(archive_dir_path, \"images.tar\"))\n",
    "\n",
    "                             \n",
    "def generate_bar_updater():\n",
    "    \"\"\"\n",
    "    Create a tqdm reporthook function for urlretrieve\n",
    "    :returns: bar_update function which can be used by urlretrieve \n",
    "              to display and update a progress bar\n",
    "    \"\"\"\n",
    "    \n",
    "    pbar = tqdm.tqdm(total=None)\n",
    "\n",
    "    # Define progress bar update function\n",
    "    def bar_update(count, block_size, total_size):\n",
    "        if pbar.total is None and total_size:\n",
    "            pbar.total = total_size\n",
    "        progress_bytes = count * block_size\n",
    "        pbar.update(progress_bytes - pbar.n)\n",
    "\n",
    "    return bar_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data to the local compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../data has been removed.\n",
      "Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar to ../data/images.tar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 791437312/793579520 [00:48<00:00, 20206760.29it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar to ../data/lists.tar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/481280 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 16384/481280 [00:00<00:03, 132883.44it/s]\u001b[A\n",
      "  9%|▊         | 40960/481280 [00:00<00:03, 139857.45it/s]\u001b[A\n",
      " 12%|█▏        | 57344/481280 [00:00<00:03, 128956.94it/s]\u001b[A\n",
      " 19%|█▊        | 90112/481280 [00:00<00:02, 146948.82it/s]\u001b[A\n",
      " 27%|██▋       | 131072/481280 [00:01<00:02, 170608.20it/s]\u001b[A\n",
      " 36%|███▌      | 172032/481280 [00:01<00:01, 192142.76it/s]\u001b[A\n",
      " 46%|████▌     | 221184/481280 [00:01<00:01, 219028.58it/s]\u001b[A\n",
      " 58%|█████▊    | 278528/481280 [00:01<00:00, 250994.84it/s]\u001b[A\n",
      " 71%|███████▏  | 344064/481280 [00:01<00:00, 287515.59it/s]\u001b[A\n",
      " 85%|████████▌ | 409600/481280 [00:01<00:00, 320181.53it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "archive_file_list = [\"images.tar\", \"lists.tar\"]\n",
    "force_dir_deletion_list = [True, False] # Delete directory before starting to download images.tar\n",
    "\n",
    "# Download archive files from the stanford vision website\n",
    "for i, archive_file in enumerate(archive_file_list):\n",
    "    download_file(download_url=\"http://vision.stanford.edu/aditya86/ImageNetDogs/\" + archive_file,\n",
    "                  file_dir=\"../data\",\n",
    "                  file_name=archive_file,\n",
    "                  skip_if_dir_exists=False,\n",
    "                  force_dir_deletion=force_dir_deletion_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists.tar archive has been extracted successfully.\n",
      "File lists have been read successfully.\n",
      "Extracting images.tar archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "793583616it [00:54, 14671536.01it/s]                               \n",
      "483328it [00:03, 144078.55it/s]                            \n",
      " 98%|█████████▊| 20283/20701 [28:58<00:33, 12.48it/s] "
     ]
    }
   ],
   "source": [
    "# Extract archive files\n",
    "extract_stanford_dogs_archive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a utility file with functions to generate train, val and test dataloaders and to show example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/data_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def load_data(data_dir: str) -> Tuple[dict, dict, list]:\n",
    "    \"\"\"\n",
    "    Load the train, val and test data.\n",
    "    :param data_dir: path where the images are stored\n",
    "    :return (dataloaders, dataset_sizes, class_names)\n",
    "        dataloaders: dictionary of train, val, and test torch dataloaders\n",
    "        dataset_sizes: dictionary of train, val and test torch dataset lengths\n",
    "        class_names: list of all classes\n",
    "    \"\"\"\n",
    "\n",
    "    # Data augmentation and normalization for training set\n",
    "    # Just normalization for validation and test set\n",
    "    data_transforms = {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.5, 1)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "        \"test\": transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of image datasets\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    # Dictionary of image dataloaders\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                                  shuffle=True, num_workers=2) \n",
    "                         for x in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    # Dictionary of dataset sizes\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    # List of class names\n",
    "    class_names = image_datasets[\"train\"].classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "\n",
    "def show_image(image_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and show an example image\n",
    "    :param image_path: path of the image to be loaded\n",
    "    \"\"\"\n",
    "    # Read in example image\n",
    "    img = mpimg.imread(image_path)\n",
    "\n",
    "    # Check format of image\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    # Show example image\n",
    "    imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataloaders, dataset_sizes, class_names = load_data(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display an example image. All images have different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(image_path=\"../data/train/n02085620-Chihuahua/n02085620_11140.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first batch of 4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transpose dimensions from Pytorch format to default numpy format\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(dataloaders[\"train\"])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(\"\\n\".join(\"%s\" % class_names[labels[j]].split('-')[1] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to the default AML datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_c2732f6b964349b499d99f2cc857dd1e"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=\"../data\", target_path=\"data/stanford_dogs\", overwrite=True, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Register AML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the data as a dataset in the AML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object from datastore location\n",
    "dataset = Dataset.File.from_files(path=(datastore, \"data/stanford_dogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name=\"stanford_dogs\",\n",
    "                           description=\"Stanford dogs dataset\",\n",
    "                           create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AML Compute Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a remote compute target to run experiments on. The below code will first check whether a compute target with name `cpu_cluster_name` already exists and if it does, it will use that instead of creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-01-18T15:17:11.553000+00:00', 'errors': None, 'creationTime': '2021-01-15T09:55:01.226729+00:00', 'modifiedTime': '2021-01-15T09:55:16.691497+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT2400S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for the CPU cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # CPU\n",
    "                                                           # vm_size='STANDARD_NC6', # GPU\n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=2400)\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use get_status() to get a detailed status for the current cluster\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Kubernetes Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_name = \"aks-cluster\" \n",
    "\n",
    "# Retrieve all computes from workspace\n",
    "cts = ws.compute_targets\n",
    "\n",
    "# Use existing cluster if available, else create a new one\n",
    "if aks_name in cts and cts[aks_name].type == \"AKS\":\n",
    "    print(\"Found existing AKS cluster, will use it!\")\n",
    "    aks_target = cts[aks_name]\n",
    "else:\n",
    "    print(\"Creating a new AKS cluster...\")\n",
    "    # Use the default provisioning config (no input parameters) or provide parameters to customize\n",
    "    # For example, to create a dev/test cluster, use:\n",
    "    prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "    # Example configuration to use an existing virtual network\n",
    "    # prov_config.vnet_name = \"mynetwork\"\n",
    "    # prov_config.vnet_resourcegroup_name = \"myresourcegroup\"\n",
    "    # prov_config.subnet_name = \"default\"\n",
    "    # prov_config.service_cidr = \"10.0.0.0/16\"\n",
    "    # prov_config.dns_service_ip = \"10.0.0.10\"\n",
    "    # prov_config.docker_bridge_cidr = \"172.17.0.1/16\"\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                      name = aks_name,\n",
    "                                      provisioning_configuration = prov_config)\n",
    "    print(\"Waiting for cluster creation completion...\")\n",
    "    aks_target.wait_for_completion(show_output=True)\n",
    "\n",
    "print(\"Cluster state:\", aks_target.provisioning_state)\n",
    "print(\"Cluster is ready!\", aks_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training script in the training directory. The training script will make use of transfer learning and use a pretrained Resnet18 model. The final fully connected layer of this model will be adjusted for multiclass classification with 120 target classes. All parameters of the model will then be trained on the stanford dogs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/amlbrikseci/code/Users/BRIKSE/pytorch-use-cases-azure-ml/template_project/notebooks/../src/training/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/train.py\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import urllib\n",
    "\n",
    "from azureml.core import Run\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from data_utils import load_data\n",
    "from model import Net\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "\n",
    "def train_model(model: torchvision.models,\n",
    "                criterion: torch.nn.modules.loss,\n",
    "                optimizer: torch.optim,\n",
    "                scheduler: torch.optim.lr_scheduler,\n",
    "                num_epochs: int,\n",
    "                dataloaders: dict,\n",
    "                dataset_sizes: dict) -> torchvision.models:\n",
    "    \"\"\"\n",
    "    Train the model on the stanford dogs dataset and track training\n",
    "    and validation loss and accuracy.\n",
    "    :param model: pretrained model which will be trained further\n",
    "    :param criterion: torch loss function\n",
    "    :param optimizer: torch optimizer\n",
    "    :param scheduler: torch learning rate scheduler\n",
    "    :param num_epochs: number of epochs to train the model\n",
    "    :param dataloaders: dictionary of torch dataloaders\n",
    "    :param dataset_sizes: dictionary with lengths of the training, val and test set\n",
    "    :return model: pretrained model with tuned final fully connected layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leverage GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load in weights of model\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_correct_preds = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                # Track history only if in training phase\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and gradient optimization only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_correct_preds += torch.sum(preds == labels.data)\n",
    "                \n",
    "            # Update learning rate if in training phase\n",
    "            if phase == \"train\":\n",
    "                scheduler.step() \n",
    "\n",
    "            # Average loss and accuracy over examples\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_correct_preds.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # Log the best val accuracy to AML run\n",
    "            run.log(\"best_val_acc\", np.float(best_acc))\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s.\")\n",
    "    print(f\"Best Val Acc: {best_acc:4f}\")\n",
    "          \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "          \n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(num_epochs: int,\n",
    "                    num_classes: int,\n",
    "                    dataloaders: dict,\n",
    "                    dataset_sizes: dict,\n",
    "                    learning_rate: float,\n",
    "                    momentum: float) -> torchvision.models:\n",
    "    \"\"\"\n",
    "    Load a pretrained model and reset the final fully connected layer.\n",
    "    :param num_epochs: number of epochs to train the model\n",
    "    :param num_classes: number of target classes \n",
    "        (supports binary and multiclass classification)\n",
    "    :param dataloaders: dictionary of torch dataloaders\n",
    "    :param dataset_sizes: dictionary with lengths of the training, val and test set\n",
    "    :param learning_rate: learning rate hyperparameter\n",
    "    :param momentum: momentum hyperparameter\n",
    "    :return model: pretrained model with tuned final fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(\"START TRAINING\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Log the hyperparameter metrics to the AML run\n",
    "    run.log(\"lr\", np.float(learning_rate))\n",
    "    run.log(\"momentum\", np.float(momentum))\n",
    "\n",
    "    # Load pretrained model and reset final fully connected layer to have num_classes output neurons\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Leverage GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Specify loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create SGD optimizer to optimize all parameters\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             momentum=momentum)\n",
    "                            \n",
    "    # Create scheduler to decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,\n",
    "                                           step_size=7,\n",
    "                                           gamma=0.1)\n",
    "    \n",
    "    # Start model training\n",
    "    model = train_model(model_ft, criterion, optimizer_ft,\n",
    "                        exp_lr_scheduler, num_epochs, dataloaders,\n",
    "                        dataset_sizes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    \n",
    "    # Retrieve command-line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Path where the images are stored\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=25, help=\"Number of epochs to train\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, help=\"Output directory\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"Momentum\")\n",
    "    args = parser.parse_args()\n",
    "         \n",
    "    print(\"-\" * 20)\n",
    "    print(\"LOAD DATA\")      \n",
    "    print(\"-\" * 20)\n",
    "          \n",
    "    # Load training and validation data\n",
    "    dataloaders, dataset_sizes, class_names = load_data(args.data_path)\n",
    "          \n",
    "    print(\"Data has been load successfully.\")\n",
    "        \n",
    "    # Train the model\n",
    "    model = fine_tune_model(num_epochs=args.num_epochs,\n",
    "                            num_classes=len(class_names),\n",
    "                            dataloaders=dataloaders,\n",
    "                            dataset_sizes=dataset_sizes,\n",
    "                            learning_rate=args.learning_rate,\n",
    "                            momentum=args.momentum)\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    torch.save(model, os.path.join(args.output_dir, \"model.pt\"))\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Model saved in {args.output_dir}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively to transfer learning, create a model file which contains the network architecture of a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/amlbrikseci/code/Users/BRIKSE/pytorch-use-cases-azure-ml/template_project/notebooks/../src/training/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $training_folder/model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training script locally for 1 epoch for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.6.0\n",
      "--------------------\n",
      "LOAD DATA\n",
      "--------------------\n",
      "Data has been load successfully.\n",
      "--------------------\n",
      "START TRAINING\n",
      "--------------------\n",
      "Attempted to log scalar metric lr:\n",
      "0.003\n",
      "Attempted to log scalar metric momentum:\n",
      "0.9\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "--------------------\n",
      "Train Loss: 3.6719 Acc: 0.1603\n",
      "Attempted to log scalar metric best_val_acc:\n",
      "0.0\n",
      "--------------------\n",
      "Val Loss: 2.8961 Acc: 0.3287\n",
      "Attempted to log scalar metric best_val_acc:\n",
      "0.32867132867132864\n",
      "--------------------\n",
      "Training completed in 27m 14s.\n",
      "Best Val Acc: 0.328671\n",
      "Model saved in ../outputs.\n"
     ]
    }
   ],
   "source": [
    "!python ../src/training/train.py --data_path \"../data\" --num_epochs 1 --output_dir \"../outputs\" --learning_rate 0.003 --momentum 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **environment.yml** file which contains all packages needed to create a conda environment for development, training and deployment. If the different stages (development, training and deployment) vary greatly, a separate conda environment file for each stage can be created. In that case they should be prefixed with their respective stage, e.g. **training_environment.yml**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../environments/conda/environment.yml\n",
    "name: pytorch-aml-env\n",
    "dependencies:\n",
    "- python=3.7.1\n",
    "- pytorch::pytorch=1.7.0\n",
    "- pytorch::torchvision=0.8.1\n",
    "- pip:\n",
    "    - azureml-defaults\n",
    "    - azureml-sdk\n",
    "    - azureml-widgets\n",
    "    - python-dotenv==0.15.0\n",
    "channels:\n",
    "- pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By instantiating an environment object, this conda environment can be used for the remote training run. Alternatively, AML curated environments can also be used. AML curated environments cover common ML scenarios and are backed by cached Docker images. Cached Docker images make the first remote run preparation faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display AML Curated Environments\n",
    "# envs = Environment.list(workspace=ws)\n",
    "\n",
    "# for env in envs:\n",
    "#     if env.startswith(\"AzureML\"):\n",
    "#         print(\"Name\", env)\n",
    "#         print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List workspace environments\n",
    "# for name, env in ws.environments.items():\n",
    "#     print(f\"Name {name} \\t version {env.version}\")\n",
    "\n",
    "# # Retrieve an environment\n",
    "# env = Environment.get(workspace=ws, name=\"AzureML-PyTorch-1.3-CPU\", version=\"1\")\n",
    "\n",
    "# # Get base image of retrieved environment\n",
    "# print(env.docker.base_image)\n",
    "\n",
    "# print(\"\\n Attributes of retrieved environment:\")\n",
    "# env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first run in a given environment, Azure ML spends some time building the environment. On the subsequent runs, Azure ML keeps track of changes and uses the existing environment, resulting in faster run completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name=\"pytorch-aml-env\",\n",
    "                                           file_path=\"../environments/conda/environment.yml\")\n",
    "\n",
    "# Attribute docker.enabled controls whether to use Docker container or host OS for execution.\n",
    "# This is only relevant for local execution as execution on AML Compute Cluster will always use Docker container.\n",
    "# env.docker.enabled = True\n",
    "\n",
    "# Use Python dependencies from your Docker image (as opposed to from conda specification)\n",
    "# env.python.user_managed_dependencies=True\n",
    "\n",
    "## Only uncomment one of the three below options\n",
    "# OPTION 1: Use mcr base image\n",
    "#env.docker.base_image = \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20201113.v1\"\n",
    "#env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04' # GPU base image\n",
    "\n",
    "# Option 2: Use custom base image from workspace-native ACR\n",
    "#env.docker.base_image = \"eafc0c3ef9714c74a4fa655ee90531ba.azurecr.io/base/pytorch\"\n",
    "\n",
    "# OPTION 3: Use custom base image from standalone ACR and use admin user credentials. For this you need to enable admin user in the ACR.\n",
    "env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "env.docker.base_image_registry.address = \"sbirkacr.azurecr.io\"\n",
    "env.docker.base_image_registry.username = \"sbirkacr\"\n",
    "env.docker.base_image_registry.password = \"HqAu5Y2We0gZ42IunR5MBXkKc+shf2uj\" # replace with Key Vault\n",
    "\n",
    "# Option 4: Use custom base image from standalone ACR and use service principal authentication. \n",
    "#           The service principal needs the AcrPull permission on the standalone ACR.\n",
    "env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "env.docker.base_image_registry.address = \"sbirkacr.azurecr.io\"\n",
    "env.docker.base_image_registry.username = keyvault.get_secret(name=\"sbirk-acr-sp-username\")\n",
    "env.docker.base_image_registry.password = keyvault.get_secret(name=\"sbirk-acr-sp-password\")\n",
    "\n",
    "# Option 5: Use custom base image from standalone ACR with anonymous access preview feature.\n",
    "# env.docker.base_image = \"sbirkacr.azurecr.io/base/pytorch\"\n",
    "\n",
    "# Create an environment variable.\n",
    "# This can be retrieved in the training script with os.environ.get(\"MESSAGE\").\n",
    "# env.environment_variables = {\"MESSAGE\": \"Hello from Azure Machine Learning\"}\n",
    "\n",
    "env.inferencing_stack_version = \"latest\" # None\n",
    "\n",
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment & Run Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training artifacts are prepared, a model can be trained on the remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: \"Normal\" Script Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "experiment = Experiment(workspace=ws, \n",
    "                        name=\"cifar-image-classification-pytorch\")\n",
    "\n",
    "# Create the script run configuration\n",
    "config = ScriptRunConfig(source_directory=\"../src/training\",\n",
    "                         script=\"train.py\",\n",
    "                         compute_target=cpu_cluster_name,\n",
    "                         arguments=[\n",
    "                             \"--data_path\", dataset.as_named_input(\"input\").as_mount(),\n",
    "                             \"--num_epochs\", 25,\n",
    "                             \"--output_dir\", \"./outputs\"\n",
    "                             \"--learning_rate\", 0.001,\n",
    "                             \"--momentum\", 0.9])\n",
    "\n",
    "config.run_config.environment = env\n",
    "\n",
    "# Submit the run\n",
    "run = experiment.submit(config)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Hyperdrive Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be tuned using AML's hyperdrive capability.\n",
    "\n",
    "The initial learning rate is tuned. The training script uses a LR schedule to decay the learning rate every several epochs starting from that initial learning rate.\n",
    "\n",
    "Random sampling is used to try different configuration sets of hyperparameters to maximize the primary metric, the best validation accuracy (best_val_acc).\n",
    "\n",
    "An early termination policy is specified to early terminate poorly performing runs. The BanditPolicy is used, which will terminate any run that doesn't fall within the slack factor of the primary evaluation metric. In this tutorial, this policy will be applied every epoch (since the best_val_acc metric is reported every epoch and evaluation_interval=1). The first policy evaluation will be delayed until after the first 10 epochs (delay_evaluation=10). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = RandomParameterSampling({\n",
    "        \"num_epochs\": choice(10,15,20),\n",
    "        \"learning_rate\": uniform(0.0005, 0.005), \n",
    "        \"momentum\": uniform(0.9, 0.99),\n",
    "    }\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(run_config=config,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name=\"best_val_acc\",\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=8,\n",
    "                                     max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get portal URL\n",
    "run.get_portal_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run metrics, details and file names\n",
    "print(run.get_metrics())\n",
    "print(run.get_details())\n",
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/cifar_net.pt\"\n",
    "\n",
    "model = run.register_model(model_name=\"cifar10-model\",\n",
    "                           model_path=model_path,\n",
    "                           model_framework=Model.Framework.PYTORCH,\n",
    "                           description=\"cifar10 model\")\n",
    "\n",
    "print(model.name, model.id, model.version, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "run.download_file(name=os.path.join(\"../\", model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu_cluster.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/get-started-day1/day1-part4-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"your-tenant-id\")\n",
    "Additional details on authentication can be found here: https://aka.ms/aml-notebook-auth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The very first run will take 5-10minutes to complete. This is because in the background a docker image is built in the cloud, the compute cluster is resized from 0 to 1 node, and the docker image is downloaded to the compute. Subsequent runs are much quicker (~15 seconds) as the docker image is cached on the compute - you can test this by resubmitting the code below after the first run has completed.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> The first time you run this script, Azure Machine Learning will build a new docker image from your PyTorch environment. The whole run could take 5-10 minutes to complete. You can see the docker build logs in the widget by selecting the `20_image_build_log.txt` in the log files dropdown. This image will be reused in future runs making them run much quicker.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies in the Azure ML environment don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
    "\n",
    "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling typically takes **about 5 minutes.**\n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need to create an Azure Machine Learning _compute instance_. The benefits of a compute instance over a local machine (e.g. laptop) or cloud VM are as follows:\n",
    "\n",
    "* It is a pre-configured with all the latest data science libaries (e.g. panads, scikit, TensorFlow, PyTorch) and tools (Jupyter, RStudio). In this tutorial we make extensive use of PyTorch, AzureML SDK, matplotlib and we do not need to install these components on a compute instance.\n",
    "* Notebooks are seperate from the compute instance - this means that you can develop your notebook on a small VM size, and then seamlessly scale up (and/or use a GPU-enabled) the machine when needed to train a model.\n",
    "* You can easily turn on/off the instance to control costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "samkemp"
   }
  ],
  "categories": [
   "tutorials",
   "get-started-day1"
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "pytorch-aml-env",
   "language": "python",
   "name": "pytorch-aml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
