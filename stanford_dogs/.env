# Azure Subscription Variables
TENANT_ID = "cf36141c-ddd7-45a7-b073-111f66d0b30c"
LOCATION = 'westeurope'
BASE_NAME = 'sb94'
SP_APP_ID = "8a0b5ebf-55c7-4dfa-a49c-37b0acd9c3ce"
SP_APP_SECRET = "5tG.N6t9_wKmzV4.gP3pOTYwwXF2i-1jDR"

# Custom Vision Variables
CUSTOM_VISION_ENDPOINT = 'https://westeurope.api.cognitive.microsoft.com/'
CUSTOM_VISION_TRAINING_KEY = '48930db8a19e46d9be825529afb2e568'
CUSTOM_VISION_PREDICTION_KEY = '55c618216177405bb3b4bbce3d13ecac'
CUSTOM_VISION_PREDICTION_RESOURCE_ID = '/subscriptions/e58a23da-421e-4b52-99d5-e615f2f8be41/resourceGroups/sbirkamlrg/providers/Microsoft.CognitiveServices/accounts/sbirkamlcvprediction'
CUSTOM_VISION_PROJECT_NAME = 'Stanford Dogs'
CUSTOM_VISION_PUBLISH_ITERATION_NAME = 'Stanford Dogs'

# Mock build/release ID for local testing
BUILD_ID = "001"
BUILD_URI = ""

# Azure ML Workspace Variables
WORKSPACE_NAME = "sbirkamlws" 
SUBSCRIPTION_ID = "e58a23da-421e-4b52-99d5-e615f2f8be41" 
RESOURCE_GROUP = "sbirkamlrg"
EXPERIMENT_NAME = "stanford-dogs-classifier-train-pipeline"

# AML Compute Cluster Config
AML_TRAIN_ENV_NAME="stanford-dogs-train-env"
AML_TRAIN_ENV_CONDA_FILE_PATH="environments/conda/training_environment.yml"
AML_COMPUTE_CLUSTER_NAME = "gpu-cluster"
AML_COMPUTE_CLUSTER_GPU_SKU = 'STANDARD_NC6'
AML_CLUSTER_MAX_NODES = '4'
AML_CLUSTER_MIN_NODES = '0'
AML_CLUSTER_PRIORITY = 'lowpriority'

# Training Config
MODEL_NAME = "dog_clf_model"
MODEL_VERSION = '1'

# AML Pipeline Config
TRAINING_PIPELINE_NAME = "dog-classification-training-pipeline"
MODEL_PATH = ''
PIPELINE_TRAIN_SCRIPT_PATH = "pipeline/train_model_step.py"
PIPELINE_EVALUATE_SCRIPT_PATH = "pipeline/evaluate_model_step.py"
PIPELINE_REGISTER_SCRIPT_PATH = "pipeline/register_model_step.py"
SOURCES_DIR_TRAIN = 'src/training'
DATASET_NAME = "stanford-dogs-dataset"
DATASET_VERSION = "1"
# Optional. Set it if you have configured non default datastore to point to your data
DATASTORE_NAME = ""
SCORE_SCRIPT = 'scoring/score.py'
CONDA_ENV_DIR='environments/conda'

# Optional. Used by a training pipeline with R on Databricks
DB_CLUSTER_ID = ''

# Optional. Container Image name for image creation
IMAGE_NAME = 'mltrained'

# Run Evaluation Step in AML pipeline
PIPELINE_RUN_EVALUATION = "true"

# Set to true cancels the Azure ML pipeline run when evaluation criteria are not met.
PIPELINE_ALLOW_RUN_CANCEL = "true"

# Flag to allow rebuilding the AML Environment after it was built for the first time. This enables dependency updates from conda_dependencies.yaml.
AML_TRAIN_ENV_REBUILD = "false"



USE_GPU_FOR_SCORING = "false"
AML_ENV_SCORE_CONDA_DEP_FILE="conda_dependencies_scoring.yml"
AML_ENV_SCORECOPY_CONDA_DEP_FILE="conda_dependencies_scorecopy.yml"
# AML Compute Cluster Config for parallel batch scoring
AML_ENV_NAME_SCORING='diabetes_regression_scoring_env'
AML_ENV_NAME_SCORE_COPY='diabetes_regression_score_copy_env'
AML_COMPUTE_CLUSTER_NAME_SCORING = 'score-cluster'
AML_COMPUTE_CLUSTER_CPU_SKU_SCORING = 'STANDARD_DS2_V2'
AML_CLUSTER_MAX_NODES_SCORING = '4'
AML_CLUSTER_MIN_NODES_SCORING = '0'
AML_CLUSTER_PRIORITY_SCORING = 'lowpriority'
AML_REBUILD_ENVIRONMENT_SCORING = 'true'
BATCHSCORE_SCRIPT_PATH = 'scoring/parallel_batchscore.py'
BATCHSCORE_COPY_SCRIPT_PATH = 'scoring/parallel_batchscore_copyoutput.py'


SCORING_DATASTORE_INPUT_CONTAINER = 'input'
SCORING_DATASTORE_INPUT_FILENAME = 'diabetes_scoring_input.csv'
SCORING_DATASTORE_OUTPUT_CONTAINER = 'output'
SCORING_DATASTORE_OUTPUT_FILENAME = 'diabetes_scoring_output.csv'
SCORING_DATASET_NAME = 'diabetes_scoring_ds'
SCORING_PIPELINE_NAME = 'diabetes-scoring-pipeline'

# To be removed!
AML_SERVICE_PRINCIPAL_PASSWORD=~IwB8ryrB.F1RF82Zxh96WYc~7.jgvX2V7

SRC_DIR = "src"

DATA_DIR = "data"
DATASTORE_TARGET_DIR = "data/stanford_dogs"